[
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/5-workshop/5.3-s3/create-s3/",
	"title": "Tạo S3",
	"tags": [],
	"description": "",
	"content": "Tạo S3 Truy cập vào S3\nNhấn chọn Create Bucket:\nTại giao diện Create Bucket: Chỉ định tên cho bucket: s3-demo-text\nKhông thêm tag cho VPC endpoint vào lúc này. Nhấn Create bucket Sau đó chọn bucket vừa tạo rồi chọn Create folder Trong bảng điều khiển Tạo thư mục: Chỉ định tên của bucket: input sau đó chọn Create folder Chúng ta làm tương tự khi tạo tệp output. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/5-workshop/5.1-workshop-overview/",
	"title": "Tổng quan Workshop",
	"tags": [],
	"description": "",
	"content": "GIỚI THIỆU Giới thiệu về S3 Event Notifications S3 Event Notifications là tính năng cho phép Amazon S3 tự động gửi thông báo khi có các sự kiện cụ thể xảy ra trong Bucket (ví dụ: khi một file mới được upload, xóa hoặc sao chép). S3 có thể gửi thông báo đến nhiều đích đến khác nhau như AWS Lambda, Amazon SNS, hoặc Amazon SQS. Trong workshop này, chúng ta sử dụng sự kiện PutObject để tự động kích hoạt Lambda function ngay khi file văn bản được upload lên S3. Giới thiệu về Amazon Polly Amazon Polly là dịch vụ chuyển văn bản thành giọng nói (Text-to-Speech) sử dụng công nghệ Deep Learning tiên tiến của AWS. Polly hỗ trợ hơn 60 giọng nói trong hơn 20 ngôn ngữ, bao gồm cả tiếng Việt, với chất lượng âm thanh tự nhiên giống con người. Dịch vụ này hoàn toàn được quản lý (fully managed), bạn không cần lo lắng về việc quản lý hạ tầng hay khả năng mở rộng. Tổng quan về workshop Trong workshop này, bạn sẽ xây dựng một ứng dụng Serverless Text-to-Speech Converter (Chuyển đổi văn bản thành giọng nói). Hệ thống hoạt động hoàn toàn tự động dựa trên mô hình Event-driven Architecture:\nAmazon S3 (Input Bucket): Lưu trữ file văn bản đầu vào (.txt) do người dùng upload. S3 Event Notifications: Phát hiện sự kiện upload file mới và kích hoạt AWS Lambda. AWS Lambda: Bộ xử lý trung tâm, đóng vai trò điều phối luồng dữ liệu giữa S3 và Polly. Lambda đọc file văn bản, gọi Polly API để chuyển đổi, và lưu kết quả. Amazon Polly: Dịch vụ AI thực hiện chuyển đổi văn bản thành âm thanh với giọng nói tự nhiên. Amazon S3 (Output Bucket): Lưu trữ file âm thanh đầu ra (.mp3) sau khi xử lý hoàn tất. Toàn bộ quy trình diễn ra tự động mà không cần can thiệp thủ công, giúp tiết kiệm thời gian và chi phí vận hành.\nKiến trúc hệ thống Mô hình dưới đây mô tả chi tiết kiến trúc và luồng dữ liệu (Data Flow) của hệ thống Text-to-Speech Converter:\nLuồng hoạt động (Workflow):\nUpload file: Người dùng tải file văn bản (.txt) lên Amazon S3 Input Bucket.\nEvent trigger: S3 phát hiện sự kiện PutObject (có file mới), tự động gửi event notification kích hoạt AWS Lambda Function.\nLambda xử lý: Lambda function được trigger và thực hiện:\nĐọc nội dung file văn bản từ S3 Input Bucket Gửi nội dung văn bản đến Amazon Polly API cùng với các tham số (voice ID, output format) Polly chuyển đổi: Amazon Polly nhận request, xử lý văn bản và trả về audio stream (dữ liệu âm thanh) cho Lambda.\nLưu kết quả: Lambda nhận audio stream từ Polly và lưu dưới dạng file .mp3 vào Amazon S3 Output Bucket.\nHoàn tất: Người dùng có thể tải xuống file MP3 từ S3 Output Bucket để sử dụng.\nLợi ích của kiến trúc Serverless Không cần quản lý server: AWS tự động xử lý scaling, patching và high availability. Chi phí tối ưu: Chỉ trả tiền khi có request xử lý (pay-per-use model). Tự động mở rộng: Hệ thống có thể xử lý từ vài requests đến hàng triệu requests mà không cần cấu hình thêm. Triển khai nhanh: Tập trung vào logic ứng dụng thay vì quản lý infrastructure. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Báo cáo tổng hợp: “Hội thảo Dịch vụ AWS AI/ML \u0026amp; Generative AI” Mục tiêu Sự kiện Cung cấp cái nhìn tổng quan về bức tranh AI/ML tại Việt Nam. Minh họa quy trình máy học (Machine Learning) toàn diện sử dụng Amazon SageMaker. Giới thiệu các khả năng của Generative AI với Amazon Bedrock (Mô hình nền tảng, Agents, Guardrails). Chia sẻ các kỹ thuật về Kỹ thuật tạo câu lệnh (Prompt Engineering) và Tạo nội dung tăng cường truy xuất (RAG). Diễn giả Các Kiến trúc sư giải pháp (Solutions Architects) Các Chuyên gia về AI/ML Điểm nhấn Chính Tổng quan Dịch vụ AWS AI/ML (SageMaker) Nền tảng Toàn diện: Bao phủ toàn bộ vòng đời từ chuẩn bị dữ liệu đến triển khai. Chuẩn bị dữ liệu: Các chiến lược gán nhãn và làm sạch dữ liệu. Huấn luyện \u0026amp; Tinh chỉnh: Tối ưu hóa mô hình về hiệu năng và chi phí. Triển khai: Đưa mô hình lên môi trường production (thực tế) một cách hiệu quả. MLOps Tích hợp: Nêu bật khả năng tự động hóa và chuẩn hóa các đường ống (pipeline) ML. SageMaker Studio: Demo trực tiếp giao diện thống nhất để xây dựng, huấn luyện và triển khai mô hình. Generative AI với Amazon Bedrock Mô hình Nền tảng (FMs): Hướng dẫn so sánh và lựa chọn các mô hình hàng đầu: Claude: Khả năng suy luận cao. Llama: Mã nguồn mở và hiệu quả. Titan: Tích hợp nguyên bản (native) với AWS. Prompt Engineering (Kỹ thuật Prompt): Chain-of-Thought (Chuỗi suy luận): Chia nhỏ các tác vụ suy luận phức tạp. Few-shot learning (Học từ vài mẫu): Sử dụng các ví dụ mẫu để cải thiện kết quả đầu ra. Kiến trúc Nâng cao: RAG (Retrieval-Augmented Generation): Tích hợp Knowledge Bases (Cơ sở tri thức) để căn cứ câu trả lời dựa trên dữ liệu doanh nghiệp. Bedrock Agents: Tạo quy trình làm việc đa bước và tích hợp với các công cụ bên ngoài. Guardrails (Bộ lọc an toàn): Đảm bảo an toàn và lọc nội dung. Bài học Chính (Key Takeaways) Tư duy Thiết kế Lựa chọn Mô hình: Chọn Mô hình nền tảng phù hợp (ví dụ: Claude vs. Titan) dựa trên yêu cầu cụ thể của trường hợp sử dụng (tốc độ vs. khả năng suy luận). An toàn là trên hết: Việc triển khai Guardrails là rất quan trọng đối với ứng dụng AI có trách nhiệm trong môi trường doanh nghiệp. Kiến trúc Kỹ thuật Ưu tiên RAG hơn Fine-tuning: Đối với hầu hết các trường hợp sử dụng kiến thức nội bộ, RAG mang lại giải pháp linh hoạt và tiết kiệm chi phí hơn so với việc tinh chỉnh mô hình. Quy trình làm việc Agentic: Chuyển từ chatbot thụ động sang các Agent chủ động có thể thực thi tác vụ là biên giới tiếp theo của GenAI. Chiến lược Hiện đại hóa Áp dụng MLOps: Chuyển từ huấn luyện mô hình thủ công sang các pipeline tự động (MLOps) là điều cần thiết để mở rộng quy mô. Bối cảnh Địa phương: Hiểu rõ các xu hướng AI/ML cụ thể tại Việt Nam giúp đối chiếu tiêu chuẩn cho các dự án trong nước. Ứng dụng vào Công việc Thử nghiệm SageMaker: Đánh giá quy trình ML hiện tại và xác định cơ hội chuyển đổi sang Amazon SageMaker để quản lý vòng đời tốt hơn. Xây dựng Bot Kiến thức: Tạo nguyên mẫu sử dụng Amazon Bedrock và RAG để tra cứu tài liệu nội bộ hoặc hướng dẫn kỹ thuật. Cải thiện Prompt: Áp dụng ngay kỹ thuật Chain-of-Thought và Few-shot để nâng cao độ chính xác của các tương tác AI hiện tại. Triển khai Guardrails: Cấu hình bộ lọc nội dung trên Bedrock để đảm bảo an toàn thương hiệu cho các ứng dụng thử nghiệm. Trải nghiệm Sự kiện Tham dự hội thảo “AWS AI/ML Services \u0026amp; Generative AI” đã cung cấp một lộ trình thực tế để áp dụng các dịch vụ thông minh. Những trải nghiệm chính bao gồm:\nHọc hỏi từ chuyên gia Hiểu rõ hơn về bức tranh AI/ML tại Việt Nam, nắm bắt các cơ hội và thách thức tại địa phương. Đào sâu kiến thức kỹ thuật về sự khác biệt giữa các Mô hình nền tảng lớn (Claude, Llama, Titan). Tiếp cận kỹ thuật thực tế Phần hướng dẫn SageMaker Studio đã minh họa cách thống nhất chuỗi công cụ ML rời rạc vào một giao diện quản lý duy nhất. Demo trực tiếp về Bedrock đã cho thấy việc triển khai thực tế một chatbot GenAI, giải mã sự phức tạp của RAG và Agents. Kết nối và thảo luận Hoạt động ice-breaker và các phiên networking cho phép trao đổi ý tưởng với đồng nghiệp về những thách thức thực tế khi triển khai GenAI. Các cuộc thảo luận củng cố tầm quan trọng của Prompt Engineering như một kỹ năng quan trọng cho phát triển hiện đại. Bài học rút ra RAG là chìa khóa để làm cho LLM hữu ích với dữ liệu đặc thù của doanh nghiệp mà không tốn chi phí huấn luyện cao. Guardrails không phải là tùy chọn; chúng là một lớp nền tảng của ngăn xếp (stack) Generative AI. Hiệu quả trong ML đến từ MLOps tích hợp thay vì các thử nghiệm khoa học dữ liệu riêng lẻ. Ảnh sự kiện "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/",
	"title": "Báo cáo thực tập",
	"tags": [],
	"description": "",
	"content": "Báo cáo thực tập Thông tin sinh viên: Họ và tên: Nguyễn Trương Phúc Thịnh\nSố điện thoại: 0907511320\nEmail: nguyentruongphucthinh@gmail.com\nTrường: Đại học FPT\nNgành: Công nghệ thông tin\nChuyên ngành: Trí tuệ nhân tạo\nCông ty thực tập: Công ty TNHH Amazon Web Services Vietnam\nVị trí thực tập: Thực tập sinh FCJ\nThời gian thực tập: Từ ngày 08/09/2025 đến ngày 09/12/2025\nNội dung báo cáo Worklog Proposal Các bài blogs đã dịch Các events đã tham gia Workshop Tự đánh giá Chia sẻ, đóng góp ý kiến "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Thí nghiệm ML nhanh chóng cho doanh nghiệp với Amazon SageMaker AI và Comet Bài đăng này được viết cùng với Sarah Ostermeier từ Comet.\nKhi các tổ chức doanh nghiệp mở rộng các sáng kiến học máy (ML) từ giai đoạn chứng minh ý tưởng sang sản xuất, độ phức tạp trong việc quản lý các thí nghiệm, theo dõi dòng dõi mô hình, và quản lý khả năng tái tạo tăng lên theo cấp số nhân. Điều này chủ yếu là do các nhà khoa học dữ liệu và kỹ sư ML liên tục khám phá các kết hợp khác nhau của siêu tham số, kiến trúc mô hình, và phiên bản tập dữ liệu, tạo ra lượng lớn siêu dữ liệu cần được theo dõi để đảm bảo khả năng tái tạo và tuân thủ quy định. Khi việc phát triển mô hình ML mở rộng qua nhiều đội nhóm và yêu cầu quy định ngày càng nghiêm ngặt, việc theo dõi thí nghiệm trở nên phức tạp hơn bao giờ hết. Với các quy định AI ngày càng tăng, đặc biệt là ở EU, các tổ chức hiện yêu cầu các bản ghi kiểm toán chi tiết về dữ liệu huấn luyện mô hình, kỳ vọng hiệu suất, và quy trình phát triển, khiến việc theo dõi thí nghiệm trở thành một nhu cầu kinh doanh thiết yếu, không chỉ là một thực hành tốt nhất.\nAmazon SageMaker AI cung cấp cơ sở hạ tầng được quản lý mà các doanh nghiệp cần để mở rộng khối lượng công việc ML, xử lý việc cung cấp tính toán, huấn luyện phân tán, và triển khai mà không cần gánh nặng cơ sở hạ tầng. Tuy nhiên, các đội nhóm vẫn cần khả năng theo dõi thí nghiệm mạnh mẽ, so sánh mô hình, và khả năng cộng tác vượt xa việc ghi log cơ bản.\nComet là một nền tảng quản lý thí nghiệm ML toàn diện, tự động theo dõi, so sánh, và tối ưu hóa các thí nghiệm ML trong suốt vòng đời mô hình. Nó cung cấp cho các nhà khoa học dữ liệu và kỹ sư ML các công cụ mạnh mẽ để theo dõi thí nghiệm, giám sát mô hình, tối ưu hóa siêu tham số, và phát triển mô hình cộng tác. Nó cũng cung cấp Opik, nền tảng mã nguồn mở của Comet để quan sát và phát triển LLM.\nComet có sẵn trong SageMaker AI dưới dạng một Ứng dụng AI Đối tác, như một khả năng quản lý thí nghiệm được quản lý hoàn toàn, với bảo mật cấp doanh nghiệp, tích hợp quy trình làm việc liền mạch, và quy trình mua sắm đơn giản thông qua AWS Marketplace.\nSự kết hợp này đáp ứng nhu cầu của một quy trình làm việc ML doanh nghiệp từ đầu đến cuối, trong đó SageMaker AI xử lý cơ sở hạ tầng và tính toán, còn Comet cung cấp khả năng quản lý thí nghiệm, đăng ký mô hình, và giám sát sản xuất mà các đội nhóm yêu cầu để tuân thủ quy định và hiệu quả vận hành. Trong bài đăng này, chúng tôi thể hiện một quy trình phát hiện gian lận hoàn chỉnh bằng cách sử dụng SageMaker AI với Comet, thể hiện khả năng tái tạo và ghi log sẵn sàng kiểm toán mà các doanh nghiệp cần hiện nay.\nComet sẵn sàng cho doanh nghiệp trên SageMaker AI Trước khi tiến hành các hướng dẫn cài đặt, các tổ chức phải xác định mô hình vận hành của mình và dựa trên đó, quyết định cách cài đặt Comet. Chúng tôi khuyên bạn nên triển khai Comet bằng cách sử dụng mô hình vận hành liên kết. Trong kiến trúc này, Comet được quản lý tập trung và lưu trữ trong một tài khoản dịch vụ chia sẻ, và mỗi đội ngũ khoa học dữ liệu duy trì các môi trường hoàn toàn tự trị. Mỗi mô hình vận hành đi kèm với những lợi ích và hạn chế riêng. Để biết thêm thông tin, hãy tham khảo Thực hành tốt nhất cho Quản trị Amazon SageMaker Studio.\nHãy cùng đi sâu vào việc cài đặt Comet trong SageMaker AI. Các doanh nghiệp lớn thường có các vai trò sau:\nQuản trị viên – Chịu trách nhiệm thiết lập các dịch vụ cơ sở hạ tầng chung và môi trường cho các đội nhóm sử dụng trường hợp. Người dùng – Các nhà thực hành ML từ các đội nhóm sử dụng trường hợp, sử dụng các môi trường do đội nền tảng thiết lập để giải quyết các vấn đề kinh doanh của họ. Trong các phần sau, chúng ta sẽ đi qua hành trình của từng vai trò.\nComet hoạt động tốt với cả SageMaker AI và Amazon SageMaker. SageMaker AI cung cấp môi trường phát triển tích hợp (IDE) Amazon SageMaker Studio, và SageMaker cung cấp IDE Amazon SageMaker Unified Studio. Trong bài đăng này, chúng tôi sử dụng SageMaker Studio.\nHành trình của quản trị viên Trong kịch bản này, quản trị viên nhận được yêu cầu từ một đội nhóm làm việc trên trường hợp sử dụng phát hiện gian lận để cung cấp một môi trường ML với thiết lập huấn luyện và thí nghiệm được quản lý hoàn toàn. Hành trình của quản trị viên bao gồm các bước sau:\nThực hiện các điều kiện tiên quyết để thiết lập Ứng dụng AI Đối tác. Điều này thiết lập quyền cho quản trị viên, cho phép Comet đảm nhận vai trò thực thi SageMaker AI thay mặt cho người dùng và các đặc quyền bổ sung để quản lý đăng ký Comet thông qua AWS Marketplace. Trên bảng điều khiển SageMaker AI, trong mục Ứng dụng và IDE trong bảng điều hướng, chọn Ứng dụng AI Đối tác, sau đó chọn Xem chi tiết cho Comet. Các chi tiết được hiển thị, bao gồm mô hình định giá hợp đồng cho Comet và chi phí ước tính của cấp độ cơ sở hạ tầng. Comet cung cấp các tùy chọn đăng ký khác nhau từ hợp đồng 1 tháng đến 36 tháng. Với hợp đồng này, người dùng có thể truy cập Comet trong SageMaker. Dựa trên số lượng người dùng, quản trị viên có thể xem xét và phân tích kích thước phiên bản phù hợp cho máy chủ bảng điều khiển Comet. Comet hỗ trợ từ 5–500 người dùng chạy hơn 100 công việc thí nghiệm. Chọn Đi đến Marketplace để đăng ký để được chuyển hướng đến danh sách Comet trên AWS Marketplace. Chọn Xem tùy chọn mua hàng. Trong biểu mẫu đăng ký, cung cấp các chi tiết cần thiết.\nKhi đăng ký hoàn tất, quản trị viên có thể bắt đầu cấu hình Comet.\nTrong khi triển khai Comet, thêm trưởng nhóm dự án của đội nhóm trường hợp sử dụng phát hiện gian lận làm quản trị viên để quản lý các hoạt động quản trị cho bảng điều khiển Comet. Việc triển khai máy chủ Comet mất vài phút. Để biết thêm chi tiết về bước này, hãy tham khảo Partner AI App provisioning.\nThiết lập một miền SageMaker AI theo các bước trong Use custom setup for Amazon SageMaker AI. Theo thực hành tốt nhất, cung cấp một URL miền đã ký trước cho thành viên đội nhóm trường hợp sử dụng để truy cập trực tiếp vào giao diện người dùng Comet mà không cần đăng nhập vào bảng điều khiển SageMaker. Thêm các thành viên đội nhóm vào miền này và kích hoạt quyền truy cập vào Comet trong khi cấu hình miền. Bây giờ miền SageMaker AI đã sẵn sàng để người dùng đăng nhập và bắt đầu làm việc trên trường hợp sử dụng phát hiện gian lận.\nHành trình của người dùng Bây giờ, hãy khám phá hành trình của một nhà thực hành ML từ trường hợp sử dụng phát hiện gian lận. Người dùng hoàn thành các bước sau:\nĐăng nhập vào miền SageMaker AI thông qua URL đã ký trước. Bạn sẽ được chuyển hướng đến IDE SageMaker Studio. Tên người dùng và vai trò thực thi AWS Identity and Access Management (IAM) của bạn được quản trị viên cấu hình sẵn.\nTạo một Không gian JupyterLab theo hướng dẫn người dùng JupyterLab. Bạn có thể bắt đầu làm việc trên trường hợp sử dụng phát hiện gian lận bằng cách khởi tạo một sổ Jupyter. Quản trị viên cũng đã thiết lập quyền truy cập cần thiết vào dữ liệu thông qua một thùng Amazon Simple Storage Service (Amazon S3).\nĐể truy cập API Comet, cài đặt thư viện comet_ml và cấu hình các biến môi trường cần thiết như mô tả trong Set up the Amazon SageMaker Partner AI Apps SDKs. Để truy cập giao diện người dùng Comet, chọn Ứng dụng AI Đối tác trong bảng điều hướng SageMaker Studio và chọn Mở cho Comet. Bây giờ, hãy cùng đi qua việc triển khai trường hợp sử dụng.\nTổng quan về giải pháp Trường hợp sử dụng này làm nổi bật các thách thức doanh nghiệp phổ biến: làm việc với các tập dữ liệu không cân bằng (trong ví dụ này, chỉ 0,17% giao dịch là gian lận), yêu cầu nhiều lần lặp thí nghiệm, và duy trì khả năng tái tạo hoàn toàn để tuân thủ quy định. Để theo dõi, hãy tham khảo tài liệu Comet và Hướng dẫn Bắt đầu Nhanh để biết thêm chi tiết về cài đặt và API.\nĐối với trường hợp sử dụng này, chúng tôi sử dụng tập dữ liệu Phát hiện Gian lận Thẻ Tín dụng. Tập dữ liệu chứa các giao dịch thẻ tín dụng với nhãn nhị phân đại diện cho giao dịch gian lận (1) hoặc hợp pháp (0). Trong các phần sau, chúng tôi đi qua một số phần quan trọng của việc triển khai. Toàn bộ mã của việc triển khai có sẵn trong kho GitHub.\nĐiều kiện tiên quyết Là điều kiện tiên quyết, cấu hình các thư viện nhập cần thiết và các biến môi trường cho tích hợp Comet và SageMaker:\n# Comet ML để theo dõi thí nghiệm import comet_ml from comet_ml import Experiment, API, Artifact from comet_ml.integration.sagemaker import log_sagemaker_training_job_v1 AWS_PARTNER_APP_AUTH=true AWS_PARTNER_APP_ARN=\u0026lt;Your_AWS_PARTNER_APP_ARN\u0026gt; COMET_API_KEY=\u0026lt;Your_Comet_API_Key\u0026gt; # Từ Trang Chi tiết, nhấp vào Mở Comet. Ở góc trên bên phải, nhấp vào người dùng -\u0026gt; Khóa API # Cấu hình Comet ML COMET_WORKSPACE = \u0026#39;\u0026lt;your-comet-workspace-name\u0026gt;\u0026#39; COMET_PROJECT_NAME = \u0026#39;\u0026lt;your-comet-project-name\u0026gt;\u0026#39; Chuẩn bị tập dữ liệu Một trong những tính năng doanh nghiệp chính của Comet là phiên bản hóa tập dữ liệu tự động và theo dõi dòng dõi. Khả năng này cung cấp khả năng kiểm toán đầy đủ về dữ liệu nào được sử dụng để huấn luyện mỗi mô hình, điều này rất quan trọng để tuân thủ quy định và khả năng tái tạo. Bắt đầu bằng cách tải tập dữ liệu:\n# Tạo một Artifact Comet để theo dõi tập dữ liệu gốc dataset_artifact = Artifact( name=\u0026#34;fraud-dataset\u0026#34;, artifact_type=\u0026#34;dataset\u0026#34;, aliases=[\u0026#34;raw\u0026#34;] ) # Thêm tệp tập dữ liệu gốc vào artifact dataset_artifact.add_remote(s3_data_path, metadata={ \u0026#34;dataset_stage\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;dataset_split\u0026#34;: \u0026#34;not_split\u0026#34;, \u0026#34;preprocessing\u0026#34;: \u0026#34;none\u0026#34; }) Bắt đầu một thí nghiệm Comet Với artifact tập dữ liệu đã được tạo, bạn có thể bắt đầu theo dõi quy trình làm việc ML. Việc tạo một thí nghiệm Comet tự động bắt đầu ghi lại mã, thư viện đã cài đặt, siêu dữ liệu hệ thống, và các thông tin ngữ cảnh khác trong nền. Bạn có thể ghi lại artifact tập dữ liệu đã tạo trước đó trong thí nghiệm. Xem mã sau:\n# Tạo một thí nghiệm Comet mới experiment_1 = comet_ml.Experiment( project_name=COMET_PROJECT_NAME, workspace=COMET_WORKSPACE, ) # Ghi lại artifact tập dữ liệu vào thí nghiệm này để theo dõi dòng dõi experiment_1.log_artifact(dataset_artifact) Tiền xử lý dữ liệu Các bước tiếp theo là các bước tiền xử lý tiêu chuẩn, bao gồm loại bỏ các bản sao, bỏ các cột không cần thiết, chia thành các tập huấn luyện/xác thực/kiểm tra, và chuẩn hóa các đặc trưng bằng cách sử dụng StandardScaler của scikit-learn. Chúng tôi gói mã xử lý trong preprocess.py và chạy nó như một công việc Xử lý SageMaker. Xem mã sau:\n# Chạy công việc xử lý SageMaker processor = SKLearnProcessor( framework_version=\u0026#39;1.0-1\u0026#39;, role=sagemaker.get_execution_role(), instance_count=1, instance_type=\u0026#39;ml.t3.medium\u0026#39; ) processor.run( code=\u0026#39;preprocess.py\u0026#39;, inputs=[ProcessingInput(source=s3_data_path, destination=\u0026#39;/opt/ml/processing/input\u0026#39;)], outputs=[ProcessingOutput(source=\u0026#39;/opt/ml/processing/output\u0026#39;, destination=f\u0026#39;s3://{bucket_name}/{processed_data_prefix}\u0026#39;)] ) Sau khi bạn gửi công việc xử lý, SageMaker AI khởi động các phiên bản tính toán, xử lý và phân tích dữ liệu đầu vào, và giải phóng tài nguyên sau khi hoàn thành. Đầu ra của công việc xử lý được lưu trữ trong thùng S3 đã chỉ định.\nTiếp theo, tạo một phiên bản mới của artifact tập dữ liệu để theo dõi dữ liệu đã được xử lý. Comet tự động phiên bản hóa các artifact có cùng tên, duy trì dòng dõi hoàn chỉnh từ dữ liệu gốc đến dữ liệu đã được xử lý.\n# Tạo một phiên bản cập nhật của Artifact \u0026#39;fraud-dataset\u0026#39; cho dữ liệu đã được xử lý preprocessed_dataset_artifact = Artifact( name=\u0026#34;fraud-dataset\u0026#34;, artifact_type=\u0026#34;dataset\u0026#34;, aliases=[\u0026#34;preprocessed\u0026#34;], metadata={ \u0026#34;description\u0026#34;: \u0026#34;Tập dữ liệu phát hiện gian lận thẻ tín dụng\u0026#34;, \u0026#34;fraud_percentage\u0026#34;: f\u0026#34;{fraud_percentage:.3f}%\u0026#34;, \u0026#34;dataset_stage\u0026#34;: \u0026#34;preprocessed\u0026#34;, \u0026#34;preprocessing\u0026#34;: \u0026#34;StandardScaler + chia train/val/test\u0026#34;, } ) # Thêm các tệp tập dữ liệu huấn luyện, xác thực, và kiểm tra làm tài sản từ xa preprocessed_dataset_artifact.add_remote( uri=f\u0026#39;s3://{bucket_name}/{processed_data_prefix}\u0026#39;, logical_path=\u0026#39;split_data\u0026#39; ) # Ghi lại tập dữ liệu đã cập nhật vào thí nghiệm để theo dõi các cập nhật experiment_1.log_artifact(preprocessed_dataset_artifact) Quy trình thí nghiệm Comet và SageMaker AI Các nhà khoa học dữ liệu thích thí nghiệm nhanh; do đó, chúng tôi tổ chức quy trình làm việc thành các hàm tiện ích có thể tái sử dụng, có thể được gọi nhiều lần với các siêu tham số khác nhau trong khi duy trì ghi log và đánh giá nhất quán qua tất cả các lần chạy. Trong phần này, chúng tôi giới thiệu các hàm tiện ích cùng với một đoạn mã ngắn bên trong hàm:\ntrain() – Khởi tạo một công việc huấn luyện mô hình SageMaker bằng thuật toán XGBoost tích hợp sẵn của SageMaker: # Tạo bộ ước lượng SageMaker estimator = Estimator( image_uri=xgboost_image, role=execution_role, instance_count=1, instance_type=\u0026#39;ml.m5.large\u0026#39;, output_path=model_output_path, sagemaker_session=sagemaker_session_obj, hyperparameters=hyperparameters_dict, max_run=1800 # Thời gian huấn luyện tối đa tính bằng giây ) # Bắt đầu huấn luyện estimator.fit({ \u0026#39;train\u0026#39;: train_channel, \u0026#39;validation\u0026#39;: val_channel }) log_training_job() – Ghi lại siêu dữ liệu và số liệu huấn luyện và liên kết tài sản mô hình với thí nghiệm để đảm bảo khả năng truy xuất nguồn gốc hoàn chỉnh: # Ghi lại công việc huấn luyện SageMaker vào Comet\rlog_sagemaker_training_job_v1(\restimator=training_estimator,\rexperiment=api_experiment\r) log_model_to_comet() – Liên kết các artifact mô hình với Comet, ghi lại siêu dữ liệu huấn luyện, và liên kết tài sản mô hình với thí nghiệm để đảm bảo khả năng truy xuất nguồn gốc hoàn chỉnh: experiment.log_remote_model(\rmodel_name=model_name,\ruri=model_artifact_path,\rmetadata=metadata\r) deploy_and_evaluate_model() – Thực hiện triển khai và đánh giá mô hình, và ghi log số liệu: # Triển khai đến điểm cuối\rpredictor = estimator.deploy(\rinitial_instance_count=1,\rinstance_type=\u0026#34;ml.m5.xlarge\u0026#34;)\r# Ghi lại số liệu và hình ảnh hóa vào Comet\rexperiment.log_metrics(metrics)\rexperiment.log_confusion_matrix(matrix=cm, labels=[\u0026#39;Normal\u0026#39;, \u0026#39;Fraud\u0026#39;])\r# Ghi lại đường cong ROC\rfpr, tpr, _ = roc_curve(y_test, y_pred_prob_as_np_array)\rexperiment.log_curve(\u0026#34;roc_curve\u0026#34;, x=fpr, y=tpr) Mã dự đoán và đánh giá hoàn chỉnh có sẵn trong kho GitHub.\nChạy các thí nghiệm Bây giờ bạn có thể chạy nhiều thí nghiệm bằng cách gọi các hàm tiện ích với các cấu hình khác nhau và so sánh các thí nghiệm để tìm ra cài đặt tối ưu nhất cho trường hợp sử dụng phát hiện gian lận.\nĐối với thí nghiệm đầu tiên, chúng tôi thiết lập một đường cơ sở bằng cách sử dụng các siêu tham số XGBoost tiêu chuẩn:\n# Xác định siêu tham số cho thí nghiệm đầu tiên hyperparameters_v1 = { \u0026#39;objective\u0026#39;: \u0026#39;binary:logistic\u0026#39;, # Phân loại nhị phân \u0026#39;num_round\u0026#39;: 100, # Số vòng tăng cường \u0026#39;eval_metric\u0026#39;: \u0026#39;auc\u0026#39;, # Số liệu đánh giá \u0026#39;learning_rate\u0026#39;: 0.15, # Tỷ lệ học \u0026#39;booster\u0026#39;: \u0026#39;gbtree\u0026#39; # Thuật toán tăng cường } # Huấn luyện mô hình estimator_1 = train( model_output_path=f\u0026#34;s3://{bucket_name}/{model_output_prefix}/1\u0026#34;, execution_role=role, sagemaker_session_obj=sagemaker_session, hyperparameters_dict=hyperparameters_v1, train_channel_loc=train_channel_location, val_channel_loc=validation_channel_location ) # Ghi lại công việc huấn luyện và artifact mô hình log_training_job(experiment_key=experiment_1.get_key(), training_estimator=estimator_1) log_model_to_comet(experiment=experiment_1, model_name=\u0026#34;fraud-detection-xgb-v1\u0026#34;, model_artifact_path=estimator_1.model_data, metadata=metadata) # Triển khai và đánh giá deploy_and_evaluate_model(experiment=experiment_1, estimator=estimator_1, X_test_scaled=X_test_scaled, y_test=y_test ) Trong khi chạy một thí nghiệm Comet từ một sổ Jupyter, chúng ta cần kết thúc thí nghiệm để đảm bảo mọi thứ được ghi lại và lưu trữ trong máy chủ Comet. Xem mã sau: experiment_1.end()\nKhi thí nghiệm cơ sở hoàn tất, bạn có thể chạy các thí nghiệm bổ sung với các siêu tham số khác nhau. Kiểm tra sổ để xem chi tiết của cả hai thí nghiệm.\nKhi thí nghiệm thứ hai hoàn tất, điều hướng đến giao diện người dùng Comet để so sánh hai lần chạy thí nghiệm này.\nXem các thí nghiệm Comet trong giao diện người dùng Để truy cập giao diện người dùng, bạn có thể tìm URL trong IDE SageMaker Studio hoặc bằng cách thực thi mã được cung cấp trong sổ: experiment_2.url\nẢnh chụp màn hình sau đây cho thấy giao diện người dùng thí nghiệm Comet. Các chi tiết thí nghiệm chỉ nhằm mục đích minh họa và không đại diện cho một thí nghiệm phát hiện gian lận thực tế.\nĐiều này kết thúc thí nghiệm phát hiện gian lận.\nDọn dẹp Đối với phần thí nghiệm, cơ sở hạ tầng xử lý và huấn luyện SageMaker mang tính tạm thời và tự động tắt khi công việc hoàn tất. Tuy nhiên, bạn vẫn phải dọn dẹp thủ công một vài tài nguyên để tránh chi phí không cần thiết:\nTắt Không gian JupyterLab SageMaker sau khi sử dụng. Để biết hướng dẫn, hãy tham khảo Idle shutdown. Đăng ký Comet gia hạn dựa trên hợp đồng đã chọn. Hủy hợp đồng khi không còn yêu cầu gia hạn đăng ký Comet. Lợi ích của tích hợp SageMaker và Comet Sau khi đã thể hiện quy trình làm việc kỹ thuật, hãy xem xét các lợi ích rộng lớn hơn mà tích hợp này mang lại.\nPhát triển mô hình được đơn giản hóa Sự kết hợp giữa Comet và SageMaker giảm thiểu chi phí thủ công khi chạy các thí nghiệm ML. Trong khi SageMaker xử lý việc cung cấp và mở rộng cơ sở hạ tầng, việc ghi log tự động của Comet ghi lại các siêu tham số, số liệu, mã, thư viện đã cài đặt, và hiệu suất hệ thống từ các công việc huấn luyện của bạn mà không cần cấu hình bổ sung. Điều này giúp các đội nhóm tập trung vào phát triển mô hình thay vì quản lý sổ sách thí nghiệm. Khả năng hình ảnh hóa của Comet vượt xa các biểu đồ số liệu cơ bản. Các biểu đồ tích hợp cho phép so sánh thí nghiệm nhanh chóng, và các bảng Python tùy chỉnh hỗ trợ các công cụ phân tích đặc thù cho lĩnh vực để gỡ lỗi hành vi mô hình, tối ưu hóa siêu tham số, hoặc tạo các hình ảnh hóa chuyên biệt mà các công cụ tiêu chuẩn không thể cung cấp.\nHợp tác và quản trị doanh nghiệp Đối với các đội nhóm doanh nghiệp, sự kết hợp này tạo ra một nền tảng trưởng thành để mở rộng các dự án ML trong các môi trường được quy định. SageMaker cung cấp các môi trường ML nhất quán, an toàn, và Comet cho phép cộng tác liền mạch với việc theo dõi dòng dõi artifact và mô hình hoàn chỉnh. Điều này giúp tránh các sai lầm tốn kém xảy ra khi các đội nhóm không thể tái tạo lại các kết quả trước đó.\nTích hợp vòng đời ML hoàn chỉnh Không giống như các giải pháp điểm chỉ giải quyết việc huấn luyện hoặc giám sát, Comet kết hợp với SageMaker hỗ trợ vòng đời ML hoàn chỉnh của bạn. Các mô hình có thể được đăng ký trong sổ đăng ký mô hình của Comet với việc theo dõi phiên bản đầy đủ và quản trị. SageMaker xử lý triển khai mô hình, và Comet duy trì dòng dõi và quy trình phê duyệt để thăng cấp mô hình. Khả năng giám sát sản xuất của Comet theo dõi hiệu suất mô hình và sự trôi dạt dữ liệu sau khi triển khai, tạo ra một vòng lặp khép kín nơi các thông tin chi tiết từ sản xuất thông báo cho vòng thí nghiệm SageMaker tiếp theo của bạn.\nKết luận Trong bài đăng này, chúng tôi đã thể hiện cách sử dụng SageMaker và Comet cùng nhau để khởi tạo các môi trường ML được quản lý hoàn toàn với khả năng tái tạo và theo dõi thí nghiệm.\nĐể nâng cao quy trình làm việc SageMaker của bạn với khả năng quản lý thí nghiệm toàn diện, triển khai Comet trực tiếp trong môi trường SageMaker của bạn thông qua AWS Marketplace, và chia sẻ phản hồi của bạn trong phần bình luận.\nĐể biết thêm thông tin về các dịch vụ và tính năng được thảo luận trong bài đăng này, hãy tham khảo các tài nguyên sau:\nThiết lập Ứng dụng AI Đối tác Khởi động nhanh với Comet Sổ trên GitHub Tài liệu Comet Nền tảng mã nguồn mở Opik để quan sát LLM Về các tác giả Vikesh Pandey là một Kiến trúc sư Giải pháp Chuyên gia GenAI/ML tại AWS, giúp các tổ chức tài chính lớn áp dụng và mở rộng khối lượng công việc GenAI và ML. Ông là tác giả của cuốn sách “Generative AI for financial services.” Ông có hơn 15 năm kinh nghiệm xây dựng các ứng dụng cấp doanh nghiệp trên GenAI/ML và các công nghệ liên quan. Trong thời gian rảnh rỗi, ông chơi một môn thể thao không tên với con trai mình, nằm giữa bóng đá và bóng bầu dục.\nNaufal Mir là một Kiến trúc sư Giải pháp Chuyên gia GenAI/ML tại AWS. Ông tập trung vào việc giúp khách hàng xây dựng, huấn luyện, triển khai và di chuyển khối lượng công việc học máy sang SageMaker. Trước đây, ông làm việc tại các tổ chức tài chính phát triển và vận hành các hệ thống quy mô lớn. Ngoài công việc, ông thích chạy bộ và đạp xe đường dài.\nSarah Ostermeier là Quản lý Tiếp thị Sản phẩm Kỹ thuật tại Comet. Bà chuyên đưa các sản phẩm dành cho nhà phát triển GenAI và ML của Comet đến với các kỹ sư cần chúng thông qua nội dung kỹ thuật, tài nguyên giáo dục, và thông điệp sản phẩm. Trước đây, bà đã làm việc với tư cách là kỹ sư ML, nhà khoa học dữ liệu, và quản lý thành công khách hàng, giúp khách hàng triển khai và mở rộng các giải pháp AI. Ngoài công việc, bà thích đi du lịch ngoài lề, viết về AI, và đọc khoa học viễn tưởng.\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/1-worklog/",
	"title": "Nhật ký công việc",
	"tags": [],
	"description": "",
	"content": "Tuần 1: Làm quen với các dịch vụ cơ bản của AWS, các thành viên trong nhóm và quyết định dự án đầu tiên.\nTuần 2: Tìm hiểu chuyên sâu về NLP. Làm việc nhóm để xác định tính năng và phân chia nhiệm vụ cho dự án.\nTuần 3: Học React.js và áp dụng kiến thức để phát triển trang quản lý sản phẩm. Tìm hiểu về containerization để chuẩn bị cho giai đoạn triển khai.\nTuần 4: Tập trung phát triển các trang quản lý bán hàng.\nTuần 5: Hoàn thiện trang quản lý bán hàng. Dịch ba bài blog kỹ thuật.\nTuần 6: Ôn tập và học thêm về các dịch vụ và kiến trúc AWS để chuẩn bị cho bài kiểm tra giữa kỳ.\nTuần 7: Tiếp tục học các dịch vụ và kiến trúc AWS để chuẩn bị cho bài kiểm tra giữa kỳ.\nTuần 8: Nghiên cứu kiến trúc và giải pháp AWS. Làm bài kiểm tra giữa kỳ.\nTuần 9: Chuẩn bị nguồn dữ liệu và kiến thức nền tảng cho hệ thống gợi ý.\nTuần 10: Thu thập, tạo mô phỏng dữ liệu và tiền xử lý cho mô hình gợi ý.\nTuần 11: Thiết kế, tinh chỉnh và huấn luyện mô hình gợi ý.\nTuần 12: Tích hợp hệ thống đề xuất vào ứng dụng. Chuẩn bị triển khai dự án lên hạ tầng AWS.\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/1-worklog/1.1-week1/",
	"title": "Nhật ký công việc Tuần 1",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 1: Nắm bắt các chính sách của AWS và quy tắc ứng xử tại môi trường làm việc chuyên nghiệp. Làm quen với các thành viên trong chương trình First Cloud Journey. Thành lập một đội nhóm và quyết định kế hoạch cho dự án. Tìm hiểu về các dịch vụ AWS cơ bản và kiến thức chuyên ngành để làm dự án. Các nhiệm vụ cần triển khai trong tuần này: Thứ Nhiệm vụ Ngày Nguồn tài liệu 2 - Nắm rõ các quy định và hướng dẫn thực tập tại AWS FCJ.\n- Làm quen với các hướng dẫn viên và các bạn sinh viên trong FCJ.\n- Thành lập một nhóm gồm năm thành viên. 08/09/2025 Nội quy và Hướng dẫn của FCJ 3 - Làm quen với môi trường làm việc tại văn phòng AWS.\n- Học cách làm workshop AWS và bắt đầu ghi worklog cho tuần đầu tiên. 09/09/2025 Cách Làm Workshop AWS 4 - Bắt đầu học các bài giảng từ danh sách phát First Cloud Journey Bootcamp:\n+ Công nghệ điện toán đám mây của AWS\n+ Tạo và quản lý tài khoản AWS\n+ AWS Virtual Private Cloud (VPC) 10/09/2025 First Cloud Journey Bootcamp - 2025 5 - Hoàn thành các bài giảng trong FCJ Bootcamp:\n+ Dịch vụ máy ảo trên AWS\n+ Dịch vụ bảo mật trên AWS\n+ Dịch vụ cơ sở dữ liệu trên AWS 11/09/2025 First Cloud Journey Bootcamp - 2025 6 - Họp với nhóm để lập kế hoạch cho dự án đầu tiên.\n- Hoàn thành worklog của tuần với các kết quả đạt được. 12/09/2025 Kết quả đạt được tuần 1: Đã tìm hiểu và nắm rõ các chính sách, quy định của chương trình thực tập FCJ. Đến văn phòng AWS lần đầu, làm quen với môi trường làm việc và không gian tại đây. Làm quen với các hướng dẫn viên và các học viên trong FCJ, lập nhóm 5 thành viên để thực hiện dự án. Bắt đầu viết báo cáo thực tập và hoàn thành nhật ký công việc tuần đầu tiên. Hoàn thành toàn bộ bài học trong danh sách phát First Cloud Journey Bootcamp: Nắm được các khái niệm cơ bản về những dịch vụ của AWS. Tạo thành công tài khoản AWS bậc miễn phí. Thực hành quản lý các dịch vụ bằng tài khoản AWS. Họp với nhóm để lên kế hoạch cho dự án đầu tiên, xác định tên dự án, mục tiêu và thời gian thực hiện. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/5-workshop/5.2-prerequiste/",
	"title": "Các bước chuẩn bị",
	"tags": [],
	"description": "",
	"content": "IAM permissions Gắn IAM permission policy sau vào tài khoản aws user của bạn để triển khai và dọn dẹp tài nguyên trong workshop này. Do tôi đang thực hiện workshop này lần đầu nên tôi sẽ gắn full quyền cho các quyền tôi đã chọn\n{ { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;polly:*\u0026#34;, \u0026#34;s3:*\u0026#34;, \u0026#34;logs:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } } "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/5-workshop/5.4-lambda/create-lambda/",
	"title": "Tạo Lambda",
	"tags": [],
	"description": "",
	"content": "Tạo hàm Lambda Truy cập vào Lambda management console Tại giao diện Dashboard, chọn Create function Trong giao diện Create Lambda function Name the lambda: chọn một tên chưa trùng lặp (gợi ý: số bài lab và tên của bạn) Runtime: chọn python 3.13 hoặc mới nhất Mở rộng phần Change default execution role Tại mục Execution role, chọn Use an existing role và chọn PollyLambdaRole Kéo xuống dưới và nhấn Create function (hoặc Create lambda) Tạo thành công Lambda function. Tạo code Lambda Trong bài thực hành này, chúng ta sẽ sử dụng code để chuyển đổi một thư mục văn bản thành thư mục giọng nói (Text-to-Speech) bằng dịch vụ Amazon Polly. Kéo xuống phần Code source. Xóa tất cả code hiện có trong file lambda_function.py. Chúng ta sẽ sử dụng đoạn code như sau: import json import boto3 import os s3 = boto3.client(\u0026#39;s3\u0026#39;) polly = boto3.client(\u0026#39;polly\u0026#39;) def lambda_handler(event, context): try: # 1. Lấy thông tin file vừa upload record = event[\u0026#39;Records\u0026#39;][0] bucket_name = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] object_key = record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] # VD: input/hello.txt print(f\u0026#34;Processing file: {object_key}\u0026#34;) # 2. Đọc nội dung file text file_obj = s3.get_object(Bucket=bucket_name, Key=object_key) text_content = file_obj[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) # 3. Gọi Polly để chuyển đổi văn bản sang giọng nói (Giọng: Joanna) response = polly.synthesize_speech( Text=text_content, OutputFormat=\u0026#39;mp3\u0026#39;, VoiceId=\u0026#39;Joanna\u0026#39; ) # 4. Lưu file MP3 sang thư mục output # Đổi tên từ input/abc.txt thành output/abc.mp3 new_key = object_key.replace(\u0026#34;input/\u0026#34;, \u0026#34;output/\u0026#34;).replace(\u0026#34;.txt\u0026#34;, \u0026#34;.mp3\u0026#34;) if \u0026#34;AudioStream\u0026#34; in response: with response[\u0026#34;AudioStream\u0026#34;] as stream: s3.put_object( Bucket=bucket_name, Key=new_key, Body=stream.read(), ContentType=\u0026#39;audio/mpeg\u0026#39; ) return \u0026#34;Done!\u0026#34; except Exception as e: print(e) raise e Nhấn nút Deploy. Tạo Trigger Ngay phía trên phần code, nhấn nút + Add trigger. Source: Chọn S3. Bucket: Chọn bucket s3-demo-text. Event types: Chọn All object create events. Prefix: Nhập input/ ⚠️ Lưu ý: Bạn bắt buộc phải nhập input/. Nếu để trống, Lambda sẽ kích hoạt ngay cả khi file MP3 được tạo ra -\u0026gt; Gây ra vòng lặp vô tận -\u0026gt; Tăng chi phí. Suffix: Nhập .txt Tích vào ô \u0026ldquo;I acknowledge\u0026hellip;\u0026rdquo; -\u0026gt; Nhấn Add. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/1-worklog/1.2-week2/",
	"title": "Nhật ký công việc Tuần 2",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 2: Nắm bắt kiến thức về xử lý ngôn ngữ tự nhiên để hỗ trợ phát triển dự án. Thảo luận với nhóm để xác định và ưu tiên các tính năng sẽ được triển khai trong ứng dụng của dự án. Các nhiệm vụ cần triển khai trong tuần này: Thứ Nhiệm vụ Ngày Nguồn tài liệu 2 - Học về phân loại và không gian vector trong xử lý ngôn ngữ tự nhiên.\n+ Phân tích cảm xúc\n+ Mô hình không gian vector\n+ Dịch máy\n+ Tìm kiếm tài liệu 09/15/2025 Xử lý Ngôn ngữ tự nhiên với Phân loại và Không gian Vector 3 - Đến văn phòng AWS và thảo luận với nhóm về các tính năng sẽ triển khai trong ứng dụng.\n- Học về mô hình xác suất cho NLP.\n+ Tự động sửa lỗi và Tự động hoàn thành\n+ Mô hình Markov ẩn\n+ Mô hình ngôn ngữ\n+ Nhúng từ 09/16/2025 Xử lý Ngôn ngữ tự nhiên với Mô hình Xác suất 4 - Học về mạng thần kinh hồi quy cho NLP.\n+ Đơn vị hồi quy có cổng\n+ Đơn vị bộ nhớ dài-ngắn hạn\n+ Nhận diện thực thể có tên\n+ Mạng Xiêm 09/17/2025 Xử lý Ngôn ngữ tự nhiên với Mô hình Chuỗi 5 -Học về mô hình chú ý.\n+ Dịch máy với mạng thần kinh\n+ Tóm tắt văn bản\n+ Trả lời câu hỏi 09/18/2025 Xử lý Ngôn ngữ tự nhiên với Mô hình Chú ý 6 - Họp nhóm để phân công nhiệm vụ và đặt thời hạn.\n- Học cú pháp và tính năng cơ bản của JavaScript. 09/19/2025 Tương tác với JavaScript Kết quả đạt được tuần 2: Nắm được kiến thức của chuyên đề \u0026ldquo;Xử lý Ngôn ngữ tự nhiên\u0026rdquo; trên Coursera, bao gồm: Phân loại và không gian vector Mô hình xác suất Mô hình chuỗi Mô hình chú ý Làm việc với nhóm tại văn phòng AWS để xác định và ưu tiên các tính năng chính cho ứng dụng dự án. Đã phân công nhiệm vụ và đặt thời hạn với nhóm. Tiếp thu kiến thức nền tảng về cú pháp và tính năng JavaScript qua khóa học “Tương tác với JavaScript” trên Coursera. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/2-proposal/",
	"title": "Bản đề xuất",
	"tags": [],
	"description": "",
	"content": "Bản Word\nMealPlan Nền tảng Bán Nguyên liệu Thực phẩm Cá nhân hóa 1. Tóm tắt điều hành Nền tảng Bán Nguyên liệu Thực phẩm Cá nhân hóa tập trung vào việc cho phép mua sắm nhanh hơn và hiệu quả hơn. Người dùng đăng ký tài khoản để truy cập cơ sở dữ liệu công thức đa dạng, nhận đề xuất bữa ăn dựa trên AI dựa trên lịch sử mua hàng và đặt hàng giao tận nhà. Tận dụng cơ sở hạ tầng đám mây AWS, nền tảng đảm bảo khả năng mở rộng linh hoạt, hiệu suất cao và quản lý an toàn.\n2. Tuyên bố vấn đề Vấn đề hiện tại\nKhách hàng thường mất nhiều thời gian tìm kiếm những bữa ăn phù hợp với nhu cầu hàng ngày. Mặc dù nhiều nền tảng cung cấp gợi ý về bữa ăn hoặc thực đơn, nhưng hầu hết đều không hỗ trợ mua các món ăn hoàn chỉnh, chế biến sẵn, buộc người dùng phải tự tìm kiếm nhà hàng hoặc nhà cung cấp những bữa ăn đó.\nGiải pháp\nNền tảng sử dụng Spring Boot để xây dựng một backend ổn định với các API REST cho tài khoản người dùng, công thức nấu ăn, giỏ hàng và đơn hàng. Frontend được xây dựng bằng React và cung cấp các đề xuất bữa ăn dựa trên AI dễ sử dụng. Dữ liệu được lưu trữ trong AWS RDS (PostgreSQL), trong khi hình ảnh và tệp tĩnh được lưu trữ trên Amazon S3. Backend chạy trên Amazon EC2 bên trong một VPC an toàn và Route 53 được sử dụng để quản lý tên miền.\nLợi ích và hoàn vốn đầu tư\nGiải pháp này thiết lập một nền tảng toàn diện cho công ty khởi nghiệp về dinh dưỡng để mở rộng dịch vụ, đồng thời thu thập dữ liệu người dùng cho các hệ thống khuyến nghị nâng cao. Chi phí là 119,51 đô la Mỹ/tháng và 1.434,12 đô la Mỹ/12 tháng. Quá trình phát triển tận dụng các khuôn khổ mã nguồn mở, không phát sinh thêm chi phí phần cứng.\n3. Kiến trúc giải pháp Trang web được host trên EC2. Dữ liệu được lưu trữ bằng EC2 instance. Hình ảnh được lưu trên S3. Code sẽ dược đẩy lên github nhằm quản lý và tự động đẩy code lên s3 để CodeDeploy sẽ thực hiện deploy lên server. Cloudfront được sử dụng nhằm cải thiện tốc tải. Cognito dùng để quản lý danh tính người dùng. CloudTrail được dùng để giám sát và lữu trữ lịch sử hoạt động. CloudWatch dùng để giám sát và quản lý hiệu suất, tình trạng hoạt động của các tài nguyên và ứng dụng trên AWS. IAM dùng để cấp quyền cho các service. SecretManager được dùng nhằm quản lý các thông tin nhạy cảm.\nDịch vụ AWS sử dụng\nWAF: Bảo vệ ứng dụng web khỏi các tấn công mạng AWS CloudFront: Tăng tốc độ tải trang web. AWS EC2: Deploy sản phẩm, NAT instance, Database. AWS VPC: là mạng ảo. AWS S3: Lưu trữ code, file log, hình ảnh. CodeDeploy: Deploy code lên EC2. GitLab: chứa source code và push code lên s3. Amazon Cognito: Quản lý quyền truy cập cho người dùng trang web. IAM: Tạo user và role. Secret Manager: Chứa các thông tin quan trọng. CloudTrail: Giám sát và lữu trữ lịch sử hoạt động. CloudWatch: Giám sát và quản lý hiệu suất, tình trạng hoạt động của các tài nguyên và ứng dụng trên AWS 4. Triển khai kỹ thuật Các giai đoạn triển khai\nCác giai đoạn triển khai Dự án này gồm hai phần: phát triển back-end Spring Boot và front-end React, triển khai website lên AWS bằng các dịch vụ AWS, mỗi phần gồm 4 giai đoạn.\nXây dựng Lý thuyết và Vẽ Kiến trúc: Thu thập các yêu cầu cho ứng dụng web, thiết kế kiến ​​trúc hệ thống (Spring Boot REST API + React front-end) và định nghĩa lược đồ cơ sở dữ liệu (Tháng 1).\nPhát triển, Kiểm thử: Triển khai back-end Spring Boot với các REST API (xác thực, quản lý người dùng, CRUD món ăn/công thức, giỏ hàng, v.v.) và xây dựng front-end React (UI/UX, định tuyến, biểu mẫu, quản lý trạng thái). Thực hiện các bài kiểm tra đơn vị cho các dịch vụ back-end, kiểm tra tích hợp cho các điểm cuối API và kiểm tra front-end (Thư viện Kiểm tra Jest/React). (Tháng 1–2)\nTính toán Giá và Kiểm tra Tính khả thi: Sử dụng Công cụ Tính giá AWS để ước tính chi phí cho EC2 (lưu trữ back-end), RDS (cơ sở dữ liệu), S3 (tệp và hình ảnh tĩnh), VPC (mạng), Route53 (tên miền) sau đó điều chỉnh nếu cần (Tháng 2).\nTích hợp AWS: Tích hợp các dịch vụ AWS vào ứng dụng. Triển khai trang web trên EC2, lưu trữ hình ảnh trên S3, cấu hình RDS cho cơ sở dữ liệu, sử dụng VPC để quản lý mạng, Route53 cho miền và thiết lập các pipeline CI/CD (GitHub Actions hoặc AWS CodePipeline). Thực hiện kiểm thử dàn dựng trước khi phát hành chính thức. (Tháng 3)\nYêu cầu kỹ thuật\nBack-end (Spring Boot): REST API để xác thực, quản lý người dùng, CRUD món ăn/công thức nấu ăn, giỏ hàng và xử lý đơn hàng. Bao gồm bảo mật (JWT, Spring Security). Front-end (React): Ứng dụng web đáp ứng với UI/UX thân thiện với người dùng, tích hợp API với back-end Spring Boot. Cơ sở dữ liệu (EC2): Cơ sở dữ liệu quan hệ (MySQL/PostgreSQL) để lưu trữ người dùng, công thức nấu ăn, giỏ hàng và dữ liệu đơn hàng được host trên EC2. Lưu trữ (AWS S3): Được sử dụng để lưu trữ hình ảnh do người dùng tải lên Lưu trữ \u0026amp; Mạng (AWS EC2 \u0026amp; AWS VPC): Ứng dụng được triển khai trên các phiên bản EC2. 6, CI/CD (GitHub Actions hoặc AWS CodePipeline): Quy trình xây dựng, triển khai tự động cho cả back-end và front-end. Xác thực \u0026amp; Bảo mật: Xác thực JWT và cấu hình HTTPS; AWS Cognito tùy chọn để quản lý quyền truy cập của người dùng. 5. Lộ trình \u0026amp; Mốc triển khai Tháng 1: Xây dựng lý thuyết và vẽ kiến ​​trúc (backend Spring Boot + thiết kế frontend React, lược đồ cơ sở dữ liệu). Bắt đầu phát triển ban đầu backend và frontend.\nTháng 2: Tiếp tục phát triển backend và frontend, thực hiện kiểm thử đơn vị và tích hợp. Sử dụng Công cụ tính giá AWS để đánh giá chi phí lưu trữ và tinh chỉnh kiến ​​trúc nhằm đảm bảo hiệu quả chi phí.\nTháng 3: Tích hợp các dịch vụ AWS, cấu hình các pipeline CI/CD, tiến hành kiểm thử dàn dựng và triển khai website lên môi trường production.\nSau khi ra mắt: Tối đa 3 tháng để bảo trì, tối ưu hóa và cải tiến tính năng\n6. Ước tính ngân sách Có thể xem chi phí trên AWS Pricing Calculator\nDịch Vụ AWS (AWS Services) AWS WAF: $11.6/tháng Application Load Balancer (ALB): $18.63/tháng Amazon EC2 Application: $19.27/tháng Amazon EC2 Data Tier: $9.64/tháng Amazon EC2 Nat Instances: $19.27/tháng Amazon S3: $3.72/tháng AWS CodeDeploy: 0$ AWS Secrets Manager: $0.4/tháng Amazon Cognito: $14.25/tháng Amazon CloudWatch: $4.91/tháng AWS CloudTrail: $1.77/tháng VPC Endpoints: $16.05/tháng Total: $119.51/tháng, $1434.12/12 tháng 7. Đánh giá rủi ro Ma trận rủi ro\n⦁\tMất kết nối mạng: Ảnh hưởng trung bình / Xác suất trung bình. ⦁\tHỏng/Treo EC2 / ALB / AZ: Ảnh hưởng cao / Xác suất thấp–trung bình. ⦁\tRò rỉ bí mật: Ảnh hưởng cao / Xác suất thấp–trung bình. ⦁\tVượt chi phí: Ảnh hưởng trung bình / Xác suất trung bình.\nChiến lược giảm thiểu\n⦁\tTính khả dụng: Multi-AZ + Auto Scaling + ALB; health checks; caching bằng CloudFront; Route 53 failover. Sử dụng caching của CloudFront để giảm sự phụ thuộc vào dịch vụ backend. ⦁\tBảo mật: IAM theo nguyên tắc ít quyền nhất (least-privilege); Secrets Manager với cơ chế xoay vòng (rotation) và ghi nhật ký truy cập; bảo vệ bằng WAF/Shield. ⦁\tQuản lý chi phí: AWS Budgets \u0026amp; Cost Explorer; điều chỉnh kích thước phù hợp (rightsizing); sử dụng Reserved hoặc Spot instances khi phù hợp.\nKế hoạch dự phòng\n⦁\tTự động rollback thông qua CodeDeploy. ⦁\tDự phòng thủ công: GitLab runner → prebuilt AMIs hoặc ASGs\n8. Kết quả kỳ vọng ⦁\tCI/CD hoàn toàn tự động (GitLab → CodePipeline → CodeBuild → CodeDeploy) giảm lỗi thủ công. ⦁\tBảo mật nhiều lớp (IAM, WAF, Secrets Manager) với audit qua CloudTrail.\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Báo cáo tổng hợp: “DevOps trên AWS” Mục tiêu Sự kiện Hiểu các nguyên tắc cốt lõi của văn hóa DevOps và các chỉ số hiệu suất chính (DORA, MTTR). Nắm vững chuỗi công cụ CI/CD của AWS để tự động hóa việc xây dựng (build), kiểm thử (test) và triển khai (deployment). Tìm hiểu các khái niệm Cơ sở hạ tầng dưới dạng Mã (IaC) sử dụng CloudFormation và AWS CDK. Khám phá các chiến lược container hóa sử dụng ECR, ECS, EKS và App Runner. Triển khai khả năng quan sát toàn diện (full-stack observability) và giám sát với CloudWatch và X-Ray. Diễn giả Các Chuyên gia DevOps của AWS Các Kiến trúc sư Giải pháp Cấp cao Điểm nhấn Chính Văn hóa \u0026amp; Tư duy DevOps Tóm tắt: Tích hợp với các khái niệm AI/ML từ các phiên trước. Các chỉ số quan trọng: Tập trung vào Tần suất triển khai, Thời gian thực hiện thay đổi, Thời gian trung bình để khôi phục (MTTR) và Tỷ lệ lỗi khi thay đổi (các chỉ số DORA). Chuyển đổi văn hóa: Chuyển từ các nhóm làm việc riêng lẻ (siloed) sang mô hình chia sẻ trách nhiệm. Đường ống CI/CD trên AWS Kiểm soát nguồn (Source Control): Sử dụng AWS CodeCommit và triển khai các chiến lược Git như GitFlow và Trunk-based. Xây dựng \u0026amp; Kiểm thử: Cấu hình AWS CodeBuild để kiểm thử và biên dịch tự động. Triển khai: Sử dụng AWS CodeDeploy để thực hiện các chiến lược triển khai an toàn: Blue/Green: Giảm thiểu thời gian ngừng hoạt động và rủi ro. Canary: Triển khai dần dần cho một nhóm nhỏ người dùng. Rolling: Cập nhật các instance theo từng bước. Điều phối: Kết nối tất cả các khâu với AWS CodePipeline Cơ sở hạ tầng dưới dạng Mã (IaC) AWS CloudFormation: Định nghĩa hạ tầng bằng template, stack và quản lý sự sai lệch cấu hình (drift). AWS CDK (Cloud Development Kit): Sử dụng ngôn ngữ lập trình quen thuộc để định nghĩa tài nguyên đám mây dưới dạng các cấu trúc mã (constructs). So sánh: Lựa chọn giữa template khai báo (CloudFormation) và mã mệnh lệnh (CDK) dựa trên kỹ năng của đội ngũ. Dịch vụ Container \u0026amp; Khả năng quan sát Quản lý Container: Lưu trữ image trong Amazon ECR với các chính sách vòng đời. Điều phối: Lựa chọn giữa Amazon ECS (đơn giản, native AWS) và Amazon EKS (chuẩn Kubernetes), hoặc AWS App Runner để triển khai đơn giản kiểu PaaS. Giám sát: Sử dụng Amazon CloudWatch cho các chỉ số/cảnh báo và AWS X-Ray cho truy vết phân tán để xác định điểm nghẽn hiệu năng. Bài học Chính (Key Takeaways) Chiến lược DevOps Tự động hóa là ưu tiên: Triển khai thủ công dễ gặp lỗi; mọi thứ từ hạ tầng đến triển khai mã nguồn cần được tự động hóa. Đo lường: Bạn không thể cải thiện những gì bạn không đo lường. Sử dụng chỉ số DORA để theo dõi tốc độ và sự ổn định. Dịch chuyển sang trái (Shift Left): Tích hợp kiểm thử và bảo mật sớm trong pipeline CI/CD, thay vì để ở cuối quy trình. Kiến trúc Kỹ thuật Hạ tầng bất biến (Immutable Infrastructure): Coi máy chủ là tài nguyên dùng một lần; thay thế chúng thay vì vá lỗi tại chỗ. Container hóa: Tách biệt ứng dụng khỏi hệ điều hành bên dưới để đảm bảo tính nhất quán giữa các môi trường (Dev, Test, Prod). Khả năng quan sát: Vượt ra ngoài việc giám sát \u0026ldquo;bật/tắt\u0026rdquo; đơn giản để có thông tin sâu sắc bằng cách sử dụng truy vết phân tán. Ứng dụng vào Công việc Triển khai CI/CD: Thiết lập CodePipeline cho các dự án hiện tại để tự động hóa quy trình build/deploy cho ứng dụng Spring Boot/React. Áp dụng IaC: Bắt đầu định nghĩa tài nguyên AWS (database, S3 bucket, Cognito User Pools) bằng AWS CDK thay vì dùng console. Container hóa: Dockerize các microservices hiện có và đẩy image lên ECR. Nâng cao giám sát: Thêm X-Ray vào các dịch vụ backend để trực quan hóa độ trễ API và hiệu năng truy vấn cơ sở dữ liệu. Trải nghiệm Sự kiện Tham dự hội thảo “DevOps on AWS” đã cung cấp một lộ trình thực tế để tự động hóa vòng đời phân phối phần mềm. Nó thu hẹp khoảng cách giữa việc viết code và chạy nó một cách đáng tin cậy trên môi trường production (thực tế). Các trải nghiệm chính bao gồm:\nHọc hỏi từ chuyên gia Làm rõ \u0026ldquo;Ma trận thuật ngữ\u0026rdquo; của các công cụ AWS (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) và cách chúng tích hợp. Hiểu giá trị chiến lược của chỉ số DORA trong việc chứng minh hiệu quả đầu tư DevOps với các bên liên quan. Tiếp cận kỹ thuật thực tế CI/CD Walkthrough: Bản demo pipeline đầy đủ cho thấy chính xác cách thay đổi code kích hoạt build và deploy tự động. IaC Implementation: Xem AWS CDK hoạt động là một điểm nhấn—viết hạ tầng bằng Java/TypeScript trực quan hơn nhiều cho lập trình viên so với viết template JSON/YAML. Deployment Strategies: Trực quan hóa Blue/Green deployments đã chứng minh cách phát hành bản cập nhật mà không có thời gian chết (zero downtime). Kết nối và thảo luận Thảo luận về sự đánh đổi giữa ECS và EKS với đồng nghiệp, nhận ra rằng đối với nhiều dự án, ECS hoặc App Runner cung cấp con đường nhanh hơn để ra production với ít chi phí vận hành hơn. Trao đổi ý tưởng về cách xử lý database migrations (di chuyển dữ liệu) trong một pipeline CI/CD. Bài học rút ra Phát hiện sai lệch (Drift Detection) trong CloudFormation là rất quan trọng để duy trì tính toàn vẹn của hạ tầng. Khả năng quan sát (Observability) không phải là tùy chọn đối với microservices; nếu không có X-Ray, việc gỡ lỗi kiến trúc phân tán gần như là không thể. Một nền tảng DevOps vững chắc giúp giảm đáng kể thời gian trung bình để khôi phục (MTTR), cho phép các team đổi mới nhanh hơn mà ít sợ làm hỏng hệ thống production. Ảnh sự kiện "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/1-worklog/1.3-week3/",
	"title": "Nhật ký công việc Tuần 3",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 3: Nắm bắt kiến thức cơ bản về phát triển giao diện người dùng với React.js để hỗ trợ phát triển ứng dụng dự án. Hợp tác với nhóm để phát triển các thành phần chính của ứng dụng web dự án, bao gồm các tính năng của bảng điều khiển người bán. Tìm hiểu về các khái niệm container hóa để chuẩn bị cho việc triển khai dự án. Các nhiệm vụ cần triển khai trong tuần này: Thứ Nhiệm vụ Ngày Nguồn tài liệu 2 Học các khái niệm cơ bản về phát triển ứng dụng với React.js. 22/09/2025 Phát triển Ứng dụng Front-End với React 3 - Đến văn phòng AWS và bắt đầu làm việc với dự án đầu tiên.\n+ Phát triển trang quản lý sản phẩm cho ứng dụng 23/09/2025 4 - Tiếp tục làm việc với dự án.\n+ Phát triển trang quản lý đơn hàng 24/09/2025 5 - Hoàn thành nhiệm vụ đầu tiên trong dự án.\n+ Hoàn thiện bảng điều khiển người bán với bảng tổng quan 25/09/2025 6 - Họp nhóm để đánh giá tiến độ và lập kế hoạch cho các nhiệm vụ tiếp theo.\n- Tìm hiểu về khái niệm container hóa.\n+ Docker\n+ Kubernetes\n+ OpenShift\n+ Istio 26/09/2025 Giới thiệu về Container với Docker, Kubernetes và OpenShift Kết quả đạt được tuần 3: Nắm được kiến thức về React.js thông qua khóa học “Phát triển Ứng dụng Front-End với React” trên Coursera, bao gồm: Tạo và quản lý các thành phần (components) và thuộc tính (props) Áp dụng trạng thái (state) và phương thức vòng đời (lifecycle methods) Xử lý sự kiện để tạo giao diện người dùng tương tác Phát triển bảng điều khiển người bán cho ứng dụng của dự án, bao gồm: Quản lý sản phẩm Quản lý đơn hàng Bảng tổng quan Tìm hiểu về container hóa thông qua khóa học “Giới thiệu về Container với Docker, Kubernetes và OpenShift” trên Coursera, bao gồm: Kiến thức cơ bản về Docker Quản lý và điều phối với Kubernetes Căn bản của OpenShift Đánh giá tiến độ dự án cùng nhóm, phân công các nhiệm vụ mới. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/3-blogstranslated/",
	"title": "Các bài blogs đã dịch",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Getting started with healthcare data lakes: Using microservices Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 2 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 3 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/1-worklog/1.4-week4/",
	"title": "Nhật ký công việc Tuần 4",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 4: Củng cố kiến thức về React.js thông qua việc phát triển các thành phần cho trang quản lý bán hàng. Xây dựng các trang quản lý như quản lý đơn hàng và dashboard. Đánh giá tiến độ và phân chia nhiệm vụ cho nhóm. Các nhiệm vụ cần triển khai trong tuần này: Thứ Nhiệm vụ Ngày Nguồn tài liệu 2 - Phát triển trang quản lý đơn hàng cho trang bán hàng. 29/09/2025 3 - Tiếp tục phát triển trang quản lý đơn hàng. 30/09/2025 4 - Phát triển dashboard cho trang bán hàng. 01/10/2025 5 - Hoàn thành trang quản lý đơn hàng và dashboard. 02/10/2025 6 - Họp nhóm để đánh giá tiến độ và xác định nhiệm vụ cho tuần tiếp theo.\n- Xây dựng trang quản lý kho (quản lý nguyên liệu). 03/10/2025 Kết quả đạt được tuần 4: Hoàn thiện và tích hợp trang quản lý đơn hàng cho seller panel, cho phép người bán theo dõi, cập nhật và quản lý đơn hàng. Xây dựng trang dashboard giúp hiển thị các chỉ số về doanh số, số lượng đơn hàng và thống kê sản phẩm. Triển khai trang quản lý kho giúp người bán theo dõi lượng hàng tồn và quản lý nguyên liệu. Phối hợp cùng nhóm để đánh giá tiến độ, trao đổi ý tưởng cải thiện thiết kế và lập kế hoạch cho tuần kế tiếp. Nâng cao kỹ năng thực hành trong việc xây dựng các thành phần front-end bằng React.js. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/4-eventparticipated/",
	"title": "Sự kiện đã tham gia",
	"tags": [],
	"description": "",
	"content": "Trong kỳ thực tập của mình, em đã tham gia bốn sự kiện. Mỗi sự kiện đều là một trải nghiệm đáng nhớ, mang lại những kiến thức mới mẻ, thú vị và bổ ích, cùng với những món quà và những khoảnh khắc tuyệt vời.\nSự kiện 1 Tên sự kiện: AWS Cloud Mastery Series #1\nThời gian: 08:00, ngày 15 tháng 11 năm 2025\nĐịa điểm: Tầng 26, Tòa nhà Bitexco, số 02 Hải Triều, Quận 1, TP. Hồ Chí Minh\nVai trò: Người tham dự\nSự kiện 2 Tên sự kiện: AWS Cloud Mastery Series #2\nThời gian: 08:30, ngày 17 tháng 11 năm 2025\nĐịa điểm: Tầng 26, Tòa nhà Bitexco, số 02 Hải Triều, Quận 1, TP. Hồ Chí Minh\nVai trò: Người tham dự\nSự kiện 3 Tên sự kiện: AWS Cloud Mastery Series #3\nThời gian: 08:30, ngày 29 tháng 11 năm 2025\nĐịa điểm: Tầng 26, Tòa nhà Bitexco, số 02 Hải Triều, Quận 1, TP. Hồ Chí Minh\nVai trò: Người tham dự\nSự kiện 4 Tên sự kiện: Building Agentic AI \u0026amp; Context Optimization with Amazon Bedrock\nThời gian: 09:00, ngày 05 tháng 12 năm 2025\nĐịa điểm: Tầng 26, Tòa nhà Bitexco, số 02 Hải Triều, Quận 1, TP. Hồ Chí Minh\nVai trò: Người tham dự\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/5-workshop/5.5-testing/",
	"title": "Kiểm tra",
	"tags": [],
	"description": "",
	"content": "Tạo file đầu vào Trên máy tính của bạn, tạo một file có tên là test.txt. Tạo file với nội dung: \u0026ldquo;Hello, congratulations on completing the workshop!\u0026rdquo; Lưu file lại. Quay lại tab S3, mở thư mục input. Nhấn Upload -\u0026gt; Chọn file test.txt -\u0026gt; Nhấn Upload Chờ khoảng 5 giây.\nQuay lại giao diện Bucket, sau đó mở thư mục output. Bạn sẽ thấy file test.mp3. Chọn nó và nhấn Download.\nMở file lên để nghe thử.\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nHệ thống chuyển đổi Văn bản sang Giọng nói tự động sử dụng Serverless Tổng quan Kiến trúc hướng sự kiện (Event-Driven Architecture) cho phép bạn xây dựng các ứng dụng tự động phản hồi lại các thay đổi trạng thái, chẳng hạn như khi tải lên một tập tin, mà không cần cung cấp hay quản lý máy chủ.\nTrong bài thực hành này, bạn sẽ học cách tạo, cấu hình và kiểm thử một quy trình Serverless có khả năng tự động chuyển đổi các file văn bản thành âm thanh giọng nói sống động bằng cách sử dụng các Dịch vụ được quản lý của AWS (AWS Managed Services).\nBạn sẽ tận dụng hai cơ chế chính để xử lý dữ liệu bất đồng bộ:\nS3 Event Notifications - Cấu hình Amazon S3 đóng vai trò là nguồn sự kiện. Nó sẽ tự động kích hoạt một hàm tính toán bất cứ khi nào có một đối tượng mới được tải lên một thư mục đầu vào cụ thể. Managed AI Services - Tận dụng Amazon Polly để tổng hợp giọng nói từ văn bản. Điều này cho phép bạn thêm các khả năng học sâu (deep learning) vào ứng dụng của mình thông qua các lệnh gọi API đơn giản mà không cần chuyên môn về khoa học dữ liệu. Nội dung Tổng quan Workshop Các bước chuẩn bị Xây dựng Logic xử lý (Lambda) Cấu hình Lưu trữ (S3) Kiểm tra kết quả Dọn dẹp tài nguyên "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/1-worklog/1.5-week5/",
	"title": "Nhật ký công việc Tuần 5",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 5: Hoàn thiện các tính năng còn lại trong seller panel và đảm bảo hệ thống hoạt động ổn định thông qua integration test. Nâng cao chuyên môn và kỹ năng dịch thuật thông qua việc dịch các bài blog kỹ thuật. Đánh giá tiến độ và lập kế hoạch cho giai đoạn tiếp theo của dự án. Các nhiệm vụ cần triển khai trong tuần này: Thứ Nhiệm vụ Ngày Nguồn tài liệu 2 - Hoàn thành trang quản lý kho. 06/10/2025 3 - Thực hiện integration test để đảm bảo seller panel hoạt động ổn định. 07/10/2025 4 - Dịch bài blog kỹ thuật đầu tiên. 08/10/2025 Các mẫu kiến trúc Generative AI Serverless – Phần 1 5 - Dịch bài blog kỹ thuật thứ hai. 09/10/2025 Giảm chi phí Amazon ElastiCache của bạn lên đến 60% với Valkey và CUDOS 6 - Dịch bài blog kỹ thuật thứ ba.\n- Họp nhóm để đánh giá tiến độ dự án và xác định các bước tiếp theo. 10/10/2025 Bắt đầu với các instance Amazon EC2 bare metal dành cho Amazon RDS for Oracle và Amazon RDS Custom for Oracle Kết quả đạt được tuần 5: Hoàn thiện trang quản lý kho. Thực hiện integration test, đảm bảo các trang trong seller panel (quản lý sản phẩm, đơn hàng, kho, dashboard) hoạt động và liên kết dữ liệu chính xác. Dịch hoàn chỉnh ba bài blog kỹ thuật, nâng cao kiến thức và diễn đạt nội dung chuyên môn bằng tiếng Việt. Họp với nhóm để trao đổi kết quả và đề xuất giai đoạn phát triển tiếp theo. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/5-workshop/5.6-cleanup/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Chúc mừng bạn đã hoàn thành workshop này! Trong workshop này, bạn đã tìm hiểu các mô hình kiến trúc để xây dựng ứng dụng Serverless hướng sự kiện (Event-driven) trên AWS.\nBằng cách cấu hình S3 Event Notifications, bạn đã kích hoạt một quy trình tự động hóa nơi tài nguyên tính toán (AWS Lambda) phản ứng tức thì với việc nạp dữ liệu mà không cần can thiệp thủ công hay quản lý máy chủ.\nBằng cách tích hợp Amazon Polly, bạn đã tận dụng thành công các Dịch vụ AI được quản lý (Managed AI Services) để chuyển đổi văn bản thành giọng nói sống động, chứng minh cách thêm các khả năng máy học phức tạp vào ứng dụng của mình với lượng code tối thiểu.\nDọn dẹp tài nguyên Xóa S3 buckets Mở giao diện S3 console Mở bucket s3-demo-bucket Chọn 2 thư mục input và output Nhấn Delete và xác nhận Sau đó chọn bucket mà chúng ta đã tạo cho bài lab, nhấn Empty (làm rỗng) và xác nhận. Nhấn Delete (xóa) và xác nhận xóa. Xóa Lambda function Mở giao diện Lambda console Tìm function workshop1 và nhấn Actions Chọn Delete và xác nhận Xóa IAM role Mở giao diện IAM console Chọn Roles từ menu bên trái. Tìm role PollyLambdaRole. Chọn role đó và nhấn Delete. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/1-worklog/1.6-week6/",
	"title": "Nhật ký công việc Tuần 6",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 6: Ôn tập và học thêm những kiến thức mới về những dịch vụ và kiến trúc của AWS để chuẩn bị cho bài kiểm tra giữa học kỳ. Các nhiệm vụ cần triển khai trong tuần này: Thứ Nhiệm vụ Ngày Nguồn tài liệu 2 - Ôn tập những kiến thức về dịch vụ máy chủ ảo.\n+ Amazon EC2\n+ Amazon EBS\n+ Amazon EFS\n+ Amazon Lightsail\n- Học về dịch vụ container của AWS.\n+ Amazon ECS\n+ Amazon EKS\n+ Amazon ECR\n+ AWS Batch\n- Học về dịch vụ serverless.\n+ AWS Lambda 13/10/2025 FCJ Bootcamp Module 3\nTài liệu của EC2\nTài liệu của EBS\nTài liệu của EFS\nTài liệu của Lightsail\nTài liệu của ECS\nTài liệu của EKS\nTài liệu của ECR\nTài liệu của Batch\nTài liệu của Lambda 3 - Ôn tập và học thêm về dịch vụ cơ sở dữ liệu.\n+ Amazon RDS\n+ Amazon Aurora\n+ Amazon DynamoDB\n+ Amazon DocumentDB\n+ Amazon ElastiCache\n+ Amazon MemoryDB\n+ Amazon Neptune\n+ Amazon Timestream 14/10/2025 FCJ Bootcamp Module 6\nTài liệu của RDS và Aurora\nTài liệu của DynamoDB\nTài liệu của DocumentDB\nTài liệu của ElastiCache\nTài liệu của MemoryDB\nTài liệu của Neptune\nTài liệu của Timestream 4 - Học thêm về dịch vụ tính toán và serverless.\n+ Amazon Elastic Beanstalk\n+ AWS Fargate\n+ Elastic Load Balancing\n+ AWS App Runner\n+ Amazon GameLift Servers\n+ Amazon Amplify\n+ AWS AppSync\n+ Amazon Location Service 15/10/2025 Tài liệu của Elastic Beanstalk\nTài liệu của Fargate\nTài liệu của Elastic Load Balancing\nTài liệu của App Runner\nTài liệu của GameLift Servers\nTài liệu của Amplify\nTài liệu của AppSync\nTài liệu của Location Service 5 - Ôn tập và học thêm về dịch vụ lưu trữ.\n+ Amazon S3\n+ AWS Snowball Edge\n+ AWS Storage Gateway\n+ AWS Backup\n+ AWS DRS 16/10/2025 FCJ Bootcamp Module 4\nTài liệu của S3\nTài liệu của Snowball Edge\nTài liệu của Storage Gateway\nTài liệu của Backup\nTài liệu của DRS 6 - Học về AWS Well-Architected. 17/10/2025 Tổng quát về AWS Well-Architected Kết quả đạt được tuần 6: Cải thiện thêm kiến thức về các dịch vụ: Tính toán: EC2, Lightsail, ECS, EKS, ECR, Lambda,… Lưu trữ: S3, EBS, EFS, Snowball Edge, Storage Gateway,… Cơ sở dữ liệu: RDS, DynamoDB, DocumentDB, ElastiCache,… Bổ sung kiến thức về hạn chế và giải quyết rủi ro với Backup, DRS. Học về kiến trúc đám mây với AWS Well-Architected. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/6-self-evaluation/",
	"title": "Tự đánh giá",
	"tags": [],
	"description": "",
	"content": "Trong suốt thời gian thực tập tại AWS Việt Nam từ 08/09/2025 đến 09/12/2025, tôi đã có cơ hội học hỏi, rèn luyện và áp dụng kiến thức đã được trang bị tại trường vào môi trường làm việc thực tế.\nTôi đã tham gia phát triển giao diện và học máy, qua đó cải thiện kỹ năng học máy, khoa học dữ liệu, lập trình, giao tiếp, làm việc nhóm và quản lý dự án.\nVề tác phong, tôi luôn cố gắng hoàn thành tốt nhiệm vụ, tuân thủ nội quy, và tích cực trao đổi với đồng nghiệp để nâng cao hiệu quả công việc.\nĐể phản ánh một cách khách quan quá trình thực tập, tôi xin tự đánh giá bản thân dựa trên các tiêu chí dưới đây:\nSTT Tiêu chí Mô tả Tốt Khá Trung bình 1 Kiến thức và kỹ năng chuyên môn Hiểu biết về ngành, áp dụng kiến thức vào thực tế, kỹ năng sử dụng công cụ, chất lượng công việc ☐ ✅ ☐ 2 Khả năng học hỏi Tiếp thu kiến thức mới, học hỏi nhanh ✅ ☐ ☐ 3 Chủ động Tự tìm hiểu, nhận nhiệm vụ mà không chờ chỉ dẫn ☐ ✅ ☐ 4 Tinh thần trách nhiệm Hoàn thành công việc đúng hạn, đảm bảo chất lượng ☐ ☐ ✅ 5 Kỷ luật Tuân thủ giờ giấc, nội quy, quy trình làm việc ☐ ✅ ☐ 6 Tính cầu tiến Sẵn sàng nhận feedback và cải thiện bản thân ✅ ☐ ☐ 7 Giao tiếp Trình bày ý tưởng, báo cáo công việc rõ ràng ✅ ☐ ☐ 8 Hợp tác nhóm Làm việc hiệu quả với đồng nghiệp, tham gia nhóm ☐ ✅ ☐ 9 Ứng xử chuyên nghiệp Tôn trọng đồng nghiệp, đối tác, môi trường làm việc ✅ ☐ ☐ 10 Tư duy giải quyết vấn đề Nhận diện vấn đề, đề xuất giải pháp, sáng tạo ☐ ✅ ☐ 11 Đóng góp vào dự án/tổ chức Hiệu quả công việc, sáng kiến cải tiến, ghi nhận từ nhóm ☐ ✅ ☐ 12 Tổng thể Đánh giá chung về toàn bộ quá trình thực tập ☐ ✅ ☐ Cần cải thiện Rèn luyện tính kỷ luật và tăng động lực để hoàn thành nhiệm vụ đúng hạn. Chủ động hơn trong việc tìm kiếm công việc và tự giác đề xuất nhiệm vụ. Cải thiện kỹ năng giải quyết vấn đề bằng cách xác định, đề xuất giải pháp và thể hiện sự sáng tạo một cách hiệu quả hơn. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/1-worklog/1.7-week7/",
	"title": "Nhật ký công việc Tuần 7",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 7: Cải thiện hiểu biết về các dịch vụ và kiến trúc AWS để chuẩn bị kiểm tra giữa học kỳ. Thực hành để củng cố kiến thức về một số dịch vụ chủ chốt. Các nhiệm vụ cần triển khai trong tuần này: Thứ Nhiệm vụ Ngày Nguồn tài liệu 2 - Tìm hiểu thêm về AWS Well-Architected. 20/10/2025 Tổng quát về AWS Well-Architected 3 - Tìm hiểu về VPC và CloudFormation.\n- Thực hiện một số bài thực hành về VPC, CloudFormation và EC2. 21/10/2025 Tài liệu của VPC\nTài liệu của CloudFormation\nKhởi tạo mẫu CloudFormation\nTạo EC2 instance\nTạo Security Group\nCập nhật NACL 4 - Thực hành về Backup, Storage Gateway và S3.\n- Học thêm về S3. 22/10/2025 FCJ Bootcamp Lab 13, 24, 57\nTài liệu của S3 5 - Tìm hiểu về những dịch vụ học máy.\n+ Amazon A2I\n+ Amazon SageMaker AI\n+ AWS DLAMI\n+ AWS Deep Learning Containers\n+ AWS Entity Resolution\n+ PyTorch\n+ TensorFlow\n+ Hugging Face 23/10/2025 Tài liệu của A2I\nTài liệu của SageMaker AI\nTài liệu của DLAMI\nTài liệu của Deep Learning Containers\nTài liệu của Entity Resolution\nPyTorch trên AWS\nTensorFlow trên AWS\nHugging Face trên AWS 6 - Xem qua các dịch vụ học máy chuyên dụng.\n+ Amazon CodeGuru\n+ Amazon DevOps Guru\n+ Amazon Kendra\n+ Amazon Personalize\n+ Amazon Rekognition\n+ Amazon Comprehend\n+ Amazon Translate\n+ Amazon Textract\n+ Amazon Polly\n+ Amazon Lex 24/10/2025 Tài liệu của CodeGuru\nTài liệu của DevOps Guru\nTài liệu của Kendra\nTài liệu của Personalize\nTài liệu của Rekognition\nTài liệu của Comprehend\nTài liệu của Translate\nTài liệu của Textract\nTài liệu của Polly\nTài liệu của Lex Kết quả đạt được tuần 7: Học thêm về các kiến trúc AWS. Cải thiện thêm kiến thức về các dịch vụ mạng và lưu trữ. Có thêm kinh nghiệm về những dịch vụ tính toán và lưu trữ. Học về nhiều dịch vụ học máy. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/7-feedback/",
	"title": "Chia sẻ, đóng góp ý kiến",
	"tags": [],
	"description": "",
	"content": " Tại đây bạn có thể tự do đóng góp ý kiến cá nhân về những trải nghiệm khi tham gia chương trình First Cloud Journey, giúp team FCJ cải thiện những vấn đề còn thiếu sót dựa trên các hạng mục sau:\nĐánh giá chung 1. Môi trường làm việc\nMôi trường làm việc rất thân thiện và cởi mở. Các thành viên trong FCJ luôn sẵn sàng hỗ trợ khi mình gặp khó khăn, kể cả ngoài giờ làm việc. Không gian làm việc gọn gàng, thoải mái, giúp mình tập trung tốt hơn. Tuy nhiên, mình nghĩ có thể bổ sung thêm một số buổi giao lưu hoặc team bonding để mọi người hiểu nhau hơn.\n2. Sự hỗ trợ của mentor / team admin\nMentor hướng dẫn rất chi tiết, giải thích rõ ràng khi mình chưa hiểu và luôn khuyến khích mình đặt câu hỏi. Team admin hỗ trợ các thủ tục, tài liệu và tạo điều kiện để mình làm việc thuận lợi. Mình đánh giá cao việc mentor cho phép mình thử và tự xử lý vấn đề thay vì chỉ đưa đáp án.\n3. Sự phù hợp giữa công việc và chuyên ngành học\nCông việc mình được giao phù hợp với kiến thức mình đã học ở trường, đồng thời mở rộng thêm những mảng mới mà mình chưa từng được tiếp cận. Nhờ vậy, mình vừa củng cố kiến thức nền tảng, vừa học thêm kỹ năng thực tế.\n4. Cơ hội học hỏi \u0026amp; phát triển kỹ năng\nTrong quá trình thực tập, mình học được nhiều kỹ năng mới như sử dụng công cụ quản lý dự án, kỹ năng làm việc nhóm, và cả cách giao tiếp chuyên nghiệp trong môi trường công ty. Mentor cũng chia sẻ nhiều kinh nghiệm thực tế giúp mình định hướng tốt hơn cho sự nghiệp.\n5. Văn hóa \u0026amp; tinh thần đồng đội\nVăn hóa công ty rất tích cực: mọi người tôn trọng lẫn nhau, làm việc nghiêm túc nhưng vẫn vui vẻ. Khi có dự án gấp, mọi người cùng nhau cố gắng, hỗ trợ không phân biệt vị trí. Điều này giúp mình cảm thấy mình là một phần của tập thể, dù chỉ là thực tập sinh.\n6. Chính sách / phúc lợi cho thực tập sinh\nCông ty có hỗ trợ phụ cấp thực tập và tạo điều kiện về thời gian linh hoạt khi cần thiết. Ngoài ra, việc được tham gia các buổi đào tạo nội bộ là một điểm cộng lớn.\nMột số câu hỏi khác Điều bạn hài lòng nhất trong thời gian thực tập? Điều bạn nghĩ công ty cần cải thiện cho các thực tập sinh sau? Nếu giới thiệu cho bạn bè, bạn có khuyên họ thực tập ở đây không? Vì sao? Đề xuất \u0026amp; mong muốn Bạn có đề xuất gì để cải thiện trải nghiệm trong kỳ thực tập? Bạn có muốn tiếp tục chương trình này trong tương lai? Góp ý khác (tự do chia sẻ): "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/1-worklog/1.8-week8/",
	"title": "Nhật ký công việc Tuần 8",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 8: Chuẩn bị chắc kiến thức cho bài kiểm tra giữa kỳ. Lên văn phòng để nhận áo và làm bài kiểm tra. Các nhiệm vụ cần triển khai trong tuần này: Thứ Nhiệm vụ Ngày Nguồn tài liệu 2 - Trao đổi với các bạn trong FCJ về nội dung bài kiểm tra giữa kỳ.\n- Kiểm tra chéo kiến thức lẫn nhau giữa các thành viên nhóm. 27/10/2025 3 - Tìm hiểu các câu hỏi thường gặp trong bài kiểm tra chứng chỉ SSA. 28/10/2025 4 - Đọc AWS Solutions Library. 29/10/2025 AWS Solutions Library 5 - Đọc thêm về các giải pháp từ AWS Solutions Library. 30/10/2025 AWS Solutions Library 6 - Ôn lại kiến thức về AWS Well-Architected Framework.\n- Lên văn phòng AWS để nhận áo FCJ và làm bài kiểm tra giữa kỳ. 31/10/2025 AWS Well-Architected Kết quả đạt được tuần 8: Hoàn thành trao đổi nhóm, củng cố kiến thức thông qua thảo luận và kiểm tra chéo. Nắm được nhiều dạng câu hỏi thường gặp trong kỳ thi SSA. Hiểu thêm các mô hình giải pháp thông qua AWS Well-Architected và AWS Solutions Library. Hoàn thành bài kiểm tra giữa kỳ. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/1-worklog/1.9-week9/",
	"title": "Nhật ký công việc Tuần 9",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 9: Bắt đầu triển khai ý tưởng cho hệ thống đề xuất sản phẩm. Làm rõ quá trình thu thập, mô phỏng và chuẩn hóa dữ liệu. Nắm được kiến trúc cơ bản của hệ thống đề xuất sản phẩm. Các nhiệm vụ cần triển khai trong tuần này: Thứ Nhiệm vụ Ngày Nguồn tài liệu 2 - Tìm nguồn dữ liệu để mô phỏng cho cơ sở dữ liệu. 03/11/2025 3 - Nghiên cứu về hệ thống đề xuất sản phẩm. 04/11/2025 4 - Tìm hiểu về TensorFlow và TensorFlow Recommenders. 05/11/2025 TensorFlow Recommenders 5 - Trao đổi với nhóm về cấu trúc và nội dung dữ liệu cần thu thập. 06/11/2025 6 - Thực hành làm mô hình thử nghiệm để củng cố kiến thức về hệ thống đề xuất.\n- Họp với nhóm để bàn về nhiệm vụ tiếp theo. 07/11/2025 Kết quả đạt được tuần 9: Tìm được nguồn dữ liệu ban đầu phục vụ xây dựng mô hình. Hiểu được các khái niệm của hệ thống gợi ý và các hướng tiếp cận. Làm quen với TensorFlow Recommenders và tiến hành thử nghiệm mô hình cơ bản. Thống nhất với nhóm về dữ liệu và bước phát triển tiếp theo. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/1-worklog/1.10-week10/",
	"title": "Nhật ký công việc Tuần 10",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 10: Hoàn thiện mô phỏng dữ liệu cho cơ sở dữ liệu. Bắt đầu các bước tiền xử lý dữ liệu để chuẩn bị cho huấn luyện mô hình. Các nhiệm vụ cần triển khai trong tuần này: Thứ Nhiệm vụ Ngày Nguồn tài liệu 2 - Làm việc với nhóm để thu thập và tạo dữ liệu. 10/11/2025 3 - Tạo thêm dữ liệu mô phỏng cho cơ sở dữ liệu. 11/11/2025 4 - Hoàn thành việc tạo dữ liệu cho cơ sở dữ liệu. 12/11/2025 5 - Bắt đầu xử lý dữ liệu cho mô hình đề xuất. 13/11/2025 6 - Thực hiện các chỉnh sửa dữ liệu cần thiết trong cơ sở dữ liệu. 14/11/2025 Kết quả đạt được tuần 10: Hoàn thành việc thu thập và tạo dữ liệu cần thiết cho hệ thống. Có bộ dữ liệu mô phỏng tương đối hoàn chỉnh để phục vụ huấn luyện mô hình. Bắt đầu xử lý và chuẩn hóa dữ liệu đầu vào. Đảm bảo dữ liệu được chỉnh sửa, bổ sung và cấu trúc phù hợp cho giai đoạn phát triển tiếp theo. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/1-worklog/1.11-week11/",
	"title": "Nhật ký công việc Tuần 11",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 11: Hoàn thành quá trình xử lý dữ liệu đầu vào. Xây dựng mô hình TFRS. Các nhiệm vụ cần triển khai trong tuần này: Thứ Nhiệm vụ Ngày Nguồn tài liệu 2 - Hoàn thành quá trình xử lý dữ liệu. 17/11/2025 3 - Bắt đầu làm mô hình đề xuất bằng TFRS. 18/11/2025 4 - Hoàn thiện kiến trúc của mô hình. 19/11/2025 5 - Tinh chỉnh thông số của mô hình. 20/11/2025 6 - Hoàn tất quá trình huấn luyện và lưu mô hình. 21/11/2025 Kết quả đạt được tuần 11: Hoàn tất công đoạn xử lý dữ liệu và chuẩn hóa dữ liệu phục vụ huấn luyện. Thiết kế và tinh chỉnh mô hình gợi ý. Huấn luyện thành công mô hình, sẵn sàng tích hợp vào hệ thống. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/1-worklog/1.12-week12/",
	"title": "Nhật ký công việc Tuần 12",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 12: Nghiên cứu tích hợp AI vào ứng dụng bằng DJL. Triển khai mô hình đề xuất vào hệ thống và hoàn thiện chức năng đề xuất. Các nhiệm vụ cần triển khai trong tuần này: Thứ Nhiệm vụ Ngày Nguồn tài liệu 2 - Tìm hiểu về tích hợp AI và Deep Java Library. 24/11/2025 Tài liệu của DJL 3 - Bắt đầu tích hợp mô hình vào ứng dụng. 25/11/2025 4 - Làm hệ thống đề xuất cho ứng dụng. 26/11/2025 5 - Hoàn thành tích hợp hệ thống đề xuất. 27/11/2025 6 - Họp với nhóm để thảo luận về quá trình triển khai dự án lên hạ tầng AWS. 28/11/2025 Kết quả đạt được tuần 12: Nắm được cách triển khai mô hình AI thông qua DJL. Tích hợp thành công hệ thống đề xuất vào ứng dụng. Đã có kế hoạch triển khai hệ thống lên hạ tầng AWS sau buổi họp với nhóm. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]