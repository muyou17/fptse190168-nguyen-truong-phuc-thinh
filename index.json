[
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/5-workshop/5.3-s3/create-s3/",
	"title": "Create S3",
	"tags": [],
	"description": "",
	"content": "Create S3 Open the S3\nClick Create Bucket:\nIn the Create Bucket console: Specify name of the bucket: s3-demo-text\nDo not add a tag to the VPC endpoint at this time. Click Create bucket Then click on the bucket you just created and press Create folder In the Create folder console: Specify name of the bucket: input then click Create folder We do the same with creating the output file. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/5-workshop/5.1-workshop-overview/",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "INTRODUCTION Introduction to S3 Event Notifications S3 Event Notifications is a feature that allows Amazon S3 to automatically send notifications when specific events occur in your Bucket (for example: when a new file is uploaded, deleted, or copied). S3 can send notifications to various destinations such as AWS Lambda, Amazon SNS, or Amazon SQS. In this workshop, we use the PutObject event to automatically trigger a Lambda function as soon as a text file is uploaded to S3. Introduction to Amazon Polly Amazon Polly is a Text-to-Speech service that uses AWS\u0026rsquo;s advanced Deep Learning technology. Polly supports over 60 voices in more than 20 languages, including Vietnamese, with natural human-like audio quality. This service is fully managed, so you don\u0026rsquo;t need to worry about managing infrastructure or scalability. Workshop Overview In this workshop, you will build a Serverless Text-to-Speech Converter application. The system operates fully automatically based on an Event-driven Architecture model:\nAmazon S3 (Input Bucket): Stores input text files (.txt) uploaded by users. S3 Event Notifications: Detects new file upload events and triggers AWS Lambda. AWS Lambda: The central processor that orchestrates data flow between S3 and Polly. Lambda reads the text file, calls the Polly API for conversion, and saves the result. Amazon Polly: The AI service that converts text into audio with natural voice. Amazon S3 (Output Bucket): Stores output audio files (.mp3) after processing is complete. The entire process runs automatically without manual intervention, helping save time and operational costs.\nSystem Architecture The model below describes the detailed architecture and data flow of the Text-to-Speech Converter system:\nWorkflow:\nUpload file: User uploads a text file (.txt) to Amazon S3 Input Bucket.\nEvent trigger: S3 detects the PutObject event (new file), automatically sends an event notification to trigger AWS Lambda Function.\nLambda processing: Lambda function is triggered and performs:\nReads the text file content from S3 Input Bucket Sends the text content to Amazon Polly API along with parameters (voice ID, output format) Polly conversion: Amazon Polly receives the request, processes the text, and returns an audio stream to Lambda.\nSave result: Lambda receives the audio stream from Polly and saves it as an .mp3 file to Amazon S3 Output Bucket.\nComplete: User can download the MP3 file from S3 Output Bucket for use.\nBenefits of Serverless Architecture No server management: AWS automatically handles scaling, patching, and high availability. Cost optimization: Pay only when processing requests (pay-per-use model). Auto-scaling: The system can handle from a few requests to millions of requests without additional configuration. Fast deployment: Focus on application logic instead of managing infrastructure. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Serverless generative AI architectural patterns – Part 1 by Michael Hume and Parnab Basak | on 04 SEP 2025 | in Advanced (300), Generative AI, Serverless, Technical How-to | Permalink\nOrganizations of all sizes and types are harnessing large language models (LLMs) and foundation models (FMs) to build generative AI applications that deliver new customer and employee experiences. Serverless computing offers the perfect solution, empowering organizations to focus on innovation, flexibility, and cost-efficiency without the complexity of infrastructure management. Organizations transitioning their experimental implementations into production-ready applications can implement proven, scalable, and maintainable software design patterns as the cornerstone of their architecture.\nThis two-part series explores the different architectural patterns, best practices, code implementations, and design considerations essential for successfully integrating generative AI solutions into both new and existing applications. In this post, we focus on patterns applicable for architecting real-time generative AI applications. Part 2 addresses patterns for building batch-oriented generative AI implementations using serverless services.\nSeparation of concerns A fundamental principle in building robust generative AI applications is the separation of concerns, which involves dividing the application stack into three distinct components: frontend, middleware, and backend service layers. This architectural approach (as shown in the following diagram) offers multiple benefits, including reduced complexity, enhanced maintainability, and the ability to scale components independently. By implementing this separation, you can develop cross-platform solutions while maintaining the flexibility to evolve each component according to specific requirements.\nAlthough these layers are merely extensions to the traditional software stack, they do perform some specific tasks in generative AI applications.\nFrontend Layer The frontend layer serves as the primary interface between end-users and the generative AI application. For organizations integrating generative AI into existing applications, this layer might already be established. The frontend handles critical responsibilities including user authentication, UI/UX presentation, and API communication. AWS provides a robust suite of serverless services to support frontend implementations, including AWS Amplify for full-stack development, Amazon CloudFront paired with Amazon Simple Storage Service (Amazon S3) for content delivery, and container services like Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Kubernetes Service (Amazon EKS) for application hosting. Specialized services such as Amazon Lex can enhance the user experience through conversational interfaces and intelligent search capabilities for building interactive chatbots.\nMiddleware layer This represents the integration layer, comprising of three essential sub-layers that manage different aspects of the application logic and data flow:\nAPI layer – This layer exposes backend services through various protocols, including REST, GraphQL, and WebSockets. It handles essential functions such as input validation, traffic management, and CORS support. The API layer also implements authorization and access control mechanisms, manages API versioning, and provides monitoring capabilities. It provides secure and efficient communication between the frontend and backend components while maintaining scalability and reliability. AWS managed services like Amazon API Gateway and AWS AppSync can help create an AI gateway to simplify access and API management. Prompt engineering layer – This layer encapsulates the business logic necessary for interacting with LLMs. It handles dynamic prompt generation, model selection, prompt caching, model routing, guardrails, and security enforcement. This layer implements token and context window optimization, sensitive information filtering, output content moderation, error handling, retry logic, and audit trails. By centralizing these functions, you can maintain consistent prompt strategies, enforce security, and optimize model interactions across applications. You can use Amazon DynamoDB to store prompt templates and configurations, and use Amazon Bedrock Guardrails, Amazon Bedrock prompt caching, and Amazon Bedrock Intelligent Prompt Routing to implement responsible AI safeguards, reuse of prompt prefixes, and dynamic routing, respectively. Orchestration layer – This layer manages complex interactions between various system components. It coordinates external API calls and agent calls, manages vector database queries, stores user sessions and conversation histories, and maintains conversation context across multiple LLM interactions. Frameworks like LangChain and LlamaIndex are commonly used to simplify these operations and provide standardized approaches to common generative AI tasks. AWS Step Functions has direct integrations with over 220 AWS services, including Amazon Bedrock, enabling you to construct intricate generative AI workflows without incurring additional computational resources. Additionally, with Amazon Bedrock Flows, you can create complex, flexible, multi-prompt workflows to evaluate, compare, and version. Backend services, agents, and private data sources The backend layer forms the core of generative AI response generation powered by LLMs. It consists of hosting and invoking the LLM model, agents, knowledge bases, or a Model Context Protocol (MCP) server. Amazon Bedrock, Amazon SageMaker JumpStart, and Amazon SageMaker offer a variety of high-performing FMs from leading AI companies or the option to bring your own. You can securely run an MCP server using a containerized architecture, as described in Guidance for Deploying Model Context Protocol Servers on AWS.\nPrivate data sources complement LLMs by providing authoritative proprietary knowledge outside of its training data. For Retrieval Augmented Generation (RAG) implementations, Amazon Kendra, Amazon OpenSearch Serverless, and Amazon Aurora PostgreSQL-Compatible Edition with the pgVector extension provide robust, scalable vector database options. For a deeper dive, please read The role of vector databases in generative AI applications on available AWS service options to store embeddings in a purpose built vector database.\nReal-time applications process and deliver responses with minimal latency, enhancing the user experience and facilitating faster decision-making. In the following sections, we explore some architectural patterns that can be used to implement real-time generative AI applications.\nPattern 1: Synchronous request response In this pattern, responses are generated and immediately delivered, while the client blocks/waits for response. Although this is simple to implement, has a predictable flow, and offers strong consistency, it suffers from blocking operations, high latency, and potential timeouts. When implemented for generative AI applications, this pattern is particularly suited for certain modalities like video or image generations. For fast LLM interactions, it can handle multiple concurrent requests while maintaining consistent performance under varying loads. This model can be implemented through several architectural approaches.\nREST APIs You can use RESTful APIs to communicate with your backend over HTTP requests. You can use REST or HTTP APIs in API Gateway or an Application Load Balancer for path-based routing to the middleware. API Gateway offers additional features like token-based authentication, custom authorizers, resource-based permissions, request/response mapping and transformation, versioning, and rate-limiting. However, with REST/HTTP APIs in API Gateway, the response must be generated within 29 seconds to meet the default integration timeout. You can extend this default limit to 5 minutes for REST APIs with a possible reduction in your AWS Region-level throttle quota for your account. For an example implementation, refer to Interact with Bedrock models from a Lambda function fronted with an API Gateway. The following diagram illustrates this architecture.\nGraphQL HTTP APIs You can use AWS AppSync as the API layer to take advantage of the benefits of GraphQL APIs. GraphQL APIs offer declarative and efficient data fetching using a typed schema definition, serverless data caching, offline data synchronization, security, and fine-grained access control. It also provides data sources and resolvers for writing business logic. If you don’t need the mutation layer, AWS AppSync can directly invoke an LLM in Amazon Bedrock. AWS AppSync integration timeout is set to 30 seconds by default and can’t be extended. If you need to perform operations that might take longer, consider implementing asynchronous patterns or breaking down the operation into smaller chunks. For an example integration, see Invoke Amazon Bedrock models from AWS AppSync HTTP resolver. The following diagram illustrates the solution architecture.\nConversational chatbot interface Amazon Lex is a service for building conversational interfaces with voice and text, offering speech recognition and language understanding capabilities. It simplifies multimodal development and enables publication of chatbots to various chat services and mobile devices. It offers native integration with Lambda to streamline chatbot development. When a Lambda function is used for fulfilment, the default response timeout is set to 30 seconds. To bypass, you can use fulfilment updates to provide periodic updates to the user, so the user knows that the chatbot is still working on their request. For an example implementation, see Enhance Amazon Connect and Lex with generative AI capabilities. The following diagram illustrates the solution architecture.\nModel invocation using orchestration AWS Step Functions enables orchestration and coordination of multiple tasks, with native integrations across AWS services like Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. AWS Step Functions offers built-in features like function orchestration, branching, error handling, parallel processing, and human-in-the-loop capabilities. It also has an optimized integration with Amazon Bedrock, allowing direct invocation of Amazon Bedrock FMs from AWS Step Functions workflows. With this integration, you can accomplish the following:\nEnrich Step Functions data processing with generative AI capabilities for tasks like text summarization, image generation, or personalization Retrieve and inject up-to-date data (such as product pricing or user profiles) into LLM prompts for improved accuracy Orchestrate LLM and agent calls in a customized processing chain, using the best-suited models at each stage Implement human-in-the-loop interactions to moderate responses and handle hallucinations of the FM For an example implementation using API Gateway, see Prompt chaining with Amazon API Gateway and AWS Step Functions. For an example implementation using AWS AppSync, see Prompt chaining with AWS AppSync, AWS Step Functions and Amazon Bedrock.\nPattern 2: Asynchronous request response This pattern provides a full-duplex, bidirectional communication channel between the client and server without clients having to wait for updates. The biggest advantages is its non-blocking nature that can handle long-running operations. However, they are more complex to implement because they require channel, message, and state management. This model can be implemented through two architectural approaches.\nWebSocket APIs The WebSocket protocol enables real-time, synchronous communication between the frontend and middleware, allowing for bidirectional, full-duplex messaging over a persistent TCP connection. This bidirectional behavior enhances client/service interactions, enabling services to push data to clients without requiring explicit requests. Using API Gateway, you can create a WebSocket APIs as a stateful frontend for an AWS service (such as Lambda or DynamoDB) or for an HTTP endpoint. The WebSocket API invokes your backend based on the content of the messages it receives from client apps. After the message is generated, the backend can send callback messages to connected clients. Each request-response cycle must complete within 29 seconds, as defined by the API Gateway integration timeout for WebSockets. The connection duration for API Gateway WebSocket APIs can be up to 2 hours with an idle connection timeout of 10 minutes—these can’t be extended. For an example implementation, refer to AI Chat with Amazon API Gateway (WebSockets), AWS Lambda and Amazon Bedrock.\nGraphQL WebSocket APIs AWS AppSync can establish and maintain secure WebSocket connections for GraphQL subscription operations, enabling middleware applications to distribute data in real time from data sources to subscribers. It also supports a simple publish-subscribe model, where client frontends can listen to specific channels or topics, with AWS AppSync managing multiple temporary pub/sub channels and WebSocket connections to deliver and filter data based on the channel name. For an example implementation, refer to AI Chat with AWS AppSync (WebSockets), AWS Lambda, and Amazon Bedrock. The following diagram illustrates an example architecture.\nPattern 3: Asynchronous streaming response This streaming pattern enables real-time response flow to clients in chunks, enhancing the user experience and minimizing first response latency. This pattern uses built-in streaming capabilities in services like Amazon Bedrock (InvokeModelWithResponseStream or ConverseStream APIs) and SageMaker real-time inference, enabling applications to display results incrementally rather than waiting for complete responses. This pattern is particularly effective for applications implementing text modality such as chat interfaces and word-based content generation tools.\nImplementation is achieved through the API Gateway WebSocket API or AWS AppSync WebSocket APIs or GraphQL subscriptions, with careful consideration given to timeout management and connection handling.\nThe following diagram illustrates the architecture of asynchronous streaming using API Gateway WebSocket APIs.\nThe following diagram illustrates the architecture of asynchronous streaming using AWS AppSync WebSocket APIs.\nIf you don’t need an API layer, Lambda response streaming lets a Lambda function progressively stream response payloads back to clients. For more details, see Using Amazon Bedrock with AWS Lambda. The following diagram illustrates this architecture.\nConclusion This post introduced three design patterns applicable for real-time generative AI applications: synchronous request response, asynchronous request response, and asynchronous streaming response. We also highlighted how to implement these patterns using AWS serverless services. When selecting an appropriate pattern for your implementation, it is crucial to consider the anticipated end-user experience, the existing technical stack, AWS service quotas, and the latency of your LLM responses. In Part 2, we discuss patterns for building batch-oriented generative AI implementations using AWS serverless services.\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “AWS AI/ML Services \u0026amp; Generative AI Workshop” Event Objectives Provide an overview of the AI/ML landscape in Vietnam. Demonstrate end-to-end machine learning workflows using Amazon SageMaker. Introduce Generative AI capabilities with Amazon Bedrock (Foundation Models, Agents, Guardrails). Share techniques for Prompt Engineering and Retrieval-Augmented Generation (RAG). Speakers AWS Solutions Architects AI/ML Specialists Key Highlights AWS AI/ML Services Overview (SageMaker) End-to-End Platform: Covered the full lifecycle from data preparation to deployment. Data Preparation: Strategies for labeling and cleaning data. Training \u0026amp; Tuning: optimizing models for performance and cost. Deployment: Moving models to production endpoints efficiently. Integrated MLOps: Highlighted capabilities to automate and standardize ML pipelines. SageMaker Studio: Live demo of the unified interface for building, training, and deploying models. Generative AI with Amazon Bedrock Foundation Models (FMs): Comparison and selection guide for top models: Claude: High reasoning capabilities. Llama: Open and efficient. Titan: Native AWS integration. Prompt Engineering: Chain-of-Thought: Breaking down complex reasoning tasks. Few-shot learning: Using examples to improve model output. Advanced Architectures: RAG (Retrieval-Augmented Generation): Integrating Knowledge Bases to ground answers in company data. Bedrock Agents: Creating multi-step workflows and integrating with external tools. Guardrails: Ensuring safety and content filtering. Key Takeaways Design Mindset Model Selection: Choose the right Foundation Model (e.g., Claude vs. Titan) based on the specific use case requirements (speed vs. reasoning). Safety First: Implementation of Guardrails is critical for responsible AI deployment in enterprise settings. Technical Architecture RAG over Fine-tuning: For most internal knowledge use cases, RAG provides a more flexible and cost-effective solution than fine-tuning models. Agentic Workflows: Moving from passive chatbots to active Agents that can execute tasks is the next frontier of GenAI. Modernization Strategy MLOps Adoption: Moving away from manual model training to automated pipelines (MLOps) is essential for scalability. Local Context: Understanding the specific AI/ML trends and landscape within Vietnam helps in benchmarking local projects. Applying to Work Pilot SageMaker: Evaluate current ML workflows and identify opportunities to migrate to Amazon SageMaker for better lifecycle management. Build a Knowledge Bot: Create a prototype using Amazon Bedrock and RAG to query internal documentation or technical manuals. Refine Prompting: Immediately apply Chain-of-Thought and Few-shot techniques to improve the accuracy of current AI interactions. Implement Guardrails: Configure content filters on Bedrock to ensure brand safety for any experimental applications. Event Experience Attending the “AWS AI/ML Services \u0026amp; Generative AI Workshop” provided a practical roadmap for adopting intelligent services. Key experiences included:\nLearning from experts Gained clarity on the AI/ML landscape in Vietnam, understanding local opportunities and challenges. Deepened technical knowledge on the differences between major Foundation Models (Claude, Llama, Titan). Hands-on technical exposure The SageMaker Studio walkthrough demonstrated how to unify the fragmented ML toolchain into a single pane of glass. The Live Demo of Bedrock showed the practical implementation of building a Generative AI chatbot, demystifying the complexity of RAG and Agents. Networking and discussions The ice-breaker activity and networking sessions allowed for exchanging ideas with peers about real-world challenges in deploying GenAI. Discussions reinforced the importance of Prompt Engineering as a critical skill for modern development. Lessons learned RAG is the key to making LLMs useful for business-specific data without the high cost of training. Guardrails are not optional; they are a fundamental layer of the Generative AI stack. Efficiency in ML comes from integrated MLOps rather than isolated data science experiments. Event photo "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyen Truong Phuc Thinh\nPhone Number: +84907511320\nEmail: nguyentruongphucthinh@gmail.com\nUniversity: FPT University\nMajor: Information Technology\nSpecialization: Artificial Intelligence\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Intern\nInternship Duration: From 09/08/2025 to 12/09/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Understand AWS policies and professional workspace etiquette. Connect with members in the First Cloud Journey program. Form a team and develop a project plan. Learn basic AWS services and specialized knowledge for the project. Tasks to be carried out this week: Day Task Date Reference Material 2 - Read AWS FCJ internship policies and guidelines.\n- Get acquainted with mentors and students in FCJ.\n- Form a team of five members. 09/08/2025 FCJ Policies and Guidelines 3 - Get familiar with the AWS office environment.\n- Learn how to present an AWS workshop and start writing worklog for the first week. 09/09/2025 AWS Workshop Guide 4 - Getting started with lessons from the First Cloud Journey Bootcamp playlist:\n+ AWS cloud computing technology\n+ Creating and managing an AWS account\n+ AWS Virtual Private Cloud (VPC) 09/10/2025 First Cloud Journey Bootcamp - 2025 5 - Finish studying FCJ Bootcamp lessons:\n+ Compute VMs on AWS\n+ AWS security services\n+ AWS database services 09/11/2025 First Cloud Journey Bootcamp - 2025 6 - Meet with team to plan the first project.\n- Finalize the weekly worklog by adding achievements. 09/12/2025 Week 1 Achievements: Reviewed and understood AWS FCJ internship policies and guidelines. Went to the AWS office for the first time, gaining familiarity with workplace environment. Connected with mentors and fellow students in the FCJ program, formed a team of five members for project work. Initiated this internship report and wrote the first weekly worklog. Completed all lessons from the First Cloud Journey Bootcamp playlist. Learned the basics of AWS services Created an AWS free tier account Practiced managing services using the AWS account Collaborated with the team to plan the first project, defining the name, objectives and timelines. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar basic AWS services, team members and deciding on the first project.\nWeek 2: Gain in-depth knowledge of NLP. Collaborate with team to define features and assign project tasks.\nWeek 3: Learn React.js and applying the knowledge to develop the product management page. Study containerization to prepare for deployment.\nWeek 4: Focus on developing management pages for the seller panel.\nWeek 5: Completing the seller panel. Translate three technical blogs.\nWeek 6: Study AWS services and architectures to prepare for the midterm examination.\nWeek 7: Study more AWS services and architectures to prepare for the midterm examination.\nWeek 8: Study AWS architectures and solutions. Take the midterm examination.\nWeek 9: Prepare data sources and foundational knowledge for the recommendation system.\nWeek 10: Collect, generate and preprocess data for the recommendation model.\nWeek 11: Design, tune and train the recommendation model.\nWeek 12: Integrate the recommendation system into the application. Prepare for project deployment on AWS infrastructure.\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/5-workshop/5.4-lambda/create-lambda/",
	"title": "Create Lambda",
	"tags": [],
	"description": "",
	"content": "Create Lambda function Navigate to Lambda management console In the Dashboard console, choose Create function In the Create Lambda function console Name the lambda: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Runtime: choose python 3.13 or latest Expand the Change default execution role section In Execution role, choose Use an existing role and choose PollyLambdaRole Scroll down and choose Create lambda Successfully create S3 bucket. Create code Lambda In this workshop, we will use the code to convert a text folder into a voice folder (Text-to-Speech) using Amazon Polly service. Scroll down to the Code source section. Delete all existing code in the lambda_function.py file. We will use the code as follows: import json import boto3 import os s3 = boto3.client(\u0026#39;s3\u0026#39;) polly = boto3.client(\u0026#39;polly\u0026#39;) def lambda_handler(event, context): try: # 1. Get uploaded file info record = event[\u0026#39;Records\u0026#39;][0] bucket_name = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] object_key = record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] # Ex: input/hello.txt print(f\u0026#34;Processing file: {object_key}\u0026#34;) # 2. Read text file content file_obj = s3.get_object(Bucket=bucket_name, Key=object_key) text_content = file_obj[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) # 3. Call Polly to convert text to speech (Voice: Joanna) response = polly.synthesize_speech( Text=text_content, OutputFormat=\u0026#39;mp3\u0026#39;, VoiceId=\u0026#39;Joanna\u0026#39; ) # 4. Save MP3 file to output folder # Change name from input/abc.txt to output/abc.mp3 new_key = object_key.replace(\u0026#34;input/\u0026#34;, \u0026#34;output/\u0026#34;).replace(\u0026#34;.txt\u0026#34;, \u0026#34;.mp3\u0026#34;) if \u0026#34;AudioStream\u0026#34; in response: with response[\u0026#34;AudioStream\u0026#34;] as stream: s3.put_object( Bucket=bucket_name, Key=new_key, Body=stream.read(), ContentType=\u0026#39;audio/mpeg\u0026#39; ) return \u0026#34;Done!\u0026#34; except Exception as e: print(e) raise e Click the Deploy button. Create Trigger Right above the code section, click the + Add trigger button. Source: Select S3. Bucket: Select the bucket s3-demo-text. Event types: Select All object create events. Prefix: Enter input/ ⚠️ Note: You must enter input/. If left blank, Lambda will trigger even when the MP3 file is created -\u0026gt; Causing an infinite loop -\u0026gt; Increasing costs. Suffix: Enter .txt Check the box \u0026ldquo;I acknowledge\u0026hellip;\u0026rdquo; -\u0026gt; Click Add. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.Since I am performing this workshop for the first time, I will grant full access to the selected permissions.\n{\r{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;polly:*\u0026#34;,\r\u0026#34;s3:*\u0026#34;,\r\u0026#34;logs:*\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r}\r} "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Reduce your Amazon ElastiCache costs by up to 60% with Valkey and CUDOS by Chris Gillespie, Brenno Passanha, and Yuriy Prykhodko | on 04 SEP 2025 | in Amazon ElastiCache, Cloud Cost Optimization, Intermediate (200), Technical How-to | Permalink\nIn this post, we show you how to save costs on Amazon ElastiCache by upgrading your cluster engine to ElastiCache for Valkey. If you’re currently using ElastiCache for Redis OSS, you can achieve up to 60% cost savings by upgrading to Valkey.\nWe also show you the CUDOS dashboard, part of the open source Cloud Intelligence Dashboards (CID) framework, to see the ElastiCache Redis-to-Valkey cost savings available to your organization, track progress achieving these, and prioritize upgrade efforts.\nAmazon ElastiCache for Valkey Valkey is an open source, high performance, key-value datastore stewarded by Linux Foundation and backed by over 50 companies including Ericsson, Google Cloud, Aiven, Oracle, Percona, ByteDance and Amazon Web Services (AWS). Valkey is a drop-in replacement for Redis OSS and has seen rapid adoption since its project inception. Valkey is available as a managed service through ElastiCache.\nWith ElastiCache for Valkey, you can choose between self-designed (node-based) clusters and serverless caching deployment options. ElastiCache for Valkey Serverless is priced 33% lower than other supported engines, and self-designed (node-based) clusters are priced 20% lower. You can move to these lower prices through in-place, zero downtime upgrades of your existing ElastiCache clusters from Redis OSS to Valkey. Your existing ElastiCache for Redis OSS Reservations will continue to apply.\nUpgrading to ElastiCache for Valkey When ElastiCache applies the upgrade from Redis OSS to Valkey, it follows the same process as a major version upgrade to the existing Redis OSS engine. If your cluster is self-designed, you should follow the upgrade best practices.\nWhen starting the upgrade, you choose the major version of Valkey. Valkey is available from version 7.2, which includes the changes from Redis OSS up to version 7.2.4. If your cluster is running a version of Redis OSS earlier than 7.2.4, you should take changes that could impact your workload into account.\nUpgrading to version 8.1 will bring additional benefits such as faster scaling, Bloom filters, conditional updates (and more). Version 8.0 included an improvement in memory utilization over Valkey 7.2 (and thus all Redis OSS versions), with a further improvement released in version 8.1. The following chart shows the memory improvements for a sample workload. A deeper exploration can be found in the post Year One of Valkey: Open-Source Innovations and ElastiCache version 8.1.\nA 40% improvement in memory usage might allow you to downsize your node type in a self-designed cluster. This would deliver a further cost saving of 50% for the same workload. Your overall savings in this case would be 60% (1 – (0.8 * 0.5)).\nYou can start the ElastiCache Redis OSS to Valkey upgrade through the AWS Management Console, AWS Command Line Interface (AWS CLI), AWS API, or AWS CloudFormation.\nThe console – For self-designed or serverless clusters, follow the instructions in Upgrade from ElastiCache for Redis OSS to ElastiCache for Valkey in Get started with Amazon ElastiCache for Valkey. AWS CLI – Use the modify-cache-cluster or modify-serverless-cache commands. Set the engine to valkey and specify a major version. For example, to upgrade a self-designed cluster to Valkey version 8.0, use the following: aws elasticache modify-cache-cluster \\ --cache-cluster-id my-cluster \\ --engine valkey \\ —engine-version 8.0 For a serverless cluster, use:\naws elasticache modify-serverless-cache-instance \\ --cache-cluster-id my-serverless-cluster \\ --engine valkey \\ --engine-version 8.0 API – Use the ModifyCacheCluster or ModifyServerlessCacheInstance API, setting the Engine parameter to valkey and EngineVersion to the desired major version. CloudFormation – Update the CacheClusterEngine and CacheClusterEngineVersion properties in your AWS::ElastiCache::CacheCluster or AWS::ElastiCache::GlobalReplicationGroup resources. The status of your cluster will show as modifying during the upgrade and change back to available once complete. The upgrade process will automatically handle the migration of your data. Your cluster will remain available throughout the upgrade.\nPlanning and prioritizing migration efforts, understanding per-resource potential savings, and tracking efficiency gains at scale require comprehensive visibility. To streamline these tasks, AWS offers the CUDOS dashboard, which provides detailed insights into cost-saving opportunities across your ElastiCache resources.\nThe Cloud Intelligent Dashboards Framework CUDOS dashboard is part of the open source Cloud Intelligence Dashboards (CID) framework, which you can deploy in your AWS account using the provided infrastructure as code (IaC) templates. The framework helps you drive financial accountability, optimize costs, and increase operational efficiency across your AWS organizations. The CUDOS dashboard provides detailed and actionable insights, enabling data-driven decisions for cost efficiency across your AWS infrastructure.\nElastiCache upgrade insights in CUDOS CUDOS version 5.6 introduces a comprehensive ElastiCache section that transforms your Redis-to-Valkey migration into a data-driven optimization strategy. The dashboard provides:\nVisual adoption tracking – Monitor your adoption progress with clear breakdowns showing the mix of Redis OSS, Valkey self-designed, and Valkey serverless clusters over time. Quantified savings – See your historical cost efficiency gains achieved from clusters already upgraded to Valkey. Granular recommendations – Per-cluster analysis showing current Redis OSS costs, estimated Valkey costs, and exact savings potential for each AWS account and ElastiCache cluster. The dashboard’s detailed cost breakdown table shows exactly which clusters offer the highest savings potential—with current costs on Redis OSS, estimated costs if upgraded to Valkey, and projected monthly savings. You can filter these savings opportunities based on your company taxonomy, such as cost allocation tags, organizational units, cost categories, or other dimensions that align with your organization’s decision-making process.\nGetting started with CUDOS To get started with CUDOS, you can explore the ElastiCache section in an interactive demo dashboard. Follow the deployment guide to set up CUDOS in your organization.\nIf you’re already using CUDOS, follow the update guidance to upgrade to version 5.6. You can also use the add organizational taxonomy guide to add the ability to filter visuals by tags, cost categories, and other dimensions of your organizational taxonomy.\nYou can explore the Cloud Intelligence Dashboards page to learn about the different dashboards for your operational insights.\nConclusion ElastiCache for Valkey is a drop-in replacement for ElastiCache for Redis OSS. By upgrading your existing ElastiCache for Redis OSS clusters to ElastiCache for Valkey you will:\nSee a 33% cost reduction for serverless clusters, or 20% cost reduction for self-designed (node-based) clusters. Be able to continue using your existing ElastiCache for Redis OSS Reservations with your clusters. Use an in-place, zero downtime upgrade process with minimal application disruption to upgrade existing clusters. See an improvement in ElastiCache memory utilization, with the potential of downsizing the nodes in a self-designed cluster, delivering an additional 50% cost saving. To support calculating and tracking these efficiency gains across your organization and prioritizing migration efforts, you can use CUDOS dashboard, which provides a comprehensive ElastiCache Redis-to-Valkey migration section.\nAt AWS, we believe Valkey represents the future of in-memory data stores. For a step-by-step guide on how to get started with ElastiCache for Valkey, see Get started with Amazon ElastiCache for Valkey, or visit the Amazon ElastiCache documentation.\nAbout the authors Chris Gillespie Chris is a UK-based Senior Solutions Architect. He spends most of his work with fast-moving “born in the cloud” customers. Outside of work, he fills his time with family and trying to get fit.\nBrenno Passanha Brenno is a Senior Technical Account Manager. He is part of the Cloud Operations Technical Field Community, focusing on Cloud Financial Management. Outside of work, Brenno enjoys raising his children, traveling the world, and creating memories through new experiences.\nYuriy Prykhodko Yuriy is a Principal Technical Account Manager based in Luxembourg. He helps customers build highly reliable and cost effective systems on AWS. He is also a FinOps SME and Cloud Intelligence Dashboards author and lead. In his free time he enjoys playing basketball and traveling all around the world with his family and friends.\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Acquire an understanding of natural language processing to support project development. Collaborate with team to determine and prioritize the features to be implemented in the application for the project. Tasks to be carried out this week: Day Task Date Reference Material 2 - Study classification and vector spaces in natural language processing.\n+ Sentiment analysis\n+ Vector space models\n+ Machine translation\n+ Document search 09/15/2025 Natural Language Processing with Classification and Vector Spaces 3 - Go to AWS office and discuss with team about which features to be implemented in the application.\n- Study probabilistic models for NLP.\n+ Autocorrect \u0026amp; Autocomplete\n+ Hidden Markov models\n+ Language models\n+ Word embeddings 09/16/2025 Natural Language Processing with Probabilistic Models 4 - Study recurrent neutral networks for NLP.\n+ Gated recurrent units\n+ Long short-term memory units\n+ Named entity recognition\n+ Siamese networks 09/17/2025 Natural Language Processing with Sequence Models 5 - Study attention models.\n+ Neutral machine translation\n+ Text summarization\n+ Question answering 09/18/2025 Natural Language Processing with Attention Models 6 - Meet with team to assign tasks and set deadlines.\n- Study basic JavaScript syntax and features. 09/19/2025 Interactivity with JavaScript Week 2 Achievements: Gained an understanding of natural language processing through the \u0026ldquo;Natural Language Processing\u0026rdquo; specialization on Coursera, covering: Classification and vector spaces Probabilistic models Sequence models Attention models Collaborated with the team at the AWS office to identify and prioritize key features for the project application. Assigned tasks and set deadlines with the team. Acquired foundational knowledge of JavaScript syntax and features through Coursera’s \u0026ldquo;Interactivity with JavaScript\u0026rdquo; course. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: “DevOps on AWS” Event Objectives Understand the core principles of DevOps culture and key performance metrics (DORA, MTTR). Master the AWS CI/CD toolchain for automating build, test, and deployment. Learn Infrastructure as Code (IaC) concepts using CloudFormation and AWS CDK. Explore containerization strategies using ECR, ECS, EKS, and App Runner. Implement full-stack observability and monitoring with CloudWatch and X-Ray. Speakers AWS DevOps Specialists Senior Solutions Architects Key Highlights DevOps Culture \u0026amp; Mindset Recap: Integration with AI/ML concepts from previous sessions. Metrics Matter: Focus on Deployment Frequency, Lead Time for Changes, Mean Time to Restore (MTTR), and Change Failure Rate (DORA metrics). Cultural Shift: Moving from siloed teams to shared responsibility. AWS CI/CD Pipeline Source Control: Utilizing AWS CodeCommit and implementing Git strategies like GitFlow and Trunk-based development. Build \u0026amp; Test: Configuring AWS CodeBuild for automated testing and compilation. Deployment: Using AWS CodeDeploy to implement safe deployment strategies: Blue/Green: Reduces downtime and risk. Canary: Gradual rollout to a small subset of users. Rolling: Update instances incrementally. Orchestration: Tying it all together with AWS CodePipeline. Infrastructure as Code (IaC) AWS CloudFormation: Defining infrastructure using templates, stacks, and managing configuration drift. AWS CDK (Cloud Development Kit): Using familiar programming languages to define cloud resources as code constructs. Comparison: Choosing between declarative templates (CloudFormation) vs. imperative code (CDK) based on team skills. Container Services \u0026amp; Observability Container Management: Storing images in Amazon ECR with lifecycle policies. Orchestration: Choosing between Amazon ECS (simpler, AWS-native) and Amazon EKS (Kubernetes standard), or AWS App Runner for simplified PaaS-like deployment. Monitoring: Using Amazon CloudWatch for metrics/alarms and AWS X-Ray for distributed tracing to identify performance bottlenecks. Key Takeaways DevOps Strategy Automation First: Manual deployments are error-prone; everything from infrastructure to code deployment should be automated. Measurement: You cannot improve what you do not measure. Use DORA metrics to track velocity and stability. Shift Left: Integrate testing and security early in the CI/CD pipeline, not at the end. Technical Architecture Immutable Infrastructure: Treat servers as disposable resources; replace them rather than patching them in place. Containerization: Decouple applications from the underlying OS to ensure consistency across environments (Dev, Test, Prod). Observability: Moving beyond simple \u0026ldquo;up/down\u0026rdquo; monitoring to deep insights using distributed tracing. Applying to Work Implement CI/CD: Set up a CodePipeline for current projects to automate the build/deploy process for Spring Boot/React applications. Adopt IaC: Start defining AWS resources (databases, S3 buckets, Cognito User Pools) using AWS CDK instead of the console. Containerize: Dockerize existing microservices and push images to ECR. Enhance Monitoring: Add X-Ray instrumentation to backend services to visualize API latency and database query performance. Event Experience Attending the “DevOps on AWS” workshop provided a practical roadmap for automating the software delivery lifecycle. It bridged the gap between writing code and running it reliably in production. Key experiences included:\nLearning from experts Gained clarity on the \u0026ldquo;Alphabet Soup\u0026rdquo; of AWS tools (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) and how they integrate. Understood the strategic value of DORA metrics in justifying DevOps investments to business stakeholders. Hands-on technical exposure CI/CD Walkthrough: The demo of a full pipeline showed exactly how code changes trigger builds and deployments automatically. IaC Implementation: Seeing AWS CDK in action was a highlight—writing infrastructure in Java/TypeScript is much more intuitive for developers than writing JSON/YAML templates. Deployment Strategies: Visualizing Blue/Green deployments demonstrated how to release updates with zero downtime. Networking and discussions Discussed the trade-offs between ECS and EKS with peers, realizing that for many projects, ECS or App Runner provides a faster path to production with less overhead. Exchanged ideas on how to handle database migrations within a CI/CD pipeline. Lessons learned Drift Detection in CloudFormation is critical for maintaining infrastructure integrity. Observability is not optional for microservices; without X-Ray, debugging distributed architectures is nearly impossible. A solid DevOps foundation significantly reduces Mean Time to Recovery (MTTR), allowing teams to innovate faster with less fear of breaking production. Event photo "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Word document version (right-click and choose Save link as… to download)\nMealPlan Personalized Food Ingredient Sales Platform 1. Executive Summary The Personalized Food Ingredient Sales Platform focuses on enabling faster and more efficient shopping. Users register accounts to access a diverse recipe database, receive AI-driven meal suggestions based on purchase history, and order with doorstep delivery. Leveraging AWS cloud infrastructure, the platform ensures flexible scalability, high performance, and secure management.\n2. Problem Statement Current Problem\nCustomers face challenges in efficiently sourcing ingredients for meal preparation, often spending significant time searching for supplies that match recipes. Existing platforms offer recipe and menu suggestions but lack integrated ingredient purchasing, requiring users to source supplies independently, which is time-consuming and prone to errors in portioning.\nSolution\nThe platform employs Spring Boot for a robust REST API back-end handling user registration, recipe management, shopping cart, and order processing, with React delivering a user-friendly front-end featuring AI-driven meal recommendations. Data is stored in Amazon EC2 (PostgreSQL), images and static assets in Amazon S3, with front-end, back-end, AI model deployed on another Amazon EC2 instance within a secure VPC, and Route 53 managing the domain. Users can customize portions, receive personalized recipe suggestions, and order ingredients for delivery. Key features include a diverse recipe library, accurate calorie calculations, and relatively low operational costs.\nBenefits \u0026amp; Return on Investment The solution establishes a comprehensive platform for the nutrition startup to expand services while collecting user data for enhanced recommendation systems. It eliminates manual calorie calculations and fragmented shopping through an integrated system, simplifying nutrition planning and driving revenue from ingredient sales. Estimated monthly cost is $119.51 (per AWS Pricing Calculator), totaling approximately $1434.12 for 12 months. Development leverages open-source frameworks, incurring no additional hardware costs. ROI is expected within 6–12 months through user time savings and consistent order revenue.\n3. Solution Architecture The website is hosted on EC2. Data is stored on an EC2 instance. Images are stored in S3. Code is pushed to GitHub for management and automatically uploaded to S3 so CodeDeploy can deploy the application to the server. CloudFront is used to improve loading performance. Cognito manages user identities. CloudTrail monitors and stores activity logs. CloudWatch monitors and manages performance and the health of AWS resources and applications. IAM grants permissions to services. Secrets Manager stores sensitive information.\nAWS Services Used\nWAF: Protects the web application from cyber-attacks. AWS CloudFront: Improves website loading speed. AWS EC2: Hosts the application, NAT instance, and database. AWS VPC: Virtual private network. AWS S3: Stores code, log files, and images. CodeDeploy: Deploys code to EC2. GitLab: Hosts source code and pushes it to S3. Amazon Cognito: Manages user authentication for the web application. IAM: Creates users and roles. Secrets Manager: Stores sensitive information. CloudTrail: Monitors and stores activity logs. CloudWatch: Monitors and manages the performance and health of AWS resources. 4. Technical Deployment Deployment Phases\nThis project includes two main parts: developing the Spring Boot backend and the React frontend, and deploying the website on AWS using AWS services. Each part includes four phases.\nTheory \u0026amp; Architecture Design: Gather web application requirements, design the system architecture (Spring Boot REST API + React frontend), and define the database schema. (January)\nDevelopment \u0026amp; Testing: Implement the Spring Boot backend with REST APIs (authentication, user management, meal/recipe CRUD, shopping cart, etc.) and build the React frontend (UI/UX, routing, forms, state management). Conduct unit tests for backend services, integration tests for API endpoints, and frontend tests (Jest/React Testing Library). (January–February)\nCost Estimation \u0026amp; Feasibility Check: Use the AWS Pricing Calculator to estimate costs for EC2 (backend hosting), EC2 (database), S3 (static files and images), VPC (networking), and Route 53 (domain). Adjust as needed. (February)\nAWS Integration: Integrate AWS services into the application. Deploy the website on EC2, store images on S3, configure EC2 for the database, use VPC for networking, Route 53 for domain management, and set up CI/CD pipelines (GitHub Actions or AWS CodePipeline). Perform staging tests before official release. (March)\nTechnical Requirements\nBackend (Spring Boot): REST APIs for authentication, user management, meal/recipe CRUD, shopping cart, and order processing. Includes security (JWT, Spring Security). Frontend (React): Responsive web application with a user-friendly UI/UX integrated with the backend API. Database (EC2): Relational database (MySQL/PostgreSQL) hosted on EC2, storing users, recipes, shopping carts, and order data. Storage (AWS S3): Used to store user-uploaded images. Hosting \u0026amp; Networking (AWS EC2 \u0026amp; AWS VPC): Application deployed on EC2 instances. CI/CD (GitHub Actions or AWS CodePipeline): Automated build and deployment pipeline for backend and frontend. Authentication \u0026amp; Security: JWT authentication and HTTPS configuration; optional AWS Cognito for user access management. 5. Roadmap \u0026amp; Deployment Milestones January: Build theoretical foundation and draw architecture (Spring Boot backend + React frontend design, database schema). Begin initial backend and frontend development. February: Continue backend and frontend development, perform unit and integration tests. Use AWS Pricing Calculator to evaluate hosting costs and refine architecture for cost efficiency. March: Integrate AWS services, configure CI/CD pipelines, conduct staging tests, and deploy the website to production. Post-launch: Up to 3 months for maintenance, optimization, and feature improvements. 6. Cost Estimate Costs can be viewed via the AWS Pricing Calculator.\nAWS Services AWS WAF: $11.6/month Application Load Balancer (ALB): $18.63/month Amazon EC2 Application: $19.27/month Amazon EC2 Data Tier: $9.64/month Amazon EC2 NAT Instances: $19.27/month Amazon S3: $3.72/month AWS CodeDeploy: $0 AWS Secrets Manager: $0.4/month Amazon Cognito: $14.25/month Amazon CloudWatch: $4.91/month AWS CloudTrail: $1.77/month VPC Endpoints: $16.05/month Total: $119.51/month, $1,434.12/year\n7. Risk Assessment Risk Matrix\nNetwork loss: Medium impact / Medium probability. EC2 / ALB / AZ failure: High impact / Low–medium probability. Secret leakage: High impact / Low–medium probability. Cost overrun: Medium impact / Medium probability. Mitigation Strategies\nAvailability: Multi-AZ + Auto Scaling + ALB; health checks; caching with CloudFront; Route 53 failover. Security: IAM with least-privilege; Secrets Manager with rotation and access logging; WAF/Shield protection. Cost Management: AWS Budgets \u0026amp; Cost Explorer; rightsizing; Reserved or Spot Instances where appropriate. Fallback Plan\nAutomatic rollback using CodeDeploy. Manual fallback: GitLab runner → prebuilt AMIs or ASGs. 8. Expected Outcomes Fully automated CI/CD (GitLab → CodePipeline → CodeBuild → CodeDeploy) reduces manual errors. Multi-layer security (IAM, WAF, Secrets Manager) with auditing via CloudTrail. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Getting started with Amazon EC2 bare metal instances for Amazon RDS for Oracle and Amazon RDS Custom for Oracle by Sameer Malik and Nitin Saxena | on 04 SEP 2025 | in Amazon EC2 Bare Metal, Amazon RDS, Amazon RDS Custom, Intermediate (200), RDS for Oracle, Technical How-to | Permalink\nAmazon Relational Database Service (Amazon RDS) for Oracle is a fully managed commercial database that makes it straightforward to set up, operate, and scale Oracle deployments in the cloud. Amazon RDS Custom for Oracle makes it possible for you as a database administrator to access and customize your Oracle database environment and operating system.\nIn this post, we explore the support for AWS bare metal instances on Amazon EC2 bare metal Instances for Amazon RDS for Oracle and RDS Custom for Oracle.\nAmazon EC2 bare metal instances are designed to provide your applications with direct access to the processor and memory of the underlying server. Amazon EC2 bare metal instances provide capacity fully dedicated for use in a non-shared tenancy model and offer a higher level of isolation compared to shared virtualized instances.\nThese bare metal instances might also offer additional licensing benefits to use your eligible software licenses for Bring Your Own License (BYOL) licensing models from vendors such as Microsoft and Oracle. For additional details on the licensing benefits of using Amazon EC2 bare metal instances, refer to the post Oracle Licensing Efficiency With Dedicated Hosts from AWS licensing partner House of Brick.\nAdditionally, Amazon EC2 bare metal instances on Amazon RDS for Oracle and RDS Custom for Oracle are offered at 25% lower compute cost than virtualized instances of the same size, making Oracle workloads on RDS bare metal instances more cost-effective.\nCommon use cases for EC2 bare metal instances The following are common use cases for EC2 bare metal instances:\nSupport for workloads with restrictive licensing – This is often the primary driver, especially for traditional enterprise software like Oracle Database, SQL Server, SAP, or Windows Server, where licensing might be more favorable on physical cores or dedicated hardware. High-performance computing (HPC) and scientific simulations – These workloads require the absolute highest performance, direct access to specialized hardware features, or very low-latency inter-node communication. Legacy applications – These applications are not supported in virtualized environments or require specific hardware access. Isolation and security – Bare metal instances offer a higher level of isolation compared to shared virtualized instances, which can be beneficial for highly sensitive workloads or compliance requirements. It also serves as an option when your organization is looking to run certain applications on a single-tenant infrastructure in non-virtualized environments to meet any specific corporate compliance and regulatory requirements. Benefits of using RDS bare metal instances on Amazon RDS for Oracle and RDS Custom for Oracle RDS bare metal instances offer the following benefits:\nLicensing benefits – Because RDS bare metal instances provide capacity fully dedicated for your use in a non-shared tenancy model, the licensing can become more akin to on-premises deployments where customers (depending on their contract with Oracle) can apply traditional core-based licensing and Oracle processor core factor. Lower compute cost – RDS bare metal instances for Amazon RDS for Oracle and RDS Custom for Oracle offer 25% lower compute costs compared to equivalent virtualized instances. For example, an m6i.metal bare metal instance costs 25% less than an m6i.32xlarge virtualized instance. Effective database consolidation – With the licensing benefits and lower compute costs for bare metal instances on Amazon RDS for Oracle and RDS Custom for Oracle, you can effectively consolidate your database workloads using currently supported database consolidation methods such as schema consolidation and Oracle multi-tenant options. Enhanced performance for specific workloads – Customers running their Oracle Database workload on the virtualized environment and utilizing more than half the cores of the entire physical server can now enjoy the performance of the full physical server (such as CPU, memory, IOPS, and throughput) at no additional licensing cost. Create an RDS for Oracle bare metal instance using the Amazon RDS console In this section, we demonstrate how to create an RDS for Oracle DB instance with the bare metal instance type using the AWS Management Console. For details and prerequisites, refer to Create an Oracle DB instance.\nSign in to the Amazon RDS console. In the upper-right corner of the Amazon RDS console, choose the AWS Region in which you want to create the DB instance. In the navigation pane, choose Databases. Choose Create database. Select Standard create. For Engine type, select Oracle. For Database management type, select Amazon RDS. For Edition, select Oracle Enterprise Edition. For License model, leave the default Bring Your Own License (BYOL). For Engine version, choose your preferred version. In the Templates section, select Production. For DB instance identifier, enter a name for your DB instance. In the Credentials Settings section, provide a username for the admin user, select Self-managed for Credentials management, and enter your password. For Encryption key, use your choice of encryption key. In the Instance configuration section, for the DB instance class, choose a bare metal instance (for example, db.m6i.metal). Complete the rest of the instance creation steps and choose Create database. After you create your DB instance, you can verify its status on the Amazon RDS console.\nCreate an RDS for Oracle bare metal instance using the AWS CLI You can also create a bare metal instance using the AWS Command Line Interface (AWS CLI), as shown in the following code. Make sure the AWS CLI is configured with the necessary credentials and default Region.\naws rds create-db-instance \\ --db-instance-identifier metaldb \\ --db-instance-class db.m6i.metal \\ --engine oracle-ee \\ --allocated-storage 100 \\ --master-username \u0026lt;username\u0026gt; \\ --master-user-password \u0026lt;YourStrongPassword\u0026gt; \\ --engine-version 19.0.0.0.ru-2025-04.spb-1.r1\\ --license-model license-included \\ --publicly-accessible false \\ --storage-encrypted \\ --vpc-security-group-ids sg-xxxxxxxxxxxxxxxxx \\ --db-subnet-group-name my-db-subnet-group Clean up To avoid paying the cost for running the RDS instance, delete the above provisioned RDS instance. You can delete a DB instance using the AWS Management Console, the AWS CLI, or the RDS API. For more details on the how to delete the RDS instance, refer to Deleting a DB instance.\nConclusion With Amazon EC2 bare metal instances for Amazon RDS for Oracle and RDS Custom for Oracle, you can benefit from additional licensing flexibility and lower cost options to effectively run your Oracle Database workloads on Amazon RDS for Oracle and RDS Custom for Oracle. For more details about this launch, refer to What’s new.\nAbout the authors Sameer Malik Sameer has been working on relational Databases for over 23 years now and is currently working as a Principal Database Solution Architect at AWS focused on RDS and Aurora. He has helped several customers with the migration and modernization of their Database workloads to AWS.\nNitin Saxena Nitin is a Software Development Manager in RDS DBS Managed Commercial Engines with Amazon Web Services. He focuses on services like RDS Oracle and RDS Custom for Oracle. He enjoys designing and developing new features on RDS Oracle and RDS Custom to solve customer problems.\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Gain foundational knowledge of front-end development using React.js to support project application development. Collaborate with the team to develop key components of the project’s web application, including seller panel features. Acquire an understanding of containerization concepts to prepare for deployment of the project. Tasks to be carried out this week: Day Task Date Reference Material 2 - Study the basics of developing applications using React.js. 09/22/2025 Developing Front-End Apps with React 3 - Visit the AWS office and start working on the first project.\n+ Initiated a product management page for the web application 09/23/2025 4 - Continue project development.\n+ Develop the product management page 09/24/2025 5 - Finish the first task in the project.\n+ Complete the product management page. 09/25/2025 6 - Meet with team to review progress and plan for subsequent tasks.\n- Study containerization concepts.\n+ Docker\n+ Kubernetes\n+ OpenShift\n+ Istio 09/26/2025 Introduction to Containers w/ Docker, Kubernetes \u0026amp; OpenShift Week 3 Achievements: Acquired knowledge of React.js through the Coursera course \u0026ldquo;Developing Front-End Apps with React\u0026rdquo;, covering: Creation and management of components and props Implementation of state and lifecycle methods Handling events for interactive user interfaces Developed a seller panel for the project’s web application, including: Product management Order management Dashboard Gained an understanding of containerization through the Coursera course \u0026ldquo;Introduction to Containers w/ Docker, Kubernetes \u0026amp; OpenShift\u0026rdquo;, covering: Docker basics Kubernetes orchestration OpenShift fundamentals Reviewed project progress with the team, assigned new tasks. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Serverless generative AI architectural patterns – Part 1 This two-part series explores the different architectural patterns, best practices, code implementations, and design considerations essential for successfully integrating generative AI solutions into both new and existing applications. In this post, we focus on patterns applicable for architecting real-time generative AI applications. Part 2 addresses patterns for building batch-oriented generative AI implementations using serverless services.\nBlog 2 - Reduce your Amazon ElastiCache costs by up to 60% with Valkey and CUDOS This blog shows you how to save costs on Amazon ElastiCache by upgrading your cluster engine to ElastiCache for Valkey. If you’re currently using ElastiCache for Redis OSS, you can achieve up to 60% cost savings by upgrading to Valkey.\nBlog 3 - Getting started with Amazon EC2 bare metal instances for Amazon RDS for Oracle and Amazon RDS Custom for Oracle This blog explores the support for AWS bare metal instances on Amazon EC2 bare metal Instances for Amazon RDS for Oracle and RDS Custom for Oracle.\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Strengthen understanding of React.js by implementing components for the seller panel. Developing management interfaces such as order and inventory pages. Review progress and plan for the upcoming week. Tasks to be carried out this week: Day Task Date Reference Material 2 - Develop an order management page for the seller panel. 09/29/2025 3 - Continue developing the order management page. 09/30/2025 4 - Develop a dashboard page for the seller panel. 10/01/2025 5 - Finish developing the order management and the dashboard page. 10/02/2025 6 - Team meeting to review progress and determine tasks for the next week.\n- Implement an inventory management (ingredient management) page. 10/03/2025 Week 4 Achievements: Developed and integrated the order management page for the seller panel, enabling sellers to view, update, and track orders. Created a dashboard for the seller panel, providing insights into sales performance, order volume, and product statistics. Implemented an inventory management page that allows sellers to monitor and manage ingredients. Collaborated with the team to review progress, discuss design improvements, and plan next week’s development tasks. Gained deeper hands-on experience in building front-end components with React.js. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/4-eventparticipated/",
	"title": "Events participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in four events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud Mastery Series #1\nDate \u0026amp; Time: 08:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #2\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #3\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: Building Agentic AI \u0026amp; Context Optimization with Amazon Bedrock\nDate \u0026amp; Time: 09:00, December 05, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/5-workshop/5.5-testing/",
	"title": "Testing",
	"tags": [],
	"description": "",
	"content": "Create input file On your computer, create a file named test.txt. Create with the text: \u0026ldquo;Hello, congratulations on completing the workshop!\u0026rdquo; Save file Return to the S3 tab, open the input folder. Click Upload -\u0026gt; Select test.txt -\u0026gt; Click Upload Wait for about 5 seconds.\nGo back to the Bucket console, then open the output folder. You will see the file test.mp3. Select it and click Download.\nOpen it to listen.\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nAutomated Text-to-Speech Converter using Serverless Overview Event-Driven Architecture allows you to build applications that automatically respond to changes in state, such as a file upload, without provisioning or managing servers.\nIn this lab, you will learn how to create, configure, and test a Serverless pipeline that automatically converts text files into lifelike speech audio using AWS Managed Services.\nYou will utilize two key mechanisms to process data asynchronously:\nS3 Event Notifications - Configure Amazon S3 to act as an event source. It will automatically trigger a compute function whenever a new object is uploaded to a specific input folder. Managed AI Services - Leverage Amazon Polly to synthesize speech from text. This allows you to add deep learning capabilities to your application through simple API calls, without needing data science expertise. Content Workshop overview Prerequisite Build Core Logic (Lambda) Configure Storage (S3) Testing Result Clean up "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Finalize the remaining features in the seller panel and ensure system stability through integration testing. Improve specialized knowledge and translation skills by translating technical blog articles. Review progress and plan for the next project phase. Tasks to be carried out this week: Day Task Date Reference Material 2 - Complete the inventory management page. 10/06/2025 3 - Conduct integration testing for the seller panel. 10/07/2025 4 - Translate the first technical blog. 10/08/2025 Serverless generative AI architectural patterns – Part 1 5 - Translate the second technical blog. 10/09/2025 Reduce your Amazon ElastiCache costs by up to 60% with Valkey and CUDOS 6 - Translate the third technical blog.\n- Meet with team to review project progress and determine next steps. 10/10/2025 Getting started with Amazon EC2 bare metal instances for Amazon RDS for Oracle and Amazon RDS Custom for Oracle Week 5 Achievements: Completed the inventory management page. Conducted integration testing, ensuring functionality and data connections among seller panel pages (product, order, inventory and dashboard). Translated three technical blog articles, improving understanding and expression of specialized technical content in Vietnamese. Met with team to discuss results and propose the next development phase. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for building Serverless Event-driven applications on AWS.\nBy configuring S3 Event Notifications, you enabled an automated workflow where compute resources (AWS Lambda) react instantly to data ingestion without the need for manual intervention or server management.\nBy integrating Amazon Polly, you successfully leveraged Managed AI Services to transform text into lifelike speech, demonstrating how to add complex machine learning capabilities to your application with minimal code.\nClean up Delete S3 buckets Open S3 console Open s3-demo-bucket Choose 2 folders input and output Click delete and confirm Then choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. Delete Lambda function Open the lambda console Find workshop1 and click action Choose Delete and confirm Delete IAM role Open IAM console Select Roles from the menu on the left. Find the role PollyLambdaRole. Select the role and click Delete. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Review and deepen knowledge of AWS services and architectures in preparation for the midterm examination. Tasks to be carried out this week: Day Task Date Reference Material 2 - Review computing services.\n+ Amazon EC2\n+ Amazon EBS\n+ Amazon EFS\n+ Amazon Lightsail\n+ AWS Lambda\n- Learn about AWS container services.\n+ Amazon ECS\n+ Amazon EKS\n+ Amazon ECR\n+ AWS Batch 10/13/2025 FCJ Bootcamp Module 3\nEC2 Documentation\nEBS Documentation\nEFS Documentation\nLightsail Documentation\nLambda Documentation\nECS Documentation\nEKS Documentation\nECR Documentation\nBatch Documentation 3 - Review and study more database services.\n+ Amazon RDS\n+ Amazon Aurora\n+ Amazon DynamoDB\n+ Amazon DocumentDB\n+ Amazon ElastiCache\n+ Amazon MemoryDB\n+ Amazon Neptune\n+ Amazon Timestream 10/14/2025 FCJ Bootcamp Module 6\nRDS and Aurora Documentation\nDynamoDB Documentation\nDocumentDB Documentation\nElastiCache Documentation\nMemoryDB Documentation\nNeptune Documentation\nTimestream Documentation 4 - Learn more about computing and serverless services.\n+ Amazon Elastic Beanstalk\n+ AWS Fargate\n+ Elastic Load Balancing\n+ AWS App Runner\n+ Amazon GameLift Servers\n+ Amazon Amplify\n+ AWS AppSync\n+ Amazon Location Service 10/15/2025 Elastic Beanstalk Documentation\nFargate Documentation\nElastic Load Balancing Documentation\nApp Runner Documentation\nGameLift Servers Documentation\nAmplify Documentation\nAppSync Documentation\nLocation Service Documentation 5 - Review and learn more about storage services.\n+ Amazon S3\n+ AWS Snowball Edge\n+ AWS Storage Gateway\n+ AWS Backup\n+ AWS DRS 10/16/2025 FCJ Bootcamp Module 4\nS3 Documentation\nSnowball Edge Documentation\nStorage Gateway Documentation\nBackup Documentation\nDRS Documentation 6 - Getting started with AWS Well-Architected. 10/17/2025 Well-Architected Overview Week 6 Achievements: Strengthened knowledge across various AWS services: Computing: EC2, Lightsail, ECS, EKS, ECR, Lambda, etc. Storage: S3, EBS, EFS, Snowball Edge, Storage Gateway, etc. Databases: RDS, DynamoDB, DocumentDB, ElastiCache, etc. Gained additional understanding of risk mitigation using Backup and DRS. Learned about cloud architecture with the AWS Well-Architected Framework. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at AWS Vietnam from 09/08/2025 to 12/09/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in machine learning and front-end development for an e-commerce web application, through which I improved my skills in machine learning, data science, programming, communication, teamworking and project management.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ☐ ✅ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Strengthen discipline and motivation to complete tasks on time. Be more proactive in seeking out tasks and taking initiative. Improve problem-solving skills by identifying, proposing solutions, and showing creativity more effectively. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Continue improving knowledge of AWS services and architecture in preparation for the midterm examination. Practice hands-on labs to strengthen understanding of key AWS services. Tasks to be carried out this week: Day Task Date Reference Material 2 - Learn more about AWS Well-Architected. 10/20/2025 AWS Well-Architected 3 - Study Amazon VPC and AWS CloudFormation.\n- Perform hands-on labs for VPC, CloudFormation and EC2. 10/21/2025 VPC Documentation\nCloudFormation Documentation\nInitialize CloudFormation templates\nCreate an EC2 instance\nCreate Security Groups\nUpdate NACLs 4 - Perform hands-on labs for Backup, Storage Gateway and S3.\n- Study S3 in more depth. 10/22/2025 FCJ Bootcamp Lab 13, 24, 57\nS3 Documentation 5 - Explore machine learning services.\n+ Amazon A2I\n+ Amazon SageMaker AI\n+ AWS DLAMI\n+ AWS Deep Learning Containers\n+ AWS Entity Resolution\n+ PyTorch\n+ TensorFlow\n+ Hugging Face 10/23/2025 A2I Documentation\nSageMaker AI Documentation\nDLAMI Documentation\nDeep Learning Containers Documentation\nEntity Resolution Documentation\nPyTorch on AWS\nTensorFlow on AWS\nHugging Face on AWS 6 - Explore specialized machine learning services.\n+ Amazon CodeGuru\n+ Amazon DevOps Guru\n+ Amazon Kendra\n+ Amazon Personalize\n+ Amazon Rekognition\n+ Amazon Comprehend\n+ Amazon Translate\n+ Amazon Textract\n+ Amazon Polly\n+ Amazon Lex 10/24/2025 CodeGuru Documentation\nDevOps Guru Documentation\nKendra Documentation\nPersonalize Documentation\nRekognition Documentation\nComprehend Documentation\nTranslate Documentation\nTextract Documentation\nPolly Documentation\nLex Documentation Week 7 Achievements: Gained further understanding of AWS architectures. Improved knowledge of networking and storage services. Built more experience with compute and storage services. Explored a wide range of AWS machine learning services. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe workplace is friendly and welcoming. Both mentors and colleagues at the office are very supportive and open to sharing their experience. The office space is spacious, comfortable, and aesthetically pleasing, providing a great environment for work.\n2. Support from Mentor / Team Admin\nThe mentors provide clear and detailed guidance whenever I have questions, whether at the office or through the WhatsApp group. Everyone is always ready to assist and respond promptly.\n3. Relevance of Work to Academic Major\nThe tasks I worked on are closely related to my field of study, while also expanding into other areas. As a result, both my professional knowledge and practical skills have improved significantly.\n4. Learning \u0026amp; Skill Development Opportunities\nThroughout the internship, I gained not only technical knowledge but also valuable experience in working within a business environment. This helped me strengthen my discipline and improve my workplace skills.\n5. Company Culture \u0026amp; Team Spirit\nThe company maintains a professional and respectful work culture where team members support one another. Team spirit is shown through collaboration and internal activities that promote connection and inclusion among members.\n6. Internship Policies / Benefits\nThe program offers access to learning resources, technical support, and various events. Moreover, the mentors provide thoughtful assistance whenever interns encounter difficulties.\nSuggestions \u0026amp; Expectations After completing the OJT program, I wish to continue working to finish the full internship program at FCJ. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Strengthen knowledge in preparation for the midterm examination. Visit the AWS office to receive the shirt and take the midterm test. Tasks to be carried out this week: Day Task Date Reference Material 2 - Discuss midterm topics with FCJ peers.\n- Conduct knowledge review and cross-checking with team members. 10/27/2025 3 - Research common questions for the SAA certification examination. 10/28/2025 4 - Read and explore AWS Solutions Library. 10/29/2025 AWS Solutions Library 5 - Continue studying solutions from AWS Solutions Library. 10/30/2025 AWS Solutions Library 6 - Revise the AWS Well-Architected Framework.\n- Visit the AWS office to receive the FCJ shirt and take the midterm examination. 10/31/2025 AWS Well-Architected Week 8 Achievements: Conducted group discussions and strengthened knowledge through peer review sessions. Gained familiarity with common question types for the SAA examination. Improved understanding of solution architectures via AWS Well-Architected and AWS Solutions Library. Took the midterm examination. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Begin developing the initial ideas for the product recommendation system. Clarify the process of collecting, simulating, and standardizing data. Understand the basic architecture of a recommendation system. Tasks to be carried out this week: Day Task Date Reference Material 2 - Find data sources to simulate the database. 10/03/2025 3 - Research product recommendation systems. 10/04/2025 4 - Study TensorFlow and TensorFlow Recommenders. 10/05/2025 TensorFlow Recommenders 5 - Discuss data structure and required data details with team member. 10/06/2025 6 - Build experimental models to reinforce knowledge about recommendation systems.\n- Group meeting to plan the next development tasks. 10/07/2025 Week 9 Achievements: Collected initial data sources for model development. Gained an understanding of the key concepts and approaches to recommendation systems. Became familiar with TensorFlow Recommenders and built basic experimental models. Reached team alignment on data requirements and upcoming tasks. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Complete simulated data preparation for the database. Begin data preprocessing steps to prepare for model training. Tasks to be carried out this week: Day Task Date Reference Material 2 - Work with team to collect and generate data. 11/10/2025 3 - Create additional simulated data for the database. 11/11/2025 4 - Finalize data simulation for the database. 11/12/2025 5 - Begin data preprocessing for the recommendation model. 11/13/2025 6 - Make necessary data adjustments in the database. 11/14/2025 Week 10 Achievements: Completed the collection and generation of necessary data for the system. Built a relatively complete simulated dataset for model training. Started preprocessing and standardizing input data. Ensured the dataset was updated and structured properly for the next development phase. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Complete the data preprocessing phase. Build the TFRS model. Tasks to be carried out this week: Day Task Date Reference Material 2 - Complete the data preprocessing process. 11/17/2025 3 - Start building the recommendation model using TFRS. 11/18/2025 4 - Design and implement the model architecture. 11/19/2025 5 - Fine-tune model hyperparameters. 11/20/2025 6 - Complete training and save the final model. 11/21/2025 Week 11 Achievements: Completed data preprocessing and normalization for model training. Designed and fine-tuned the recommendation model. Successfully trained the model, ready for system integration. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Research AI integration into the application using DJL. Deploy the recommendation model into the system and complete the recommendation feature. Tasks to be carried out this week: Day Task Date Reference Material 2 - Research AI integration and Deep Java Library. 11/24/2025 DJL Documentation 3 - Start integrating the model into the application. 11/25/2025 4 - Working on the recommendation system. 11/26/2025 5 - Complete integration of the recommendation system. 11/27/2025 6 - Team meeting to discuss deployment to AWS infrastructure. 11/28/2025 Week 12 Achievements: Learned how to deploy AI models using DJL. Successfully integrated the recommendation system into the application. Planned deployment strategy to AWS infrastructure after team discussion. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]