[
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/5-workshop/5.3-s3/create-s3/",
	"title": "Create S3",
	"tags": [],
	"description": "",
	"content": "Create S3 Open the S3\nClick Create Bucket:\nIn the Create Bucket console: Specify name of the bucket: s3-demo-text\nDo not add a tag to the VPC endpoint at this time. Click Create bucket Then click on the bucket you just created and press Create folder In the Create folder console: Specify name of the bucket: input then click Create folder We do the same with creating the output file. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/5-workshop/5.1-workshop-overview/",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "INTRODUCTION Introduction to S3 Event Notifications S3 Event Notifications is a feature that allows Amazon S3 to automatically send notifications when specific events occur in your Bucket (for example: when a new file is uploaded, deleted, or copied). S3 can send notifications to various destinations such as AWS Lambda, Amazon SNS, or Amazon SQS. In this workshop, we use the PutObject event to automatically trigger a Lambda function as soon as a text file is uploaded to S3. Introduction to Amazon Polly Amazon Polly is a Text-to-Speech service that uses AWS\u0026rsquo;s advanced Deep Learning technology. Polly supports over 60 voices in more than 20 languages, including Vietnamese, with natural human-like audio quality. This service is fully managed, so you don\u0026rsquo;t need to worry about managing infrastructure or scalability. Workshop Overview In this workshop, you will build a Serverless Text-to-Speech Converter application. The system operates fully automatically based on an Event-driven Architecture model:\nAmazon S3 (Input Bucket): Stores input text files (.txt) uploaded by users. S3 Event Notifications: Detects new file upload events and triggers AWS Lambda. AWS Lambda: The central processor that orchestrates data flow between S3 and Polly. Lambda reads the text file, calls the Polly API for conversion, and saves the result. Amazon Polly: The AI service that converts text into audio with natural voice. Amazon S3 (Output Bucket): Stores output audio files (.mp3) after processing is complete. The entire process runs automatically without manual intervention, helping save time and operational costs.\nSystem Architecture The model below describes the detailed architecture and data flow of the Text-to-Speech Converter system:\nWorkflow:\nUpload file: User uploads a text file (.txt) to Amazon S3 Input Bucket.\nEvent trigger: S3 detects the PutObject event (new file), automatically sends an event notification to trigger AWS Lambda Function.\nLambda processing: Lambda function is triggered and performs:\nReads the text file content from S3 Input Bucket Sends the text content to Amazon Polly API along with parameters (voice ID, output format) Polly conversion: Amazon Polly receives the request, processes the text, and returns an audio stream to Lambda.\nSave result: Lambda receives the audio stream from Polly and saves it as an .mp3 file to Amazon S3 Output Bucket.\nComplete: User can download the MP3 file from S3 Output Bucket for use.\nBenefits of Serverless Architecture No server management: AWS automatically handles scaling, patching, and high availability. Cost optimization: Pay only when processing requests (pay-per-use model). Auto-scaling: The system can handle from a few requests to millions of requests without additional configuration. Fast deployment: Focus on application logic instead of managing infrastructure. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “AWS AI/ML Services \u0026amp; Generative AI Workshop” Event Objectives Provide an overview of the AI/ML landscape in Vietnam. Demonstrate end-to-end machine learning workflows using Amazon SageMaker. Introduce Generative AI capabilities with Amazon Bedrock (Foundation Models, Agents, Guardrails). Share techniques for Prompt Engineering and Retrieval-Augmented Generation (RAG). Speakers AWS Solutions Architects AI/ML Specialists Key Highlights AWS AI/ML Services Overview (SageMaker) End-to-End Platform: Covered the full lifecycle from data preparation to deployment. Data Preparation: Strategies for labeling and cleaning data. Training \u0026amp; Tuning: optimizing models for performance and cost. Deployment: Moving models to production endpoints efficiently. Integrated MLOps: Highlighted capabilities to automate and standardize ML pipelines. SageMaker Studio: Live demo of the unified interface for building, training, and deploying models. Generative AI with Amazon Bedrock Foundation Models (FMs): Comparison and selection guide for top models: Claude: High reasoning capabilities. Llama: Open and efficient. Titan: Native AWS integration. Prompt Engineering: Chain-of-Thought: Breaking down complex reasoning tasks. Few-shot learning: Using examples to improve model output. Advanced Architectures: RAG (Retrieval-Augmented Generation): Integrating Knowledge Bases to ground answers in company data. Bedrock Agents: Creating multi-step workflows and integrating with external tools. Guardrails: Ensuring safety and content filtering. Key Takeaways Design Mindset Model Selection: Choose the right Foundation Model (e.g., Claude vs. Titan) based on the specific use case requirements (speed vs. reasoning). Safety First: Implementation of Guardrails is critical for responsible AI deployment in enterprise settings. Technical Architecture RAG over Fine-tuning: For most internal knowledge use cases, RAG provides a more flexible and cost-effective solution than fine-tuning models. Agentic Workflows: Moving from passive chatbots to active Agents that can execute tasks is the next frontier of GenAI. Modernization Strategy MLOps Adoption: Moving away from manual model training to automated pipelines (MLOps) is essential for scalability. Local Context: Understanding the specific AI/ML trends and landscape within Vietnam helps in benchmarking local projects. Applying to Work Pilot SageMaker: Evaluate current ML workflows and identify opportunities to migrate to Amazon SageMaker for better lifecycle management. Build a Knowledge Bot: Create a prototype using Amazon Bedrock and RAG to query internal documentation or technical manuals. Refine Prompting: Immediately apply Chain-of-Thought and Few-shot techniques to improve the accuracy of current AI interactions. Implement Guardrails: Configure content filters on Bedrock to ensure brand safety for any experimental applications. Event Experience Attending the “AWS AI/ML Services \u0026amp; Generative AI Workshop” provided a practical roadmap for adopting intelligent services. Key experiences included:\nLearning from experts Gained clarity on the AI/ML landscape in Vietnam, understanding local opportunities and challenges. Deepened technical knowledge on the differences between major Foundation Models (Claude, Llama, Titan). Hands-on technical exposure The SageMaker Studio walkthrough demonstrated how to unify the fragmented ML toolchain into a single pane of glass. The Live Demo of Bedrock showed the practical implementation of building a Generative AI chatbot, demystifying the complexity of RAG and Agents. Networking and discussions The ice-breaker activity and networking sessions allowed for exchanging ideas with peers about real-world challenges in deploying GenAI. Discussions reinforced the importance of Prompt Engineering as a critical skill for modern development. Lessons learned RAG is the key to making LLMs useful for business-specific data without the high cost of training. Guardrails are not optional; they are a fundamental layer of the Generative AI stack. Efficiency in ML comes from integrated MLOps rather than isolated data science experiments. Event photo "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyen Truong Phuc Thinh\nPhone Number: +84907511320\nEmail: nguyentruongphucthinh@gmail.com\nUniversity: FPT University\nMajor: Information Technology\nSpecialization: Artificial Intelligence\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Intern\nInternship Duration: From 08/09/2025 to ??/??/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Understand AWS policies and professional workspace etiquette. Connect with members in the First Cloud Journey program. Form a team and develop a project plan. Learn basic AWS services and specialized knowledge for the project. Tasks to be carried out this week: Day Task Date Reference Material 2 - Read AWS FCJ internship policies and guidelines.\n- Get acquainted with mentors and students in FCJ.\n- Form a team of five members. 09/08/2025 FCJ Policies and Guidelines 3 - Get familiar with the AWS office environment.\n- Learn how to present an AWS workshop and start writing worklog for the first week. 09/09/2025 AWS Workshop Guide 4 - Getting started with lessons from the First Cloud Journey Bootcamp playlist:\n+ AWS cloud computing technology\n+ Creating and managing an AWS account\n+ AWS Virtual Private Cloud (VPC) 09/10/2025 First Cloud Journey Bootcamp - 2025 5 - Finish studying FCJ Bootcamp lessons:\n+ Compute VMs on AWS\n+ AWS security services\n+ AWS database services 09/11/2025 First Cloud Journey Bootcamp - 2025 6 - Meet with team to plan the first project.\n- Finalize the weekly worklog by adding achievements. 09/12/2025 Week 1 Achievements: Reviewed and understood AWS FCJ internship policies and guidelines. Went to the AWS office for the first time, gaining familiarity with workplace environment. Connected with mentors and fellow students in the FCJ program, formed a team of five members for project work. Initiated this internship report and wrote the first weekly worklog. Completed all lessons from the First Cloud Journey Bootcamp playlist. Learned the basics of AWS services Created an AWS free tier account Practiced managing services using the AWS account Collaborated with the team to plan the first project, defining the name, objectives and timelines. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar basic AWS services, team members and deciding on the first project.\nWeek 2: Gain in-depth knowledge of NLP. Collaborate with team to define features and assign project tasks.\nWeek 3: Learn React.js and applying the knowledge to develop the product management page. Study containerization to prepare for deployment.\nWeek 4: Focus on developing management pages for the seller panel.\nWeek 5: Completing the seller panel. Translate three technical blogs.\nWeek 6: Study AWS services and architectures to prepare for the midterm examination.\nWeek 7: Study more AWS services and architectures to prepare for the midterm examination.\nWeek 8: Study AWS architectures and solutions. Take the midterm examination.\nWeek 9: Prepare data sources and foundational knowledge for the recommendation system.\nWeek 10: Collect, generate and preprocess data for the recommendation model.\nWeek 11: Design, tune and train the recommendation model.\nWeek 12: Integrate the recommendation system into the application. Prepare for project deployment on AWS infrastructure.\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/5-workshop/5.4-lambda/create-lambda/",
	"title": "Create Lambda",
	"tags": [],
	"description": "",
	"content": "Create Lambda function Navigate to Lambda management console In the Dashboard console, choose Create function In the Create Lambda function console Name the lambda: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Runtime: choose python 3.13 or latest Expand the Change default execution role section In Execution role, choose Use an existing role and choose PollyLambdaRole Scroll down and choose Create lambda Successfully create S3 bucket. Create code Lambda In this workshop, we will use the code to convert a text folder into a voice folder (Text-to-Speech) using Amazon Polly service. Scroll down to the Code source section. Delete all existing code in the lambda_function.py file. We will use the code as follows: import json import boto3 import os s3 = boto3.client(\u0026#39;s3\u0026#39;) polly = boto3.client(\u0026#39;polly\u0026#39;) def lambda_handler(event, context): try: # 1. Get uploaded file info record = event[\u0026#39;Records\u0026#39;][0] bucket_name = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] object_key = record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] # Ex: input/hello.txt print(f\u0026#34;Processing file: {object_key}\u0026#34;) # 2. Read text file content file_obj = s3.get_object(Bucket=bucket_name, Key=object_key) text_content = file_obj[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) # 3. Call Polly to convert text to speech (Voice: Joanna) response = polly.synthesize_speech( Text=text_content, OutputFormat=\u0026#39;mp3\u0026#39;, VoiceId=\u0026#39;Joanna\u0026#39; ) # 4. Save MP3 file to output folder # Change name from input/abc.txt to output/abc.mp3 new_key = object_key.replace(\u0026#34;input/\u0026#34;, \u0026#34;output/\u0026#34;).replace(\u0026#34;.txt\u0026#34;, \u0026#34;.mp3\u0026#34;) if \u0026#34;AudioStream\u0026#34; in response: with response[\u0026#34;AudioStream\u0026#34;] as stream: s3.put_object( Bucket=bucket_name, Key=new_key, Body=stream.read(), ContentType=\u0026#39;audio/mpeg\u0026#39; ) return \u0026#34;Done!\u0026#34; except Exception as e: print(e) raise e Click the Deploy button. Create Trigger Right above the code section, click the + Add trigger button. Source: Select S3. Bucket: Select the bucket s3-demo-text. Event types: Select All object create events. Prefix: Enter input/ ⚠️ Note: You must enter input/. If left blank, Lambda will trigger even when the MP3 file is created -\u0026gt; Causing an infinite loop -\u0026gt; Increasing costs. Suffix: Enter .txt Check the box \u0026ldquo;I acknowledge\u0026hellip;\u0026rdquo; -\u0026gt; Click Add. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.Since I am performing this workshop for the first time, I will grant full access to the selected permissions.\n{ { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;polly:*\u0026#34;, \u0026#34;s3:*\u0026#34;, \u0026#34;logs:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } } "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Acquire an understanding of natural language processing to support project development. Collaborate with team to determine and prioritize the features to be implemented in the application for the project. Tasks to be carried out this week: Day Task Date Reference Material 2 - Study classification and vector spaces in natural language processing.\n+ Sentiment analysis\n+ Vector space models\n+ Machine translation\n+ Document search 09/15/2025 Natural Language Processing with Classification and Vector Spaces 3 - Go to AWS office and discuss with team about which features to be implemented in the application.\n- Study probabilistic models for NLP.\n+ Autocorrect \u0026amp; Autocomplete\n+ Hidden Markov models\n+ Language models\n+ Word embeddings 09/16/2025 Natural Language Processing with Probabilistic Models 4 - Study recurrent neutral networks for NLP.\n+ Gated recurrent units\n+ Long short-term memory units\n+ Named entity recognition\n+ Siamese networks 09/17/2025 Natural Language Processing with Sequence Models 5 - Study attention models.\n+ Neutral machine translation\n+ Text summarization\n+ Question answering 09/18/2025 Natural Language Processing with Attention Models 6 - Meet with team to assign tasks and set deadlines.\n- Study basic JavaScript syntax and features. 09/19/2025 Interactivity with JavaScript Week 2 Achievements: Gained an understanding of natural language processing through the \u0026ldquo;Natural Language Processing\u0026rdquo; specialization on Coursera, covering: Classification and vector spaces Probabilistic models Sequence models Attention models Collaborated with the team at the AWS office to identify and prioritize key features for the project application. Assigned tasks and set deadlines with the team. Acquired foundational knowledge of JavaScript syntax and features through Coursera’s \u0026ldquo;Interactivity with JavaScript\u0026rdquo; course. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: “DevOps on AWS” Event Objectives Understand the core principles of DevOps culture and key performance metrics (DORA, MTTR). Master the AWS CI/CD toolchain for automating build, test, and deployment. Learn Infrastructure as Code (IaC) concepts using CloudFormation and AWS CDK. Explore containerization strategies using ECR, ECS, EKS, and App Runner. Implement full-stack observability and monitoring with CloudWatch and X-Ray. Speakers AWS DevOps Specialists Senior Solutions Architects Key Highlights DevOps Culture \u0026amp; Mindset Recap: Integration with AI/ML concepts from previous sessions. Metrics Matter: Focus on Deployment Frequency, Lead Time for Changes, Mean Time to Restore (MTTR), and Change Failure Rate (DORA metrics). Cultural Shift: Moving from siloed teams to shared responsibility. AWS CI/CD Pipeline Source Control: Utilizing AWS CodeCommit and implementing Git strategies like GitFlow and Trunk-based development. Build \u0026amp; Test: Configuring AWS CodeBuild for automated testing and compilation. Deployment: Using AWS CodeDeploy to implement safe deployment strategies: Blue/Green: Reduces downtime and risk. Canary: Gradual rollout to a small subset of users. Rolling: Update instances incrementally. Orchestration: Tying it all together with AWS CodePipeline. Infrastructure as Code (IaC) AWS CloudFormation: Defining infrastructure using templates, stacks, and managing configuration drift. AWS CDK (Cloud Development Kit): Using familiar programming languages to define cloud resources as code constructs. Comparison: Choosing between declarative templates (CloudFormation) vs. imperative code (CDK) based on team skills. Container Services \u0026amp; Observability Container Management: Storing images in Amazon ECR with lifecycle policies. Orchestration: Choosing between Amazon ECS (simpler, AWS-native) and Amazon EKS (Kubernetes standard), or AWS App Runner for simplified PaaS-like deployment. Monitoring: Using Amazon CloudWatch for metrics/alarms and AWS X-Ray for distributed tracing to identify performance bottlenecks. Key Takeaways DevOps Strategy Automation First: Manual deployments are error-prone; everything from infrastructure to code deployment should be automated. Measurement: You cannot improve what you do not measure. Use DORA metrics to track velocity and stability. Shift Left: Integrate testing and security early in the CI/CD pipeline, not at the end. Technical Architecture Immutable Infrastructure: Treat servers as disposable resources; replace them rather than patching them in place. Containerization: Decouple applications from the underlying OS to ensure consistency across environments (Dev, Test, Prod). Observability: Moving beyond simple \u0026ldquo;up/down\u0026rdquo; monitoring to deep insights using distributed tracing. Applying to Work Implement CI/CD: Set up a CodePipeline for current projects to automate the build/deploy process for Spring Boot/React applications. Adopt IaC: Start defining AWS resources (databases, S3 buckets, Cognito User Pools) using AWS CDK instead of the console. Containerize: Dockerize existing microservices and push images to ECR. Enhance Monitoring: Add X-Ray instrumentation to backend services to visualize API latency and database query performance. Event Experience Attending the “DevOps on AWS” workshop provided a practical roadmap for automating the software delivery lifecycle. It bridged the gap between writing code and running it reliably in production. Key experiences included:\nLearning from experts Gained clarity on the \u0026ldquo;Alphabet Soup\u0026rdquo; of AWS tools (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) and how they integrate. Understood the strategic value of DORA metrics in justifying DevOps investments to business stakeholders. Hands-on technical exposure CI/CD Walkthrough: The demo of a full pipeline showed exactly how code changes trigger builds and deployments automatically. IaC Implementation: Seeing AWS CDK in action was a highlight—writing infrastructure in Java/TypeScript is much more intuitive for developers than writing JSON/YAML templates. Deployment Strategies: Visualizing Blue/Green deployments demonstrated how to release updates with zero downtime. Networking and discussions Discussed the trade-offs between ECS and EKS with peers, realizing that for many projects, ECS or App Runner provides a faster path to production with less overhead. Exchanged ideas on how to handle database migrations within a CI/CD pipeline. Lessons learned Drift Detection in CloudFormation is critical for maintaining infrastructure integrity. Observability is not optional for microservices; without X-Ray, debugging distributed architectures is nearly impossible. A solid DevOps foundation significantly reduces Mean Time to Recovery (MTTR), allowing teams to innovate faster with less fear of breaking production. Event photo "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Word document version\nMealPlan Personalized Food Ingredient Sales Platform 1. Executive Summary The Personalized Food Ingredient Sales Platform focuses on enabling faster and more efficient shopping. Users register accounts to access a diverse recipe database, receive AI-driven meal suggestions based on purchase history, and order with doorstep delivery. Leveraging AWS cloud infrastructure, the platform ensures flexible scalability, high performance, and secure management.\n2. Problem Statement Current Problem\nCustomers face challenges in efficiently sourcing ingredients for meal preparation, often spending significant time searching for supplies that match recipes. Existing platforms offer recipe and menu suggestions but lack integrated ingredient purchasing, requiring users to source supplies independently, which is time-consuming and prone to errors in portioning.\nSolution\nThe platform employs Spring Boot for a robust REST API back-end handling user registration, recipe management, shopping cart, and order processing, with React delivering a user-friendly front-end featuring AI-driven meal recommendations. Data is stored in Amazon EC2 (PostgreSQL), images and static assets in Amazon S3, with front-end, back-end, AI model deployed on another Amazon EC2 instance within a secure VPC, and Route 53 managing the domain. Users can customize portions, receive personalized recipe suggestions, and order ingredients for delivery. Key features include a diverse recipe library, accurate calorie calculations, and relatively low operational costs.\nBenefits \u0026amp; Return on Investment The solution establishes a comprehensive platform for the nutrition startup to expand services while collecting user data for enhanced recommendation systems. It eliminates manual calorie calculations and fragmented shopping through an integrated system, simplifying nutrition planning and driving revenue from ingredient sales. Estimated monthly cost is $119.51 (per AWS Pricing Calculator), totaling approximately $1434.12 for 12 months. Development leverages open-source frameworks, incurring no additional hardware costs. ROI is expected within 6–12 months through user time savings and consistent order revenue.\n3. Solution Architecture The website is hosted on EC2. Data is stored on an EC2 instance. Images are stored in S3. Code is pushed to GitHub for management and automatically uploaded to S3 so CodeDeploy can deploy the application to the server. CloudFront is used to improve loading performance. Cognito manages user identities. CloudTrail monitors and stores activity logs. CloudWatch monitors and manages performance and the health of AWS resources and applications. IAM grants permissions to services. Secrets Manager stores sensitive information.\nAWS Services Used\nWAF: Protects the web application from cyber-attacks. AWS CloudFront: Improves website loading speed. AWS EC2: Hosts the application, NAT instance, and database. AWS VPC: Virtual private network. AWS S3: Stores code, log files, and images. CodeDeploy: Deploys code to EC2. GitLab: Hosts source code and pushes it to S3. Amazon Cognito: Manages user authentication for the web application. IAM: Creates users and roles. Secrets Manager: Stores sensitive information. CloudTrail: Monitors and stores activity logs. CloudWatch: Monitors and manages the performance and health of AWS resources. 4. Technical Deployment Deployment Phases\nThis project includes two main parts: developing the Spring Boot backend and the React frontend, and deploying the website on AWS using AWS services. Each part includes four phases.\nTheory \u0026amp; Architecture Design: Gather web application requirements, design the system architecture (Spring Boot REST API + React frontend), and define the database schema. (January)\nDevelopment \u0026amp; Testing: Implement the Spring Boot backend with REST APIs (authentication, user management, meal/recipe CRUD, shopping cart, etc.) and build the React frontend (UI/UX, routing, forms, state management). Conduct unit tests for backend services, integration tests for API endpoints, and frontend tests (Jest/React Testing Library). (January–February)\nCost Estimation \u0026amp; Feasibility Check: Use the AWS Pricing Calculator to estimate costs for EC2 (backend hosting), EC2 (database), S3 (static files and images), VPC (networking), and Route 53 (domain). Adjust as needed. (February)\nAWS Integration: Integrate AWS services into the application. Deploy the website on EC2, store images on S3, configure EC2 for the database, use VPC for networking, Route 53 for domain management, and set up CI/CD pipelines (GitHub Actions or AWS CodePipeline). Perform staging tests before official release. (March)\nTechnical Requirements\nBackend (Spring Boot): REST APIs for authentication, user management, meal/recipe CRUD, shopping cart, and order processing. Includes security (JWT, Spring Security). Frontend (React): Responsive web application with a user-friendly UI/UX integrated with the backend API. Database (EC2): Relational database (MySQL/PostgreSQL) hosted on EC2, storing users, recipes, shopping carts, and order data. Storage (AWS S3): Used to store user-uploaded images. Hosting \u0026amp; Networking (AWS EC2 \u0026amp; AWS VPC): Application deployed on EC2 instances. CI/CD (GitHub Actions or AWS CodePipeline): Automated build and deployment pipeline for backend and frontend. Authentication \u0026amp; Security: JWT authentication and HTTPS configuration; optional AWS Cognito for user access management. 5. Roadmap \u0026amp; Deployment Milestones January: Build theoretical foundation and draw architecture (Spring Boot backend + React frontend design, database schema). Begin initial backend and frontend development. February: Continue backend and frontend development, perform unit and integration tests. Use AWS Pricing Calculator to evaluate hosting costs and refine architecture for cost efficiency. March: Integrate AWS services, configure CI/CD pipelines, conduct staging tests, and deploy the website to production. Post-launch: Up to 3 months for maintenance, optimization, and feature improvements. 6. Cost Estimate Costs can be viewed via the AWS Pricing Calculator.\nAWS Services AWS WAF: $11.6/month Application Load Balancer (ALB): $18.63/month Amazon EC2 Application: $19.27/month Amazon EC2 Data Tier: $9.64/month Amazon EC2 NAT Instances: $19.27/month Amazon S3: $3.72/month AWS CodeDeploy: $0 AWS Secrets Manager: $0.4/month Amazon Cognito: $14.25/month Amazon CloudWatch: $4.91/month AWS CloudTrail: $1.77/month VPC Endpoints: $16.05/month Total: $119.51/month, $1,434.12/year\n7. Risk Assessment Risk Matrix\nNetwork loss: Medium impact / Medium probability. EC2 / ALB / AZ failure: High impact / Low–medium probability. Secret leakage: High impact / Low–medium probability. Cost overrun: Medium impact / Medium probability. Mitigation Strategies\nAvailability: Multi-AZ + Auto Scaling + ALB; health checks; caching with CloudFront; Route 53 failover. Security: IAM with least-privilege; Secrets Manager with rotation and access logging; WAF/Shield protection. Cost Management: AWS Budgets \u0026amp; Cost Explorer; rightsizing; Reserved or Spot Instances where appropriate. Fallback Plan\nAutomatic rollback using CodeDeploy. Manual fallback: GitLab runner → prebuilt AMIs or ASGs. 8. Expected Outcomes Fully automated CI/CD (GitLab → CodePipeline → CodeBuild → CodeDeploy) reduces manual errors. Multi-layer security (IAM, WAF, Secrets Manager) with auditing via CloudTrail. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Gain foundational knowledge of front-end development using React.js to support project application development. Collaborate with the team to develop key components of the project’s web application, including seller panel features. Acquire an understanding of containerization concepts to prepare for deployment of the project. Tasks to be carried out this week: Day Task Date Reference Material 2 - Study the basics of developing applications using React.js. 09/22/2025 Developing Front-End Apps with React 3 - Visit the AWS office and start working on the first project.\n+ Initiated a product management page for the web application 09/23/2025 4 - Continue project development.\n+ Develop the product management page 09/24/2025 5 - Finish the first task in the project.\n+ Complete the product management page. 09/25/2025 6 - Meet with team to review progress and plan for subsequent tasks.\n- Study containerization concepts.\n+ Docker\n+ Kubernetes\n+ OpenShift\n+ Istio 09/26/2025 Introduction to Containers w/ Docker, Kubernetes \u0026amp; OpenShift Week 3 Achievements: Acquired knowledge of React.js through the Coursera course \u0026ldquo;Developing Front-End Apps with React\u0026rdquo;, covering: Creation and management of components and props Implementation of state and lifecycle methods Handling events for interactive user interfaces Developed a seller panel for the project’s web application, including: Product management Order management Dashboard Gained an understanding of containerization through the Coursera course \u0026ldquo;Introduction to Containers w/ Docker, Kubernetes \u0026amp; OpenShift\u0026rdquo;, covering: Docker basics Kubernetes orchestration OpenShift fundamentals Reviewed project progress with the team, assigned new tasks. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Rapid ML experimentation for enterprises with Amazon SageMaker AI and Comet This blog explores how enterprises can leverage Amazon SageMaker AI and Comet to streamline machine learning (ML) experimentation for scalable and reproducible workflows. It highlights the importance of robust experiment tracking, model lineage, and compliance in handling complex ML initiatives, particularly in regulated environments. The article demonstrates a fraud detection use case, guiding readers through setting up a managed ML environment, integrating Comet for experiment management, and processing imbalanced datasets. It also covers ensuring regulatory compliance (e.g., EU AI regulations) and provides steps for administrators and users to configure and utilize SageMaker AI with Comet effectively.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Strengthen understanding of React.js by implementing components for the seller panel. Developing management interfaces such as order and inventory pages. Review progress and plan for the upcoming week. Tasks to be carried out this week: Day Task Date Reference Material 2 - Develop an order management page for the seller panel. 09/29/2025 3 - Continue developing the order management page. 09/30/2025 4 - Develop a dashboard page for the seller panel. 10/01/2025 5 - Finish developing the order management and the dashboard page. 10/02/2025 6 - Team meeting to review progress and determine tasks for the next week.\n- Implement an inventory management (ingredient management) page. 10/03/2025 Week 4 Achievements: Developed and integrated the order management page for the seller panel, enabling sellers to view, update, and track orders. Created a dashboard for the seller panel, providing insights into sales performance, order volume, and product statistics. Implemented an inventory management page that allows sellers to monitor and manage ingredients. Collaborated with the team to review progress, discuss design improvements, and plan next week’s development tasks. Gained deeper hands-on experience in building front-end components with React.js. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/4-eventparticipated/",
	"title": "Events participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in four events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud Mastery Series #1\nDate \u0026amp; Time: 08:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #2\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #3\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: Building Agentic AI \u0026amp; Context Optimization with Amazon Bedrock\nDate \u0026amp; Time: 09:00, December 05, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/5-workshop/5.5-testing/",
	"title": "Testing",
	"tags": [],
	"description": "",
	"content": "Create input file On your computer, create a file named test.txt. Create with the text: \u0026ldquo;Hello, congratulations on completing the workshop!\u0026rdquo; Save file Return to the S3 tab, open the input folder. Click Upload -\u0026gt; Select test.txt -\u0026gt; Click Upload Wait for about 5 seconds.\nGo back to the Bucket console, then open the output folder. You will see the file test.mp3. Select it and click Download.\nOpen it to listen.\n"
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nAutomated Text-to-Speech Converter using Serverless Overview Event-Driven Architecture allows you to build applications that automatically respond to changes in state, such as a file upload, without provisioning or managing servers.\nIn this lab, you will learn how to create, configure, and test a Serverless pipeline that automatically converts text files into lifelike speech audio using AWS Managed Services.\nYou will utilize two key mechanisms to process data asynchronously:\nS3 Event Notifications - Configure Amazon S3 to act as an event source. It will automatically trigger a compute function whenever a new object is uploaded to a specific input folder. Managed AI Services - Leverage Amazon Polly to synthesize speech from text. This allows you to add deep learning capabilities to your application through simple API calls, without needing data science expertise. Content Workshop overview Prerequisite Build Core Logic (Lambda) Configure Storage (S3) Testing Result Clean up "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Finalize the remaining features in the seller panel and ensure system stability through integration testing. Improve specialized knowledge and translation skills by translating technical blog articles. Review progress and plan for the next project phase. Tasks to be carried out this week: Day Task Date Reference Material 2 - Complete the inventory management page. 10/06/2025 3 - Conduct integration testing for the seller panel. 10/07/2025 4 - Translate the first technical blog. 10/08/2025 Serverless generative AI architectural patterns – Part 1 5 - Translate the second technical blog. 10/09/2025 Reduce your Amazon ElastiCache costs by up to 60% with Valkey and CUDOS 6 - Translate the third technical blog.\n- Meet with team to review project progress and determine next steps. 10/10/2025 Getting started with Amazon EC2 bare metal instances for Amazon RDS for Oracle and Amazon RDS Custom for Oracle Week 5 Achievements: Completed the inventory management page. Conducted integration testing, ensuring functionality and data connections among seller panel pages (product, order, inventory and dashboard). Translated three technical blog articles, improving understanding and expression of specialized technical content in Vietnamese. Met with team to discuss results and propose the next development phase. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for building Serverless Event-driven applications on AWS.\nBy configuring S3 Event Notifications, you enabled an automated workflow where compute resources (AWS Lambda) react instantly to data ingestion without the need for manual intervention or server management.\nBy integrating Amazon Polly, you successfully leveraged Managed AI Services to transform text into lifelike speech, demonstrating how to add complex machine learning capabilities to your application with minimal code.\nClean up Delete S3 buckets Open S3 console Open s3-demo-bucket Choose 2 folders input and output Click delete and confirm Then choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. Delete Lambda function Open the lambda console Find workshop1 and click action Choose Delete and confirm Delete IAM role Open IAM console Select Roles from the menu on the left. Find the role PollyLambdaRole. Select the role and click Delete. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Review and deepen knowledge of AWS services and architectures in preparation for the midterm examination. Tasks to be carried out this week: Day Task Date Reference Material 2 - Review computing services.\n+ Amazon EC2\n+ Amazon EBS\n+ Amazon EFS\n+ Amazon Lightsail\n+ AWS Lambda\n- Learn about AWS container services.\n+ Amazon ECS\n+ Amazon EKS\n+ Amazon ECR\n+ AWS Batch 10/13/2025 FCJ Bootcamp Module 3\nEC2 Documentation\nEBS Documentation\nEFS Documentation\nLightsail Documentation\nLambda Documentation\nECS Documentation\nEKS Documentation\nECR Documentation\nBatch Documentation 3 - Review and study more database services.\n+ Amazon RDS\n+ Amazon Aurora\n+ Amazon DynamoDB\n+ Amazon DocumentDB\n+ Amazon ElastiCache\n+ Amazon MemoryDB\n+ Amazon Neptune\n+ Amazon Timestream 10/14/2025 FCJ Bootcamp Module 6\nRDS and Aurora Documentation\nDynamoDB Documentation\nDocumentDB Documentation\nElastiCache Documentation\nMemoryDB Documentation\nNeptune Documentation\nTimestream Documentation 4 - Learn more about computing and serverless services.\n+ Amazon Elastic Beanstalk\n+ AWS Fargate\n+ Elastic Load Balancing\n+ AWS App Runner\n+ Amazon GameLift Servers\n+ Amazon Amplify\n+ AWS AppSync\n+ Amazon Location Service 10/15/2025 Elastic Beanstalk Documentation\nFargate Documentation\nElastic Load Balancing Documentation\nApp Runner Documentation\nGameLift Servers Documentation\nAmplify Documentation\nAppSync Documentation\nLocation Service Documentation 5 - Review and learn more about storage services.\n+ Amazon S3\n+ AWS Snowball Edge\n+ AWS Storage Gateway\n+ AWS Backup\n+ AWS DRS 10/16/2025 FCJ Bootcamp Module 4\nS3 Documentation\nSnowball Edge Documentation\nStorage Gateway Documentation\nBackup Documentation\nDRS Documentation 6 - Getting started with AWS Well-Architected. 10/17/2025 Well-Architected Overview Week 6 Achievements: Strengthened knowledge across various AWS services: Computing: EC2, Lightsail, ECS, EKS, ECR, Lambda, etc. Storage: S3, EBS, EFS, Snowball Edge, Storage Gateway, etc. Databases: RDS, DynamoDB, DocumentDB, ElastiCache, etc. Gained additional understanding of risk mitigation using Backup and DRS. Learned about cloud architecture with the AWS Well-Architected Framework. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at AWS Vietnam from 09/08/2025 to 12/09/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in machine learning and front-end development for an e-commerce web application, through which I improved my skills in machine learning, data science, programming, communication, teamworking and project management.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ☐ ✅ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Strengthen discipline and motivation to complete tasks on time. Be more proactive in seeking out tasks and taking initiative. Improve problem-solving skills by identifying, proposing solutions, and showing creativity more effectively. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Continue improving knowledge of AWS services and architecture in preparation for the midterm examination. Practice hands-on labs to strengthen understanding of key AWS services. Tasks to be carried out this week: Day Task Date Reference Material 2 - Learn more about AWS Well-Architected. 10/20/2025 AWS Well-Architected 3 - Study Amazon VPC and AWS CloudFormation.\n- Perform hands-on labs for VPC, CloudFormation and EC2. 10/21/2025 VPC Documentation\nCloudFormation Documentation\nInitialize CloudFormation templates\nCreate an EC2 instance\nCreate Security Groups\nUpdate NACLs 4 - Perform hands-on labs for Backup, Storage Gateway and S3.\n- Study S3 in more depth. 10/22/2025 FCJ Bootcamp Lab 13, 24, 57\nS3 Documentation 5 - Explore machine learning services.\n+ Amazon A2I\n+ Amazon SageMaker AI\n+ AWS DLAMI\n+ AWS Deep Learning Containers\n+ AWS Entity Resolution\n+ PyTorch\n+ TensorFlow\n+ Hugging Face 10/23/2025 A2I Documentation\nSageMaker AI Documentation\nDLAMI Documentation\nDeep Learning Containers Documentation\nEntity Resolution Documentation\nPyTorch on AWS\nTensorFlow on AWS\nHugging Face on AWS 6 - Explore specialized machine learning services.\n+ Amazon CodeGuru\n+ Amazon DevOps Guru\n+ Amazon Kendra\n+ Amazon Personalize\n+ Amazon Rekognition\n+ Amazon Comprehend\n+ Amazon Translate\n+ Amazon Textract\n+ Amazon Polly\n+ Amazon Lex 10/24/2025 CodeGuru Documentation\nDevOps Guru Documentation\nKendra Documentation\nPersonalize Documentation\nRekognition Documentation\nComprehend Documentation\nTranslate Documentation\nTextract Documentation\nPolly Documentation\nLex Documentation Week 7 Achievements: Gained further understanding of AWS architectures. Improved knowledge of networking and storage services. Built more experience with compute and storage services. Explored a wide range of AWS machine learning services. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Strengthen knowledge in preparation for the midterm examination. Visit the AWS office to receive the shirt and take the midterm test. Tasks to be carried out this week: Day Task Date Reference Material 2 - Discuss midterm topics with FCJ peers.\n- Conduct knowledge review and cross-checking with team members. 10/27/2025 3 - Research common questions for the SSA certification examination. 10/28/2025 4 - Read and explore AWS Solutions Library. 10/29/2025 AWS Solutions Library 5 - Continue studying solutions from AWS Solutions Library. 10/30/2025 AWS Solutions Library 6 - Revise the AWS Well-Architected Framework.\n- Visit the AWS office to receive the FCJ shirt and take the midterm examination. 10/31/2025 AWS Well-Architected Week 8 Achievements: Conducted group discussions and strengthened knowledge through peer review sessions. Gained familiarity with common question types for the SSA examination. Improved understanding of solution architectures via AWS Well-Architected and AWS Solutions Library. Took the midterm examination. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Begin developing the initial ideas for the product recommendation system. Clarify the process of collecting, simulating, and standardizing data. Understand the basic architecture of a recommendation system. Tasks to be carried out this week: Day Task Date Reference Material 2 - Find data sources to simulate the database. 10/03/2025 3 - Research product recommendation systems. 10/04/2025 4 - Study TensorFlow and TensorFlow Recommenders. 10/05/2025 TensorFlow Recommenders 5 - Discuss data structure and required data details with team member. 10/06/2025 6 - Build experimental models to reinforce knowledge about recommendation systems.\n- Group meeting to plan the next development tasks. 10/07/2025 Week 9 Achievements: Collected initial data sources for model development. Gained an understanding of the key concepts and approaches to recommendation systems. Became familiar with TensorFlow Recommenders and built basic experimental models. Reached team alignment on data requirements and upcoming tasks. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Complete simulated data preparation for the database. Begin data preprocessing steps to prepare for model training. Tasks to be carried out this week: Day Task Date Reference Material 2 - Work with team to collect and generate data. 11/10/2025 3 - Create additional simulated data for the database. 11/11/2025 4 - Finalize data simulation for the database. 11/12/2025 5 - Begin data preprocessing for the recommendation model. 11/13/2025 6 - Make necessary data adjustments in the database. 11/14/2025 Week 10 Achievements: Completed the collection and generation of necessary data for the system. Built a relatively complete simulated dataset for model training. Started preprocessing and standardizing input data. Ensured the dataset was updated and structured properly for the next development phase. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Complete the data preprocessing phase. Build the TFRS model. Tasks to be carried out this week: Day Task Date Reference Material 2 - Complete the data preprocessing process. 11/17/2025 3 - Start building the recommendation model using TFRS. 11/18/2025 4 - Design and implement the model architecture. 11/19/2025 5 - Fine-tune model hyperparameters. 11/20/2025 6 - Complete training and save the final model. 11/21/2025 Week 11 Achievements: Completed data preprocessing and normalization for model training. Designed and fine-tuned the recommendation model. Successfully trained the model, ready for system integration. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Research AI integration into the application using DJL. Deploy the recommendation model into the system and complete the recommendation feature. Tasks to be carried out this week: Day Task Date Reference Material 2 - Research AI integration and Deep Java Library. 11/24/2025 DJL Documentation 3 - Start integrating the model into the application. 11/25/2025 4 - Working on the recommendation system. 11/26/2025 5 - Complete integration of the recommendation system. 11/27/2025 6 - Team meeting to discuss deployment to AWS infrastructure. 11/28/2025 Week 12 Achievements: Learned how to deploy AI models using DJL. Successfully integrated the recommendation system into the application. Planned deployment strategy to AWS infrastructure after team discussion. "
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://muyou17.github.io/fptse190168-nguyen-truong-phuc-thinh/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]